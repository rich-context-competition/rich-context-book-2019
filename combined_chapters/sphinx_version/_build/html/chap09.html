
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Chapter 9 - Finding datasets in publications: The University of Paderborn approach &#8212; Rich Search and Discovery for Research Datasets 1.0 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Chapter 10 - Finding datasets in publications: The Singapore Management University approach" href="chap10.html" />
    <link rel="prev" title="Chapter 8 - Knowledge Extraction from scholarly publications: The GESIS contribution to the Rich Context Competition" href="chap08.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <hr class="docutils" />
<div class="section" id="chapter-9-finding-datasets-in-publications-the-university-of-paderborn-approach">
<h1>Chapter 9 - Finding datasets in publications: The University of Paderborn approach<a class="headerlink" href="#chapter-9-finding-datasets-in-publications-the-university-of-paderborn-approach" title="Permalink to this headline">¶</a></h1>
</div>
<div class="section" id="abstract">
<h1>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">¶</a></h1>
<p>The steadily increasing number of publications available to researchers
makes it difficult to keep track of the state of the art. In particular,
tracking the datasets used, topics addressed, experiments performed and
results achieved by peers becomes increasingly tedious. Current academic
search engines render a limited number of entries pertaining to this
information. However, having this knowledge would be beneficial for
researchers to become acquainted with all results and baselines relevant
to the problems they aim to address. With our participation in the NYU
Coleridge Initiative’s Rich Context Competition, we aimed to provide
approaches to automate the discovery of datasets, research fields and
methods used in publications in the domain of Social Sciences. We
trained an Entity Extraction model based on Conditional Random Fields
and combined it with the results from a Simple Dataset Mention Search to
detect datasets in an article. For the identification of Fields and
Methods, we used word embeddings. In this chapter, we describe how our
approaches performed, their limitations, some of the encountered
challenges and our future agenda.</p>
<hr class="docutils" />
<p>author:</p>
<ul class="simple">
<li><p>Rricha Jalota</p></li>
<li><p>Nikit Srivastava</p></li>
<li><p>Daniel Vollmers</p></li>
<li><p>René Speck</p></li>
<li><p>Michael Röder</p></li>
<li><p>Ricardo Usbeck</p></li>
<li><p>‘Axel-Cyrille [Ngonga Ngomo]{}’bibliography:</p></li>
<li><p>‘references.bib’title:
‘Finding datasets in publications: The University of Paderborn approach’</p></li>
</ul>
</div>
<hr class="docutils" />
<div class="section" id="literature-review">
<h1>Literature Review<a class="headerlink" href="#literature-review" title="Permalink to this headline">¶</a></h1>
<p>Previous works on information retrieval from scientific articles are
mainly seen in the field of Bio-medical Sciences and Computer Science,
with systems [&#64;DBLP:journals/ploscb/WestergaardSTJB18] built using the
MEDLINE<a class="reference external" href="https://www.nlm.nih.gov/bsd/medline.html">^1</a> abstracts, full-text articles from PubMed Central<a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/">^2</a> or ACL
Anthology dataset<a class="reference external" href="https://www.aclweb.org/anthology/">^3</a>. The documents belonging to the above-mentioned
datasets follow a similar format, and thus, several metadata and
bibliographical extraction frameworks like CERMINE [&#64;tkaczyk2014cermine]
have been built on them. However, since articles belonging to the domain
of Social Sciences do not follow a standard format, extracting key
sections and metadata using already existing frameworks like
GROBID [&#64;lopez2009grobid], ScienceParse<a class="reference external" href="https://github.com/allenai/science-parse">^4</a> or
ParsCit [&#64;councill2008parscit] did not seem as viable options, majorly
because these systems were still under development and lacked certain
desired features. Hence, building upon the approach of Westergaard et.
al [&#64;DBLP:journals/ploscb/WestergaardSTJB18], we built our own
sections-extraction framework for dataset detection and research fields
and methods identification.</p>
<p>Apart from content and metadata extraction, key-phrase or topic
extraction from scientific articles has been another emerging research
problem in the domain of information retrieval from scientific articles.
Jansen et al. [&#64;jansen2016extracting] extracted core claims from
scientific articles by first detecting keywords and key-phrases using
rule-based, statistical, machine learning and domain-specific approaches
and then applying document summarization techniques. For characterizing
a research work in terms of its focus, application domain and techniques
used, Gupta et al. [&#64;gupta2011analyzing] proposed applying semantic
extraction patterns to the dependency trees of sentences in an article’s
abstract. On the other hand, to thematically represent scientific
articles and for ranking the extracted key-phrases, Mahata et
al. [&#64;mahata2018key2vec] devised an approach for processing text
documents to train phrase embeddings.</p>
<p>The problem of dataset detection and methods and fields identification
is not only different from the ones mentioned above, but also our
approach for tackling it is radically disparate. The following sections
describe our approach in detail.</p>
</div>
<div class="section" id="project-architecture">
<h1>Project Architecture<a class="headerlink" href="#project-architecture" title="Permalink to this headline">¶</a></h1>
<p><img alt="_images/flowchart_paper.pdf" src="_images/flowchart_paper.pdf" />Data Flow Pipeline: Red lines depict the flow of given and generated
files between components whereas black lines represent the generation of
final output
files[]{data-label=&quot;fig:flowchart&quot;}{width=”\textwidth”}</p>
<p>Our pipeline (shown in Figure [fig:flowchart]) consisted of three main
components: 1) Preprocessing, 2) Fields and Methods Identification and
3) Dataset Extraction. The Preprocessing module read the text from
publications and generated some additional files (see
Section [preprocess] for details). These files along with the given
Fields and Methods vocabularies were used to infer Research Fields and
Methods from the publications. Then, the information regarding Research
Fields was passed onto the Dataset Detection module and using the
Dataset Vocabulary, Dataset Citations and Mentions were identified. The
following sections provide a detailed overview of each of these
components.</p>
</div>
<div class="section" id="preprocessing">
<h1>Preprocessing<a class="headerlink" href="#preprocessing" title="Permalink to this headline">¶</a></h1>
<p>As discussed in Chapter 5, the publications were provided in two
formats: PDF and text. For Phase-1, we used the given text files,
however during Phase-2, we came across many articles in the training
files that had not been properly converted to text and contained mostly
non-ASCII characters. To work with such articles, we relied on the open
source tool <code class="docutils literal notranslate"><span class="pre">pdf2text</span></code> from <code class="docutils literal notranslate"><span class="pre">poppler</span> <span class="pre">suite</span></code><a class="reference external" href="%5Bpoppler%5D%3Chttps://manpages.debian.org/testing/poppler-utils%3E">^5</a> to extract text from
PDFs. The <code class="docutils literal notranslate"><span class="pre">pdf2text</span></code> command served as the first preprocessing step and
was called as a subprocess from within a python script. It was used with
<code class="docutils literal notranslate"><span class="pre">-nopgbrk</span></code> argument to generate the text files.</p>
<p>Once we had the text files, we followed the rule-based approach as
proposed by Westergaard et al. [&#64;DBLP:journals/ploscb/WestergaardSTJB18]
for pre-processing. The following series of operations based mostly on
regular expressions were performed:</p>
<ul class="simple">
<li><p>Words split by hyphens were de-hyphenated</p></li>
<li><p>Irrelevant data was removed (i.e., equations, tables,
acknowledgment, references);</p></li>
<li><p>Main sections (i.e., abstract, keywords, JEL-Classification,
methodology/data, summary, conclusion) were identified and
extracted;</p></li>
<li><p>Noun phrases from these sections were extracted (using the python
library, spaCy<a class="reference external" href="https://github.com/explosion/spaCy">^6</a>).</p></li>
</ul>
<p>We came up with the heuristics for identifying the main sections after
going through the articles from different domains in the training data.
We collected the surface forms for the headings of all major sections
(abstract, keywords, introduction, data, approach, summary, discussion)
and applied regular expressions to search for them and separate them
from one another. The headings and their corresponding content were
stored as key-value pairs in a file. For generating noun-phrases, this
file was parsed and for all the values (content) in key-value
(heading-content) pairs, a spaCy object, <code class="docutils literal notranslate"><span class="pre">doc</span></code>, was created
sentence-wise. Using the built-in function for extracting noun chunks
([<code class="docutils literal notranslate"><span class="pre">doc.noun_chunks</span></code>]{}), we generated key-value pairs of heading and
noun-phrases found in the content and stored them in another file. This
file was later used for fields and methods identification.</p>
<p>To determine how well our approach performed in distinguishing sections,
we evaluated it on the articles in the validation dataset. During
evaluation, we figured out the limiting cases of our approach. A section
could not be differentiated either when there was no explicit mention of
any of its surface forms or if there were multiple mentions of the
surface forms in the articles. For instance, in the validation dataset
(see Table [tab:sections]), keywords were not extracted from 13
articles because of no explicit mention of the term ’keywords’ or its
variants. On manual inspection, we found keywords were actually not
mentioned in these 13 articles. In the remaining articles where the
keywords were present, our algorithm could not detect them from 1
article. For brevity, we have reported only four main sections in
Table [tab:sections]: title, abstract, keywords and methodology/data,
since these are the ones getting preferential treatment in methods and
fields identification. If a section was not found in the article
(because of no explicit mention of any of the surface forms), then only
the sections that could be detected were extracted. The remaining
content was saved as <code class="docutils literal notranslate"><span class="pre">reduced_content</span></code> after cleaning and noun-phrases
were extracted from it to prevent loss of any meaningful data.</p>
<p>[C[4cm]{} C[3.5cm]{} C[4cm]{}]{} <strong>Sections</strong> &amp; <strong>No explicit mention</strong>
&amp; <strong>Mentioned but not distinguished</strong>Title &amp; 0 &amp; 0Keywords &amp; 13 &amp; 1Abstract &amp; 0 &amp; 1Methodology/Data &amp; 18 &amp; 4</p>
<p>In addition to the main sections, we also extracted PDF metadata using
<code class="docutils literal notranslate"><span class="pre">pdfinfo</span></code> service from the <code class="docutils literal notranslate"><span class="pre">poppler</span> <span class="pre">suite</span></code> library. The metadata very
often contained the keywords and subject of an article, which was
helpful in those cases where the keywords were not found by the regular
expression.In the end, the preprocessing module generated four text files for a
publication: PDF-converted text, PDF-metadata, processed articles
containing relevant data, and noun phrases from the relevant sections,
respectively. These files were then passed on to the other two
components of the pipeline, which have been discussed below.</p>
</div>
<div class="section" id="approach">
<h1>Approach<a class="headerlink" href="#approach" title="Permalink to this headline">¶</a></h1>
<div class="section" id="research-fields-and-methods-identification">
<h2>Research Fields and Methods Identification<a class="headerlink" href="#research-fields-and-methods-identification" title="Permalink to this headline">¶</a></h2>
<div class="section" id="vocabulary-generation-and-model-preperation">
<h3>Vocabulary Generation and Model Preperation<a class="headerlink" href="#vocabulary-generation-and-model-preperation" title="Permalink to this headline">¶</a></h3>
<ol>
<li><p><strong>Research Methods Vocabulary</strong>: In Phase-1 of the challenge, we
used the given methods vocabulary. However, the feedback that we
received from Phase-1 evaluation gave more emphasis to statistical
methods used by the authors, references to the time scope, unit of
observation, and regression equations rather than the means used to
compile the data, i.e., surveys. Since the given methods vocabulary
was not a complete representation of statistical methods and also
consisted terms depicting surveys, in Phase-2, we decided to create
our own Research Methods Vocabulary using Wikipedia and DBpedia.<a class="reference external" href="https://wiki.dbpedia.org/services-resources/ontology">^7</a>
We manually curated a list of all the relevant statistical methods
from Wikipedia<a class="reference external" href="https://en.wikipedia.org/wiki/Category:Statistical_methods">^8</a> and fetched their descriptions from the
corresponding DBpedia resources. For each label in the vocabulary,
we extracted noun phrases from its description and added them to the
vocabulary. Please refer Table [tab:vocab] for examples.</p>
<p>[C[1.5cm]{} C[5cm]{} C[5cm]{}]{} <strong>Label</strong> &amp; <strong>Description</strong> &amp;
<strong>Noun Phrases from Description</strong>Political forecasting &amp; Political forecasting aims at predicting the
outcome of elections. &amp; Political forecasting, the outcome,
electionsNested sampling algorithm &amp; The nested sampling algorithm is a
computational approach to the problem of comparing models in
Bayesian statistics, developed in 2004 by physicist John Skilling. &amp;
algorithm, a computational approach, the problem, comparing models,
Bayesian statistics, physicist John Skilling</p>
</li>
<li><p><strong>Research Fields Vocabulary</strong>: For both the phases, we used the
given research fields vocabulary and, just like the methods
vocabulary, supplemented it with the noun phrases from the
description of the research field labels. However, since our phase-1
model seemed to confuse fields with methods, for Phase-2, we
additionally created a stopword-list of terms that didn’t contain
any domain-specific information, such as; Mixed Methods, Meta
Analysis, Narrative Analysis and the like.</p></li>
<li><p><strong>Word2Vec Model generation</strong>: In this pre-processing step, we used
the above-mentioned vocabulary files containing noun phrases to
generate a vector model for both research fields and methods. The
vector model was generated by using the labels and noun phrases from
the description of the available research fields and methods to form
a sum vector. The sum vector was basically the sum of all the
vectors of the words present in a particular noun phrase. 3em [The
pre-trained Word2Vec model
<code class="docutils literal notranslate"><span class="pre">GoogleNews-vectors-negative300.bin</span></code> [&#64;DBLP:journals/corr/abs-1301-3781]
was used to extract the vectors of the individual words.]{}</p></li>
<li><p><strong>Research Method training results creation</strong>: For research methods,
we generated an intermediate result file with the publications
present in the training data. It was generated using a
<code class="docutils literal notranslate"><span class="pre">naïve</span> <span class="pre">finder</span> <span class="pre">algorithm</span></code> which, for each publication, selected the
research method with the highest cosine similarity to any of its
noun phrase’s vectors. This file was later used to assign weights to
research methods using Inverse Document Frequency.</p></li>
</ol>
</div>
<div class="section" id="processing-with-trained-models">
<h3>Processing with Trained Models<a class="headerlink" href="#processing-with-trained-models" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><strong>Finding Research Fields and Methods:</strong> To find the research fields
and methods for a given list of publications, we performed the
following steps: (At first, Step 1 was executed for all the
publications, thereafter Step 2 and 3 were executed iteratively for
each publication).</p>
<ol class="simple">
<li><p><strong>Naïve Research Method Finder run</strong> - In this step, we executed
the <code class="docutils literal notranslate"><span class="pre">naïve</span> <span class="pre">research</span> <span class="pre">method</span> <span class="pre">finding</span> <span class="pre">algorithm</span></code> (i.e. selected a
research method based on the highest cosine similarity between
vectors) against all the current publications and then merged
the results with the existing result from the
<code class="docutils literal notranslate"><span class="pre">research</span> <span class="pre">methods’</span> <span class="pre">preprocessing</span> <span class="pre">step</span></code>. The combined result was
then used to generate IDF weight values for each
<code class="docutils literal notranslate"><span class="pre">research</span> <span class="pre">method</span></code>, to compute the significance of recurring
terms.</p></li>
<li><p><strong>IDF-based Research Method Selection</strong> - We re-ran the
algorithm to find the closest research method to each noun
phrase and then sorted the pairs based on their weighted cosine
similarity. The weights were taken from the IDF values generated
in the first step and the manual weights assigned (section-wise
weighting). Here, the noun phrases that came from the
methodology section and from the methods listed in
JEL-classification (if present) were given a higher preference.
The pair with the highest weighted cosine similarity was then
chosen as the Research Method of the article.</p></li>
<li><p><strong>Research Field Finder run</strong> - In this step, we first found the
closest research field from each noun phrase in the publication.
Then we selected the Top N (= 10) pairs that had the highest
weighted cosine similarity. Afterwards, the noun phrases that
had a similarity score less than a given threshold (= 0.9) were
filtered out. The end-result was then passed on to a
post-processing algorithm.For weighted cosine similarity, the weights were assigned
manually based on the section of publication from which the noun
phrases came. In general, noun phrases from title and keywords
(if present) were given a higher preference than other sections,
since usually these two sections hold the crux of an article.
Note, if sections could not be discerned from an article, then
noun phrases from the section, reduced_content (see section
[preprocess]), were used to find both fields and methods.</p></li>
<li><p><strong>Research Field Selection</strong> - The top-ranked term from the
result of step 3, which was not present in the stopword-list of
irrelevant terms, was marked as the research field of the
article.</p></li>
</ol>
</li>
</ul>
<p>The experimental set-up and average training times (ATT) have been
reported in Table [tab:setup]:</p>
<p>[|C[5cm]{} | C[7cm]{} |]{} Computing Infrastructure &amp; macOS, 2 GHz Intel
Core i7 processor, 4 cores RAM 16 GB 1600 MHzATT - RF model &amp; 3m 21sATT - RM model &amp; 3m 19sLink to Implemented Code &amp;
<a class="reference external" href="https://github.com/nikit91/Jword2vec/tree/rich-context">https://github.com/nikit91/Jword2vec/tree/rich-context</a></p>
</div>
</div>
<div class="section" id="dataset-extraction">
<h2>Dataset Extraction<a class="headerlink" href="#dataset-extraction" title="Permalink to this headline">¶</a></h2>
<p>For identifying the datasets in a publication, we followed two
approaches and later combined results from both. Both the approaches
have been described below.</p>
<ol>
<li><p><strong>Simple Dataset Mention Search:</strong> We chose the dataset citations
from the given Dataset Vocabulary that occurred for one dataset only
and used these unique mentions to search for the corresponding
datasets using regular expressions in the text documents. Then, we
computed a frequency distribution of the datasets. As can be seen
from Figure [fig:graph], certain dataset citations occurred more
often than others. This is because while searching for dataset
citations, apart from the dataset title, the corresponding
mention_list from Dataset Vocabulary was also considered, which
contained many commonly occurring terms like ‘time’, ‘series’, ‘time
series’, ‘population’ etc. Therefore, we filtered out those dataset
citations that occurred more than a certain threshold value (=1.20)
multiplied by the median of the frequency distribution and that had
less than 3 distinct mentions in a publication. The remaining
citations were written to an interim result file.
Table [tab:simple] depicts the improvement in performance of
Simple Dataset Mention Search with the inclusion of filtering. The
filtering process improved the F1-measure by 42.86%. Note, as the
validation data consisted of only 100 articles, changing the
threshold value to 1.10 or 1.30 didn’t result in any significant
change, hence we have maintained a constant threshold value of 1.20
in our comparison table.</p>
<p>[C[1.5cm]{} C[3.5cm]{} C[3.5cm]{} C[3.5cm]{}]{} <strong>Metrics</strong> &amp;
<strong>without filtering</strong> &amp; <strong>Threshold=1.20, mentions $&lt;$ 3</strong> &amp;
<strong>Threshold=1.20, mentions $&lt;$ 4</strong>Precision &amp; 0.09 &amp; 0.71 &amp; 0.09Recall &amp; 0.28 &amp; 0.12 &amp; 0.28F1-score &amp; 0.14 &amp; <strong>0.20</strong> &amp; 0.14</p>
<p><img alt="_images/freq.pdf" src="_images/freq.pdf" />Frequency Distribution of Dataset
Citations[]{data-label=&quot;fig:graph&quot;}</p>
</li>
<li><p><strong>Rasa-based Dataset Detection:</strong> In our second approach, we trained
an entity extraction model based on conditional random fields (CRF)
using Rasa NLU [&#64;DBLP:journals/corr/abs-1712-05181]. For training
the model we used the Spacy Tokenizer<a class="reference external" href="https://spacy.io/api/tokenizer">^9</a> for the preprocessing
step. For Entity Recognition we used BILOU tagging and used 50
iterations to train the CRF. We used the Part of Speech tags, the
case of the input tokens and the suffixes of the tokens as input
features for the CRF model. We particularly tested two
configurations for training the CRF-based Named Entity Recognition
(NER) model. In Phase-1, the 2500 labeled publications from the
training dataset were used for training the Rasa NLU<a class="reference external" href="https://rasa.com/docs/nlu">^10</a> model.
Later in Phase-2, when the Phase-1 holdout corpus was released, we
combined its 5000 labeled publications with the previously given
2500 labeled publications and then retrained the model again with
these 7500 labeled publications.<strong>Running the CRF-Model:</strong> The trained model was run against the
preprocessed data to detect dataset citations and mentions. Only the
entities that had a confidence score greater than a certain
threshold value (= 0.72) were considered as dataset mentions. A
dataset mention was considered as a citation only if it was found in
the given Dataset Vocabulary (via string matching either with a
dataset title or any of the terms in a dataset ‘mention_list’) and
if it belonged to the research field of the article. To check if a
dataset belonged to the field of research, we found the cosine
similarity of the terms in the ‘subjects’ field of the Dataset
Vocabulary with the keywords and the identified Research Field of
the article.</p></li>
<li><p><strong>Combining the two approaches:</strong> The output generated by the
Rasa-based approach was first checked for irrelevant citations
before a union was performed to combine the results. This was done
by checking if a given dataset_id occured more than a threshold
value (= 1.20) multiplied by the median of the frequency
distribution (same as the filtering process of the Simple Dataset
Mention Search).</p></li>
</ol>
<p>Note that, the threshold values mentioned above were set after some
experiments of trial and testing. For dataset extraction, the goal was
to keep the number of false positives low while not compromising the
true positives. For research methods and fields, a manual evaluation
(see the next section for details) was done to test if the results made
sense with the articles.</p>
</div>
</div>
<div class="section" id="evaluation">
<h1>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">¶</a></h1>
<p>We performed a quantitative evaluation for Dataset Extraction using the
evaluation script provided by the competition organizers (refer Chapter
5 for more details). This evaluation (see Table [tab:dataset]) was
carried out against the validation data, wherein we compared four
different configurations. As can be inferred from the table, there was
only a slight increase in performance for the Rasa-based model, when the
training samples were increased. However, combining it with the Simple
Dataset Mention Search, increased the performance by <em>19.42%</em>.
Interestingly, there was no improvement in performance in the combined
approach even when the training samples for the Rasa-based model were
increased. This might be because of the removal of frequently-occuring
terms from the Rasa-generated output, based on the frequency
distribution of dataset mentions as computed in the Simple Dataset
Mention Search.</p>
<p>[ M[2.2cm]{} | M[2.3cm]{} | M[2.2cm]{} M[2.2cm]{} M[2.2cm]{} ]{} &amp; &amp;<strong>Metrics</strong>&amp; <strong>Rasa-based Approach</strong> (2500) &amp; <strong>Rasa-based Approach</strong>
(7500) &amp; <strong>Combined Approach</strong> (2500) &amp; <strong>Combined Approach</strong> (7500)<strong>Precision</strong> &amp; 0.382 &amp; 0.388 &amp; <strong>0.456</strong> &amp; <strong>0.456</strong><strong>Recall</strong> &amp; 0.26 &amp; 0.26 &amp; <strong>0.31</strong> &amp; <strong>0.31</strong><strong>F1</strong> &amp; 0.309 &amp; 0.311 &amp; <strong>0.369</strong> &amp; <strong>0.369</strong></p>
<p>For Research Fields and Methods, we carried out a qualitative evaluation
against 10 randomly selected articles from Phase-1 holdout corpus.
Tables [tab:field] and [tab:method] depict a comparison between the
predicted fields and methods in Phase-1 and Phase-2. In general, our
models returned a more granular output in the second phase, solely
because of the modifications we made in the vocabularies.</p>
<p>[C[1cm]{} C[4.5cm]{} C[3cm]{} C[3.5cm]{}]{} <strong>pubid</strong> &amp; <strong>Keywords</strong> &amp;
<strong>Phase-1</strong> &amp; <strong>Phase-2</strong>10328 &amp; Cycling for transport, leisure and sport cyclists &amp; Health
evaluation &amp; <strong>Public health and health promotion</strong>7270 &amp; Older adult drug users, harm reduction &amp; Health Education &amp;
<strong>Correctional health care</strong>6053 &amp; Economic conditions - crime relationship, homicide &amp; Homicide &amp;
<strong>Gangs and crime</strong></p>
<p>[C[1cm]{} C[4.5cm]{} C[3cm]{} C[3.5cm]{}]{} <strong>pubid</strong> &amp; <strong>Keywords</strong> &amp;
<strong>Phase-1</strong> &amp; <strong>Phase-2</strong>10328 &amp; Thematic content analysis &amp; Thematic analysis &amp; <strong>Sidak
correction</strong>7270 &amp; Interviews conducted face to face, finding systematic patterns or
relationships among categories identified by reading the interview
transcript &amp; Qualitative interviewing &amp; <strong>Sampling design</strong>6053 &amp; Autoregressive integrated moving average (ARIMA) time-series
model &amp; Methodological pluralism &amp; <strong>Multivariate statistics</strong></p>
</div>
<div class="section" id="discussion">
<h1>Discussion<a class="headerlink" href="#discussion" title="Permalink to this headline">¶</a></h1>
<p>Throughout the course of this competition, we encountered several
challenges and limitations in all the three stages of the pipeline. In
the preprocessing step, the appropriate extraction of text from PDFs
turned out to be rather challenging. This was especially due to the
varied formats of the publications, which made the extraction of
specific sections—that contained all data relevant to our
work—demanding. As mentioned before, if there was no explicit mention of
the key-terms like
<code class="docutils literal notranslate"><span class="pre">Abstract,</span> <span class="pre">Keywords,</span> <span class="pre">Introduction,</span> <span class="pre">Methodology/Data,</span> <span class="pre">Summary,</span> <span class="pre">Conclusion</span></code>
in the text, then the content was saved as ‘reduced_content’ after
applying all other preprocessing steps and filtering out any irrelevant
data.Our experiments suggest that the labeled publications we received for
dataset detection were not uniform in the dataset mentions provided,
which made it difficult to train an entity extraction model even with an
increased number of training samples. Hence, there was only a slight
improvement in performance when the Rasa-model was trained with 7500
publications instead of 2500. This was also why we combined the
Rasa-based approach with the Simple Dataset Mention Search, so that at
least the datasets that were present in the vocabulary do not get
missed.</p>
<p>Regarding the fields and methods, vocabularies played an immense role in
their identification. The vocabularies that were provided by the SAGE
publications contained some terms that were either polysemous or very
high-level and therefore, were picked up by our model very often. Hence,
for research methods, we created our own vocabulary containing all the
relevant statistical methods, and for fields, we introduced a
stopword-list of irrelevant terms and looked it up each time, before
writing the result to the output file. The goal of stopword-list
generation was to filter the terms that did not carry domain-specific
information and sounded more like research methods than fields. Since
the focus was on more granulated results, we tried to look for open
ontologies for Social Science Fields and Methods and unfortunately,
could not find any. It is worth mentioning that since our approach for
Fields and Methods identification relied heavily upon vocabularies, it
could not find any new methods or fields from the publications.</p>
<p>Based on the final evaluation feedback, since our Phase-2 models did not
perform as good as we expected, following are a few things that we could
have done differently.</p>
<ol class="simple">
<li><p>For research methods, merging the given SAGE methods vocabulary with
our manually curated vocabulary, could have resulted in methods that
would have been both granular and statistical while still being
relevant to the publications. Introducing a stopword-list just as we
did for research field identification, could also have been another
workaround.</p></li>
<li><p>For both fields and methods identification, we could have also tried
pre-trained embeddings from glove<a class="reference external" href="https://nlp.stanford.edu/projects/glove">^11</a> and fastText<a class="reference external" href="https://fasttext.cc/docs/en/crawl-vectors.html">^12</a>.</p></li>
<li><p>As our entity-extraction approach for Dataset Detection suffered
from a limitation of inconsistent labels (i.e. datasets mentioned in
the form of abbreviation, full-name, collection procedure, and
keywords) in training data, we could have extended the Simple
Dataset Mention Search to a pattern-oriented search based on
handcrafted rules derived from sentence structure and other
heuristics.</p></li>
</ol>
</div>
<div class="section" id="future-agenda">
<h1>Future Agenda<a class="headerlink" href="#future-agenda" title="Permalink to this headline">¶</a></h1>
<p>The data provided to us in the competition displayed a cornucopia of
inconsistencies even after human processing. We hence propose that
machine-aided methods for computing correct and complete structured
representation of publications are of central importance for scientific
research such as an Open Research Knowledge
Graph [&#64;DBLP:journals/corr/abs-1901-10816][&#64;DBLP:conf/esws/BuscaldiDMOR19].
Previous works on never-ending learning have shown how humans and
extraction algorithms can work together to achieve high-precision and
high-recall knowledge extraction from unstructured sources. In our
future work, we hence aim to populate a <strong>scientific knowledge graph</strong>
based on never-ending learning. The methodology we plan to develop will
be domain-independent and rely on active learning to classify, extract,
link and publish scientific research artifacts extracted from
open-access papers. Inconsistency will be remedied by ontology-based
checks learned from other publications such as SHACL constraints which
can be manually or automatically added.<a class="reference external" href="https://www.w3.org/TR/shacl/">^13</a> The resulting graphs will</p>
<ul class="simple">
<li><p>rely on advanced distributed storage for RDF to scale to the large
number of publications available;</p></li>
<li><p>be self-feeding, i.e., crawl the web for potentially relevant
content and make this content available for processing;</p></li>
<li><p>be self-repairing, i.e., be able to update previous extraction
results based on insights gathered from new content;</p></li>
<li><p>be weakly supervised by humans (e.g. authors of publications), who
would assist in correcting wrong hypotheses, thereby leveraging
semi-supervised learning;</p></li>
<li><p>provide standardized access via W3C Standards such as SPARQL.</p></li>
</ul>
<p>Having such knowledge graphs would make it easier for the researchers
(both young and veteran) to easily follow along with their domain of
fast-paced research and eliminate the need to manually update the
domain-specific ontologies for fields, methods and other metadata as new
research innovations come up.</p>
</div>
<div class="section" id="appendix">
<h1>Appendix<a class="headerlink" href="#appendix" title="Permalink to this headline">¶</a></h1>
<p>The code and documentation for all our submissions can be found here:
<a class="reference external" href="https://github.com/dice-group/rich-context-competition">https://github.com/dice-group/rich-context-competition</a>.</p>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Rich Search and Discovery for Research Datasets</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="chap00.html">Contributor Bios</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap01.html">Chapter 1 - Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap01.html#introduction">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap01.html#how-this-book-came-to-be">How this book came to be</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap01.html#book-overview">Book overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap01.html#section-2">Section 2:</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap01.html#section-3">Section 3:</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap01.html#section-4-looking-forward">Section 4: Looking forward</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap01.html#more-resources">More resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap01.html#references">References</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap02.html">Chapter 2 - Bundesbank</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap02.html#building-blocks-for-user-centric-data-services-usage-data-to-support-empirical-research">Building blocks for user-centric data-services: Usage data to support empirical research</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap03.html">Chapter 3 - Digital Science Use Cases</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap03.html#chapter-3-digital-science-use-cases-enriching-context-and-enhancing-engagement-around-datasets">Chapter 3 – Digital Science Use Cases: Enriching context and enhancing engagement around datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap04.html">Chapter 4 - Metadata for Social Science Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap04.html#introduction">INTRODUCTION</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap04.html#data-elements-and-datasets">DATA ELEMENTS AND DATASETS</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap04.html#metadata-schemas-and-catalogs">METADATA SCHEMAS AND CATALOGS</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap04.html#linked-data">LINKED DATA</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap04.html#richer-semantics">RICHER SEMANTICS</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap04.html#data-repositories-and-collections-of-datasets">DATA REPOSITORIES AND COLLECTIONS OF DATASETS</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap04.html#repository-services">REPOSITORY SERVICES</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap04.html#infrastructure">INFRASTRUCTURE</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap04.html#conclusion">CONCLUSION</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap04.html#acknowledgments">ACKNOWLEDGMENTS</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap04.html#references-references-listparagraph">REFERENCES {#references .ListParagraph}</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap05.html">Chapter 5 - Compettion Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap05.html#introduction">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap05.html#competition-design">Competition Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap05.html#data">Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap05.html#submission-process">Submission Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap05.html#evaluation">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap05.html#discussion">Discussion</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap05.html#conclusion">Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap05.html#appendix-standardized-metadata-full-text-and-training-evaluation-for-extraction-models">Appendix - Standardized Metadata, Full Text and Training/Evaluation for Extraction Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap05.html#references">References</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html">Chapter 6 - Finding datasets in publications: The Allen Institute for Artificial Intelligence approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html#introduction">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html#methods">Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html#results">Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html#conclusion">Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html#acknowledgments">Acknowledgments</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html#references">References</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html#footnotes">Footnotes</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html#appendix">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html">Chapter 7 - Finding datasets in publications: The KAIST approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#non-technical-overview">Non-technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#literature-review">Literature Review</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#what-did-you-do">What did you do</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#what-worked-and-what-didnt">What worked and what didn’t</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#summary-of-your-results-and-caveats">Summary of your results and caveats</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#lessons-learned-and-what-would-you-do-differently">Lessons learned and what would you do differently</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#conclusion">Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#what-comes-next">What comes next</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#acknowledgments">Acknowledgments</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#appendix-description-of-the-code-and-documentation">Appendix: Description of the code and documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap08.html">Chapter 8 - Knowledge Extraction from scholarly publications: The GESIS contribution to the Rich Context Competition</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap08.html#knowledge-extraction-from-scholarly-publications-the-gesis-contribution-to-the-rich-context-competition">Knowledge Extraction from scholarly publications - The GESIS contribution to the Rich Context Competition</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Chapter 9 - Finding datasets in publications: The University of Paderborn approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="#abstract">Abstract</a></li>
<li class="toctree-l1"><a class="reference internal" href="#literature-review">Literature Review</a></li>
<li class="toctree-l1"><a class="reference internal" href="#project-architecture">Project Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="#preprocessing">Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="#approach">Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="#evaluation">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="#discussion">Discussion</a></li>
<li class="toctree-l1"><a class="reference internal" href="#future-agenda">Future Agenda</a></li>
<li class="toctree-l1"><a class="reference internal" href="#appendix">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap10.html">Chapter 10 - Finding datasets in publications: The Singapore Management University approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap10.html#finding-datasets-in-publications-the-singapore-management-university-approach">Finding datasets in publications: The Singapore Management University approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html">Chapter 11 - Finding datasets in publications: The University of Syracuse approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html#abstract">Abstract</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html#introduction">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html#the-dataset">The dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html#the-proposed-method">The Proposed Method</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html#results">Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html#conclusion">Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html#acknowledgements-acknowledgements-unnumbered-unnumbered">Acknowledgements {#acknowledgements .unnumbered .unnumbered}</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap12.html">Chapter 12 - The future of context</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap12.html#the-future-of-ai-in-rich-context">The Future of AI in Rich Context</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="chap08.html" title="previous chapter">Chapter 8 - Knowledge Extraction from scholarly publications: The GESIS contribution to the Rich Context Competition</a></li>
      <li>Next: <a href="chap10.html" title="next chapter">Chapter 10 - Finding datasets in publications: The Singapore Management University approach</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, NYU.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/chap09.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>