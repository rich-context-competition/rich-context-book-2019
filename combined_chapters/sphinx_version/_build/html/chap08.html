
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Chapter 8 - Knowledge Extraction from scholarly publications: The GESIS contribution to the Rich Context Competition &#8212; Rich Search and Discovery for Research Datasets 1.0 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Chapter 9 - Finding datasets in publications: The University of Paderborn approach" href="chap09.html" />
    <link rel="prev" title="Chapter 7 - Finding datasets in publications: The KAIST approach" href="chap07.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <hr class="docutils" />
<div class="section" id="chapter-8-knowledge-extraction-from-scholarly-publications-the-gesis-contribution-to-the-rich-context-competition">
<h1>Chapter 8 - Knowledge Extraction from scholarly publications: The GESIS contribution to the Rich Context Competition<a class="headerlink" href="#chapter-8-knowledge-extraction-from-scholarly-publications-the-gesis-contribution-to-the-rich-context-competition" title="Permalink to this headline">¶</a></h1>
</div>
<div class="section" id="knowledge-extraction-from-scholarly-publications-the-gesis-contribution-to-the-rich-context-competition">
<h1>Knowledge Extraction from scholarly publications - The GESIS contribution to the Rich Context Competition<a class="headerlink" href="#knowledge-extraction-from-scholarly-publications-the-gesis-contribution-to-the-rich-context-competition" title="Permalink to this headline">¶</a></h1>
<p><strong>Authors:</strong> <em>Wolfgang Otto, Andrea Zielinski, Behnam Ghavimi, Dimitar
Dimitrov, Narges Tavakolpoursaleh, Karam Abdulahhad, Katarina Boland,
Stefan Dietze</em></p>
<p><strong>Affiliation:</strong> <em>GESIS – Leibniz Institute for the Social Sciences,
Cologne, Germany</em></p>
<p><strong>Corresponding author:</strong> <em>wolfgang.otto&#64;gesis.org</em></p>
<div class="section" id="introduction">
<h2>1. Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>GESIS - the Leibniz Institute for the Social Sciences (GESIS)[1] is
the largest European research and infrastructure provider for the social
sciences and offers research data, services and infrastructures
supporting all stages of the scientific process. The GESIS department
<em>Knowledge Technologies for the Social Sciences (WTS)</em>[2] is
responsible for developing all digital services and research data
infrastructures at GESIS and aims at providing integrated access to
social sciences data and services. Next to traditional social sciences
research data, such as surveys and census data, an emerging focus is to
build data infrastructures able to exploit novel forms of social
sciences research data, such as large Web crawls and archives.</p>
<p>Research at WTS[3] addresses areas such as Information Retrieval (IR),
Information Extraction (IE) &amp; Natural Language
Processing (NLP), semantic technologies and human computer interaction
and aims at ensuring access and use of social sciences research data
along the FAIR principles, for instance, through interlinking of
research data, established vocabularies and knowledge graphs and by
facilitating semantic search across distinct platforms and datasets. Due
to the increasing importance of Web- and W3C standards as well as
Web-based research data platforms, in addition to traditional research
data portals, findability and interoperability of research data across
the Web constitutes one current challenge. In the context of Web-scale
reuse of social sciences resources, the extraction of structured data
about scholarly entities such as datasets and methods from unstructured
and semi-structured text, as found in scientific publications or
resource metadata, is crucial in order to be able to uniquely identify
social sciences resources and to understand their inherent relations.</p>
<p>Prior works at WTS/GESIS addressing such challenges apply NLP and
machine learning techniques to, for instance, extract and disambiguate
mentions of datasets[4] (Boland et al., 2012; Ghavimi et al., 2016)),
authors (Backes, 2018a, 2018b) or software tools (Boland and Krüger,
2019) from scientific publications or to extract and fuse scholarly data
from large-scale Web crawls (Sahoo et al., 2017; Yu et al., 2019).
Resulting pipelines and data are used to empower scholarly search
engines such as the <em>GESIS-wide search</em>[5] (Hienert et al., 2019)
which provides federated search for scholarly resources (datasets,
publications etc.) across a range of GESIS information systems or the
<em>GESIS DataSearch</em> platform[6] (Krämer et al., 2018), which enables
search across a vast number of social sciences research datasets mined
from the Web.</p>
<p>Given the strong overlap of our research and development profile with
the recent initiatives of the Coleridge Initiative to evolve this
research field through the Rich Context Competition (RCC)[7], we are
enthusiastic about having participated in the RCC2018 and are looking
forward to continue this collaboration towards providing sound
frameworks and tools which automate the process of interlinking and
retrieving scientific resources.</p>
<p>The central tasks in the RCC are the extraction and disambiguation of
mentions of datasets and research methods as well as the classification
of scholarly articles into a discrete set of research fields. After the
first phase, each team received feedback from the organizers of the RCC
consisting of a quantitative and qualitative evaluation. Whereas
quantitative results of our inital contribution throughout phase one
have shown significant room for improvement, the qualitative
assessement, conducted by four judges on a sample of ten documents,
underlined the potential of our approach.</p>
<p>Since we have been shortlisted for the second phase of the RCC, this
chapter describes our approaches, techniques, and additional data used
to address all three tasks. As described in the following subsections,
we decided to follow a module-based approach where each module or the
entire pipeline can be reused. The remaining chapter is organised as
follows. The following Section 2 provides an overview
of our approach, used background data and preprocessing steps, whereas
Sections 3, 4, and 5 describe our approaches in more
detail, including results towards each of the tasks. Finally, we discuss
our results in Section 6 and provide an overview of
future work in Section 7.</p>
</div>
<div class="section" id="approach-data-and-pre-processing">
<h2>2. Approach, data and pre-processing<a class="headerlink" href="#approach-data-and-pre-processing" title="Permalink to this headline">¶</a></h2>
<p>This section describes the external data sources we used as well as our
pre-processing steps.</p>
<div class="section" id="approach-overview-and-initial-evaluation-feedback">
<h3>2.1 Approach overview and initial evaluation feedback<a class="headerlink" href="#approach-overview-and-initial-evaluation-feedback" title="Permalink to this headline">¶</a></h3>
<p>The central tasks in the RCC are the extraction of dataset mentions from
text. Even so, we considered the discovery of research methods and
research fields important. To this end, we decided to follow a
module-based approach. Users could choose to use each specific module
solely or as parts of a data processing pipeline.
Figure 8.2 shows an overview of
modules developed and their dependencies. Here, the upper three modules
(which are in gray) describe the pre-processing steps (cf.
Section 2.3). The lower four modules (blue) are used
to generate the output in a predefined format as specified by the
competition.</p>
<p><img alt="_images/information-flow.png" src="_images/information-flow.png" />image
Figure 8.1: An overview of the individual software modules described in this
document and their dependencies. Gray: Our pre-processing pipeline. Blue: three main tasks of the RCC.</p>
<p>The pre-processing step consists of extracting metadata and raw text
from PDF documents. The output of this step is then used by the software
modules responsible for tackling the individual sub-tasks. These
sub-tasks are to discover research datasets (cf. Section 3),
methods (cf. Section 4) and fields (cf. Section 5). First, a Named Entity
Recognition module is used to find dataset mentions. This module used a
supervised approach trained on a weakly labled corpus. In the next step,
we combine all recognized mentions for each publication and compare
these mentions to the metadata from the list of datasets given by the
competition. For this linking step the mentions and year information
located in the same sentence are used. The corresponding sentence and
extracted information are saved for debugging and potential usage in
future pipeline components. The task of identifying research methods is
solved unsing a Named Entity Recognition and Linking module with
incorporated word embeddings and lexical resources. For identifying
research fields, we trained a classifier on openly available abstracts
and metadata from the domain of social sciences crawled from the Social
Science Open Access Repository[8] (SSOAR). We tried different
classifiers and selected the best performing one, a classifier based on
fasttext[9], i.e. a neural net based approach with a high
performance(Joulin et al., 2017).</p>
<p>After the first phase, each team received feedback from the organizers
of the RCC. The feedback is two folds, a quantitative and qualitative
evaluation. Unfortunately, the quantitative assessment showed our
algorithm for dataset mention retrieval did not perform well regarding
precision and recall metrics. In contrast to this, our approach has been
found convincing regarding the quality of results. The qualitative
feedback is based on a random sample of ten documents given to four
judges. The judges were asked to manually extract dataset mentions.
After this the overlap between their dataset extractions and the output
of our algorithm was calculated. Other factors that judges took into
consideration are specificity, uniqueness, and multiple occurrences of
dataset mentions. As for the extraction of research methods and fields,
no ground truth has been provided; these tasks were evaluated against
the judges’ expert knowledge. Similarly to the extraction of dataset
mentions, specificity and uniqueness have been considered for these two
tasks. The feedback our team received was overall positive.</p>
</div>
<div class="section" id="external-data-sources">
<h3>2.2 External data sources<a class="headerlink" href="#external-data-sources" title="Permalink to this headline">¶</a></h3>
<p>For developing our algorithms, we utilized two external data sources.
For the discovery of research methods and fields, we resort to data from
the Social Science Open Access Repository[10] (SSOAR). GESIS – Leibniz
Institute for the Social Sciences maintains SSOAR by collecting and
archiving literature of relevance to the social sciences.</p>
<p>In SSOAR, full texts are indexed using controlled social science
vocabulary (Thesaurus[11], Classification[12]) and are assigned rich
metadata. SSOAR offers documents in various languages. The corpus of
English language publications that can be used for purposes of the
competition consists of a total of 13,175 documents. All SSOAR documents
can be accessed through the OAI-PMH[13] interface.</p>
<p>Another external source we have used to discover research methods is the
ACL Anthology Reference Corpus (Bird et al., 2008). ACL ARC is a corpus
of scholarly publications about computational linguistics. The corpus
consists of a total of 22,878 articles.</p>
</div>
<div class="section" id="pre-processing">
<h3>2.3 Pre-processing<a class="headerlink" href="#pre-processing" title="Permalink to this headline">¶</a></h3>
<p>Although the organizers of the RCC offered plain texts for the
publication, we decided to build our own pre-processing pipeline. The
extraction of text from PDF files is still an error prone process. To
handle de-hyphenation and paragraph segmentation during extraction time
and benefit from automatic metadata extraction (i.e. title, author,
abstracts and references) we decided to use a third party extraction
tool. The Cermine Extraction Tool[14](Tkaczyk et al., 2015) transforms
the files into XML documents using the Journal Article Tag
Suite[15](Jats). For the competition we identified two interesting
elements of the Jats XML format, i.e., (&lt;)front(&gt;) and
(&lt;)body(&gt;). The (&lt;)front(&gt;) element contains the metadata of the
publication, whereas the (&lt;)body(&gt;) contains the main textual and
graphic content of the publication. As a last step of the
pre-processing, we removed all linebreaks from the publication. The
output of this step is a list of metadata fields and values, as shown in
Table 8.1 for each publication paragraph.</p>
<p>|                   | Example Text Field Data         |
| :—————- | :—————————— |
| publication_id   | 12744                           |
| label             | paragraph_text                 |
| text              | A careful reading of text, word |
|                   | for word, was …               |
| section_title    | Data Analysis                   |
| annotations       | [{’start’: 270, ’end’: 295,    |
|                   | ’type’: ’bibref’, …           |
| section_nr       | [3, 2]                        |
| text_field_nr   | 31                              |
| para_in_section | 1                               |</p>
<p>Table 8.1: Example preprocessing output for a paragraph in a given publication.</p>
</div>
</div>
<div class="section" id="dataset-extraction">
<h2>3. Dataset extraction<a class="headerlink" href="#dataset-extraction" title="Permalink to this headline">¶</a></h2>
<div class="section" id="task-description">
<h3>3.1 Task description<a class="headerlink" href="#task-description" title="Permalink to this headline">¶</a></h3>
<p>In the scientific literature, datasets are cited to reference, for
example, the data on which an analysis is performed or on which a
particular result or claim is based. In this competition, we focus on
(i) extracting and (ii) disambiguating dataset mentions from social
science publications to a list of given dataset references. Identifying
dataset mentions in literature is a challenging problem due to the huge
number of styles of citing datasets. Although there are proposed
standards for dataset citation in full-texts, researchers still ignore
or neglect such standards (see, e.g., (Altman and King, 2007)).
Furthermore, in many research publications, a correct citation of
datasets is often missing (Boland et al., 2012). The following two
sentences exemplify the problem of the usage of an abbreviation to make
a reference to an existing dataset. The first example illustrates the
use of abbreviations that are known mainly in the author’s research
domain. The latter illustrates the ambiguity of abbreviations. In this
case, <em>WHO</em> identifies a dataset published by the World Health
Organization and does not refer to the institution itself.
<strong>Example 1</strong>: <em>P-values are reported for the one-tail paired t-test on
<em>Allbus</em> (dataset mention) and <em>ISSP</em> (dataset mention).</em>
<strong>Example 2</strong>: <em>We used <em>WHO data from 2001</em> (dataset mention) to
estimate the spreading degree of AIDS in Uganda.</em>
We treat the problem of detecting dataset mentions in full-text as a
Named Entity Recognition (NER) task.</p>
<div class="section" id="formal-problem-definition">
<h4>Formal problem definition<a class="headerlink" href="#formal-problem-definition" title="Permalink to this headline">¶</a></h4>
<p>Let (D) denote a set of existing datasets (d) and the knowledge base
(K) as a set of known dataset references (k). Furthermore, each
element of (K) is referencing an existing dataset (d). The Named
Entity Recognition and Linking task is defined as (i) the identification
of dataset mentions (m) in a sentence, where (m) references a
dataset (d) and (ii) linking them, when possible, to one element in
(K) (i.e., the reference dataset list given by the RCC).</p>
</div>
</div>
<div class="section" id="challenges">
<h3>3.2 Challenges<a class="headerlink" href="#challenges" title="Permalink to this headline">¶</a></h3>
<p>We focus on the extraction of dataset mentions in the body of the
full-text of scientific publications. There are three types of dataset
mentions: (i) The full name of a dataset (”National Health and Nutrition
Examination Survey“), (ii) an abbreviation (”NHaNES“) or (iii) a vague
reference, e.g., ”the monthly statistic“. With all these these types,
the NER task faces special challenges. In the first case, the used
dataset name can vary in different publications. For instance one
publication cites the dataset with ”National Health and Nutrition
Examination Survey“ the other could use the words ”Health and Nutrition
Survey“. In the case where abbreviations are used, a disambiguation
problem occurs, e.g., in ”WHO data“. WHO may describe the World Health
Organization or the White House Office. In the case, that an
abbreviation is used after the dataset name has been written in full,
the mapping between these different spellings in one text is referred to
as Coreference Resolution. The biggest challenge is again the lack of
annotated training data. In the following we describe how we have dealt
with this lack of ground truth data.</p>
</div>
<div class="section" id="phase-one-approach">
<h3>3.3 Phase one approach<a class="headerlink" href="#phase-one-approach" title="Permalink to this headline">¶</a></h3>
<p>Missing ground truth data is the main problem to handle during this
competition. To this end, supervised learning methods for dataset
mentions extraction from texts are not applicable without the
identification of external training data or the creation of useful
labeled training data from information given by the competition. Because
of the lack of existing training data for the task of dataset mention
extraction we resort to the provided list of dataset mentions and
publication pairs and re-annotate the particular sentences in the
publication text. A list of dataset identifying words is provided for
some of the known links between publications and datasets by the
competition. These words represent the evidence of the linkage between
publication and datasets and are extracted from the publication text. In
the course of re-annotation, we search for each of the identifying words
in the corresponding publication texts. For each match, we annotate the
occurence in our raw text and use these annotations as ground truth. As
described in the pre-processing section, our units for processing the
publication text are paragraphs. The re-annotated corpus consists of a
list of paragraphs for each publication with stand-off annotations
identifying the mentions of datasets (i.e. position of the start and end
characters and the entity type for each mention: <em>dataset</em>). This
re-annotation is then used to train Spacy’s neural network-based NER
model[16]. We created a holdout set of 1,000 publications and a
training set of size 4,000. Afterwards, we train our model with the
paragraphs as a sampling unit. In the training set, 0.45 percent of the
paragraphs contained mentions. For each positive training example, we
have added one negative sample that contains no known dataset mentions
and is randomly selected. We used a batch size of 25 and a dropout rate
of 0.4. The model was trained for 300 iterations.</p>
<div class="section" id="evaluation">
<h4>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">¶</a></h4>
<p>We evaluated our model with respect to four metrics: precision and
recall, each for strict and for partial match. While the strict match
metrics are standard evaluation metrics, the partial match metrics are
their relaxed variants in which the degree to which dataset mentions
have to match can vary. Consider the following partial match example:
“National Health and Nutrition Examination Survey” is the extracted
dataset mention, while “National Health and Nutrition Examination Survey
(NHANES)” is the true dataset mention. In contrast to the strict version
of the metrics, this overlapping match is considered a match for the
partial version. The scores describe whether a model is able to find the
correct positions of dataset mentions in the texts, even if the start
and end positions of the characters are not the same, but the ranges
overlap.</p>
<p>| Metric                    | Value |
| :———————— | :—: |
| Precision (partial match) | 0.93  |
| Recall (strict match)     | 0.95  |
| Precision (strict match)  | 0.80  |
| Recall (strict match)     | 0.81  |</p>
<p>Table 8.2: Performance of phase one approach of dataset extraction.</p>
<p>Table 8.2 shows the results of the dataset mention extraction on the holdout set. The
model can achieve high strict precision and recall values. As expected,
the results are even better for the partial version of the metrics. It
means that even if we couldn’t match the dataset mention in a text
exactly, we can find the right context with very high precision.</p>
</div>
</div>
<div class="section" id="phase-two-approach">
<h3>3.4 Phase two approach<a class="headerlink" href="#phase-two-approach" title="Permalink to this headline">¶</a></h3>
<p>In the second phase of the competition, additional 5,000 publications
were provided by RCC. We extended our approach to consider the list with
dataset names supplied by the organizers and re-annotated the complete
corpus of 15,000 publications in the same manner as in phase one to
obtain training data. This time we split the data in 80% for training
and 20% for test.</p>
<div class="section" id="id1">
<h4>Evaluation<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>We resort to the same evaluation metrics as in phase one. However, we
calculate precision and recall on the full-text of the publication and
not on the paragraphs as in the first phase.
Table 8.3 shows the results achieved by our model. We observe lower precision and
recall values. Compared to phase one, there is also a smaller difference
between the precision and recall values for the strict and partial
version of the metrics.</p>
<p>| Metric                    | Value |
| :———————— | :—: |
| Precision (partial match) | 0.51  |
| Recall (partial match)    | 0.90  |
| Precision (strict match)  | 0.49  |
| Recall (strict match)     | 0.87  |</p>
<p>Table 8.3: Performance of phase two approach for dataset extraction.</p>
</div>
</div>
</div>
<div class="section" id="research-method-extraction">
<h2>4. Research method extraction<a class="headerlink" href="#research-method-extraction" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id2">
<h3>4.1 Task description<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>Inspired by a recent work of Nasar et al. (Nasar et al., 2018), we
define a list of basic entity types that give key-insights into
scholarly publications. We adapted the list of semantic entity types to
the domain of the social sciences with a focus on <em>research methods</em>,
but also including related entity types such as <em>theory, todel,
measurement, tool, performance</em>. We suspect that the division into
semantic types might be helpful to find <em>research methods</em>. The reason
is that the related semantic entities types might provide clues or might
be directly related to the research method itself.</p>
<p>For example, in order to achieve a certain research goal, an experiment
is used in which a certain combination of <em>methods</em> is applied to a
<em>dataset</em>. The methods can be specified as concepts or indirectly
through the use of certain <em>software</em>. The result is then quantified
with a <em>performance</em> using a specific measure.</p>
<p><strong>Example</strong>: <em>P-values</em> (measurement) are reported for the <em>one-tail
paired t-test</em> (method) on <em>Allbus</em> (dataset) and <em>ISSP</em> (dataset).
We selected the entity types <em>research method</em>, <em>research theory</em>,
<em>research tool</em> and <em>research measurement</em> as the target research method
related entity types (see Table 8.4). This decision is based
on an ecxamination of the SAGE ontology given by the RCC as a sample of
how research method terms might look like.</p>
<p>|                            |                   |                                                  |
| :————————- | :—————- | :———————————————– |
| <strong>Entity type</strong>            | <strong>Corresponding</strong> | <strong>Examples</strong>                                     |
|                            | <strong>SAGE type</strong>     |                                                  |
| Research Method            | SAGE-METHOD       | Bootstrapping, Active Interviews                 |
| Research Measurement       | SAGE-MEASURE      | Latent Variables, Phi coefficient, Z-score       |
| Research Theory            | SAGE-THEORY       | Frankfurt school, Feminism, Actor network theory |
| Research Tool              | SAGE-TOOL         | SPSS, R statistical package                      |</p>
<p>Table 8.4: Entity types of relevance for the research method extraction task.</p>
<div class="section" id="id3">
<h4>Formal problem definition<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h4>
<p>The task of Named Entity Recognition and Linking is to (i) identify the
mentions (m) of research-related entities in a sentence and (ii) link
them, if possible, to a reference knowledge base (K) (i.e. the SAGE
Thesaurus[17]) or (iii) assign a type to each entity, e.g. a <em>research
method</em>, selected from a set of predefined types.</p>
</div>
</div>
<div class="section" id="id4">
<h3>4.2 Challenges<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>There are some major challenges that any named entity recognition,
classification and linking system needs to handle. First, regarding NER,
identifying the entities boundary is important, thus detecting the exact
sequence span. Second, ambiguity errors might arise in classification.
For instance,‘range’ might be a domain-specific term from the knowledge
base or belong to the general domain vocabulary. This is a challenging
task for which context information is required. In the literature, this
relates to the problem of <strong>domain adaptation</strong> which includes
fine-tuning to specific named entity classes[18]. With respect to
entity linking, another challenge is detecting name variations, since
entities can be referred to in many different ways. Semantically similar
words, synonyms or related words, which might be lexically or
syntactically different, are often not listed in the knowledge base
(e.g., the lack of certain terms like ‘questioning’ but not
‘questionnaire’). This problem of automatically detecting these
relationships is generally known as <strong>linking problem</strong>. Note that part
of this problem also results from PDF-to-text conversion which is
error-prone. Dealing with incomplete knowledge bases, i.e. <strong>handling of
out of vocabulary (OOV) items</strong>, is also a major issue, since knowledge
bases are often not exhaustive enough and do not cover specific terms or
novel concepts from recent research. Last but not least, the combination
of different semantic types gives a more coherent picture of a research
article. We hypothesize that such information would be helpful and
results in an insightful co-occurrence statistics, and provides
additional detail directly related to entity resolution, and finally
helps to assess the <strong>relevance of terms</strong> by means of a score.</p>
</div>
<div class="section" id="our-approach">
<h3>4.3 Our approach<a class="headerlink" href="#our-approach" title="Permalink to this headline">¶</a></h3>
<p>Our research method extraction tool builds on Stanford’s CoreNLP and
Named Entity Recognition System[19]. The information extraction
process follows the workflow depicted in
figure 8.2, using separate modules for
pre-processing, classification, linking and term filtering.</p>
<p><img alt="_images/research-methods-pipeline.png" src="_images/research-methods-pipeline.png" />image
Figure 8.2: Overview of the entity extraction pipeline.</p>
<p>We envision the task of finding entities in scientific publications as a
sequence labeling problem, where each input word is classified as being
of a dedicated semantic type or not. In order to handle entities related
to our domain, we train a CRF based machine learning classifier with
major semantic classes, (see Table 8.4, using training material
from the ACL RD-TEC 2.0 dataset (QasemiZadeh and Schumann, 2016). Apart
from this, we follow a domain adaptation approach inspired by (Agerri
and Rigau, 2016) and ingest semantic background knowledge extracted from
external scientific corpora, in particular the ACL Anthology (Bird et
al., 2008; Gildea et al., 2018). We perform entity linking by means of a
new gazetteer based on th SAGE dictionary of Social Research
Methods (Lewis-Beck et al., 2003), thus putting a special emphasis on
the social sciences. The linking component addresses the synonymy
problem and matches an entity despite name variations such as spelling
variations. Finally, term filtering is carried out based on termhood and
unithood, while scoring is achieved by calculating a relevance score
based on TF-IDF (cf Table 8.6).</p>
<p>Our research experiments are based on publications from the Social
Science Open Access Repository (SSOAR)[20] as well as the train and
test data of the Rich Context Competition corpus[21]. Our work extends
previous work on this topic (cf. (Eckle-Kohler et al., 2013)) in various
ways: First, we do not limit our study to abstracts, but use the entire
fulltext. Second, we focus on a broader range of semantic classes, i.e.
<em>Research Method</em>, <em>Research Theory</em>, <em>Research Tool</em> and <em>Research
Measurement</em>, tackling also the problem of identifying novel entities.</p>
<div class="section" id="distributed-semantic-models">
<h4>Distributed semantic models<a class="headerlink" href="#distributed-semantic-models" title="Permalink to this headline">¶</a></h4>
<p>For domain adaptation, we integrate further background knowledge. We use
topical information from word embeddings trained on an scientific corpus
as an additional feature to our NER model. For this, we use
agglomerative clustering of the word embeddings to identify topical
groups of words. The cluster number of each word is used as additional
sequential input feature for our CRF model. Semantic representations of
words are a successful extension of common features, resulting in higher
NER performance (Turian et al., 2010) and can be trained offline. In
this work, the word vectors were learned based on 22,878 documents of
the scientific ACL Anthology Reference Corpus[22] using Gensim[23]
with the skip-gram model (cf. (Mikolov et al., 2013)) and a
pre-clustering algorithm[24].</p>
</div>
<div class="section" id="features">
<h4>Features<a class="headerlink" href="#features" title="Permalink to this headline">¶</a></h4>
<p>The features incorporated into the linear chain CRF are shown in the
Table 8.5. The features depend mainly on
the observations and on pairs of adjacent labels, using a log-linear
combination. However, since simple token level training of CRFs leads to
poor performance, more effective text features such as word shape,
orthographic, gazetteer, Part-Of-Speech (POS) tags, along with word
clustering have been used.</p>
<p>| <strong>Type</strong>                 |                            <strong>Features</strong>                            |
| :———————– | :—————————————————————-: |
| <strong>Token unigrams</strong>       | (w_{i-2}), (w_{i-1}), (w_{i}), (w_{i+1}), (w_{i+2}), … |
| <strong>POS unigrams</strong>         |                (p_{i}), (p_{i-1}), (p_{i-2})                 |
| <strong>Shapes</strong>               |                      shape and capitalization                      |
| <strong>NE-Tag</strong>               |                      (t_{i-1}), (t_{i-2})                      |
| <strong>WordPair</strong>             |                 ((p_{i}), (w_{i}), (c_{i}))                  |
| <strong>WordTag</strong>              |                       ((w_{i}), (c_{i}))                       |
| <strong>Gazetteer</strong>            |                           SAGE Gazetteer                           |
| <strong>Distributional Model</strong> |                        ACL Anthology model                         |</p>
<p>Table 8.5: Features used for NER.</p>
</div>
<div class="section" id="knowledge-resources">
<h4>Knowledge resources<a class="headerlink" href="#knowledge-resources" title="Permalink to this headline">¶</a></h4>
<p>We use the SAGE thesaurus which includes well-defined concepts, an
explicit taxonomic hierarchy between concepts as well as labels that
specify synonyms of the same concept. A portion of terms is unique to
the social science domain (e. g., ‘dependent interviewing’), while
others are drawn from related disciplines such as statistics (e. g.,
‘conditional likelihood ratio test’)[25]. However, since the
thesaurus is not exhaustive and covers only the top-level concepts
related to social science methods, our aim was to extend it by
automatically extracting further terms from domain-specific texts, in
particular from the Social Science Open Access Repository. More
concretely, we carried out the following steps to extend SAGE as an
off-line step. For step 2 and 3, candidate terms have been extracted by
our pipeline for the entire SSOAR corpus.</p>
<ol class="simple">
<li><p>Assignment of semantic types to concepts (manual)</p></li>
<li><p>Extracting terms variants such as abbreviations, synonyms, related
terms from SSOAR (semi-automatic)</p></li>
<li><p>Computation of term and document frequency scores for SSOAR
(automatic)</p></li>
</ol>
</div>
<div class="section" id="extracting-term-variants-such-as-abbreviations-synonyms-and-related-terms">
<h4>Extracting term variants such as abbreviations, synonyms, and related terms<a class="headerlink" href="#extracting-term-variants-such-as-abbreviations-synonyms-and-related-terms" title="Permalink to this headline">¶</a></h4>
<p>26,082 candidate terms have been recognized and classified by our
pipeline and manually inspected to a) find synonyms and related words
that could be linked to SAGE, and b) build a post-filter for incorrectly
classified terms. Moreover, abbreviations have been extracted using the
algorithm of Schwartz and Hearst (Schwartz and Hearst, 2003). This way,
a Named Entity gazetteer could be built and is used at run-time. It
comprises 1,111 terms from SAGE and 447 terms from the used glossary of
statistical terms[26] as well as 54 previously unseen terms detected
by the model-based classifier.</p>
</div>
<div class="section" id="computation-of-term-and-document-frequency-scores">
<h4>Computation of term and document frequency scores<a class="headerlink" href="#computation-of-term-and-document-frequency-scores" title="Permalink to this headline">¶</a></h4>
<p>Term frequency statistics have been calculated off-line for the entire
SSOAR corpus. The term frequency at corpus level will be used at run
time to determine the term relevance at the document level by
calculating the TF-IDF scores. The most relevant terms from SAGE are
listed in Table 8.6.</p>
<p>| <strong>SAGE Term</strong>                   | <strong>TF-IDF Score</strong> | <strong>Semantic Class</strong> |
| :—————————–  | :————— | :—————– |
| Fuzzy logic                     | 591,29           | Research Method    |
| arts-based research             | 547,21           | Research Method    |
| cognitive interviewing          | 521,13           | Research Method    |
| QCA                             | 463,13           | Research Method    |
| oral history                    | 399,68           | Research Method    |
| market research                 | 345,37           | Research Field     |
| life events                     | 186,61           | Research Field     |
| Realism                         | 314,34           | Research Theory    |
| Marxism                         | 206,77           | Research Theory    |
| ATLAS.ti                        | 544,51           | Research Tool      |
| GIS                             | 486,01           | Research Tool      |
| SPSS                            | 136,52           | Research Tool      |</p>
<p>Table 8.6: Most relevant terms from SAGE by Semantic Type.</p>
</div>
<div class="section" id="definition-of-a-relevance-score">
<h4>Definition of a relevance score<a class="headerlink" href="#definition-of-a-relevance-score" title="Permalink to this headline">¶</a></h4>
<p>Relevance of terminology is often assessed using the notion of
<em>unithood</em>, i.e. ‘the degree of strength or stability of syntagmatic
combinations of collections’, and <em>termhood</em>, i.e. ‘the degree that a
linguistic unit is related to domain-specific concepts’ (Kageura and
Umino, 1996). Regarding <em>unithood</em>, the NER model implicitly contains
heuristics about legal POS tag sequences for candidate terms, consisting
of at least one noun (NN), preceeded or followed by modifiers such as
adjectives (JJ), participles (VB*) or cardinal numbers (CD),
complemented by wordshape features.</p>
<p>In order to find out if the candidate term also fulfills the <em>termhood</em>
requirement, domain-specific term frequency statistics have been
computed on the SSOAR repository, and set in contrast to general domain
vocabulary terms. It has to be noted that only a small portion of the
social science terms is actually unique to the domain (e.g., ‘dependent
interviewing’), while others might be drawn from related disciplines
such as statistics (e.g., ‘conditional likelihood ratio test’).</p>
</div>
<div class="section" id="preliminary-results">
<h4>Preliminary results<a class="headerlink" href="#preliminary-results" title="Permalink to this headline">¶</a></h4>
<p>Our method has been tested on 100 fulltext papers from SSOAR and ten
documents from the Rich Context Competition (RCC), all randomly selected
from a hold out corpus. In our experiments on SSOAR Social Science
publications, we compared results to the given metadata information. The
main finding was that while most entities from the SAGE thesaurus could
be extracted and linked reliably (e.g., ’Paired t-test’), they could not
be easily mapped to the SSOAR metadata terms, which consist of only a
few abstract classes (e.g., ’quantitative analysis’). Furthermore, our
tool was tested by the RCC organizer, were the judges reviewed ten
random publications and generated qualitative scores for each document.
In this evaluation, the research method extraction tool received the
overall best results of all competitors for this task.[27]</p>
</div>
</div>
</div>
<div class="section" id="research-field-classification">
<h2>5. Research field classification<a class="headerlink" href="#research-field-classification" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id5">
<h3>5.1 Task description<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>The goal of this task is to identify the research fields covered in the
social science publications. In general, two approaches could be applied
to this task. One is the extraction of relevant terms of the
publications. It means that this task could be seen as a keyword
extraction task and the detected terms considered as descriptive terms
regarding the research field. The second approach is to learn to
classify publications research fields with the use of annotated data in
a superviesed manner. The benifit of the second approach is that the
classification scheme to describe the research field can be defined by
experts of the domain. The disadvantage of supervised trained
classifiers for this task is the lack of applicable training data.
Furthermore, it must be ensured that the training data is comparable to
the texts the research field classifier should be applied on.</p>
<div class="section" id="id6">
<h4>Formal problem definition<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h4>
<p>Let (P) denote a set of publications of size (n), (A) a set of
corresponding abstracts of the same size and (L) a set of (k)
defined class labels describing research fields. The task of research
field classification is to select for each publication (p_i\in{P})
based on the information contained in the corresponding abstract
(a_i\in{A}) a set of labels
(C_i = \varnothing \cap {c_1\dots c_n|c_n \in L}) of (n) labels.
The number of (n) denotes the number of labels from (L) describing
the research field of (a_i) and can vary for each publication (p_i).
If there is no label (l_k) representing the information given by the
abstract (a_i) the set of class labels is the empty set
(\varnothing).</p>
</div>
</div>
<div class="section" id="id7">
<h3>5.2 Our approach<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>Since we didn’t receive any gold standard for this task during the
competition we decided to make use of external resources. We decided to
use an external labeled dataset to train a text classifier which is able
to predict one or moreresearch label for a given abstract of a
publication.</p>
<p>The publications given througout the competition belongs to the domain
of social sciences we considered metadata from a open access repository
for the social sciences called SSOAR. The advantages are twofold. On the
one hand, we could rely on professional annotations in a given
classification scheme covering the social sciences and related areas. On
the other hand the source is openly available.[28]</p>
<p>The annotated data of SSOAR contains four different annotation schemes
for research field related information. By reviewing these schemes, we
decided to use the Classification Social Science (classoz) annotation
scheme. The number of classes in each schema, coverage of each
classification, and the distribution of data in each schema affected our
decision. An exhausitve description of the used data can be found in
Section 8.2.</p>
<div class="section" id="pre-processing-and-model-architecture">
<h4>Pre-processing and model architecture<a class="headerlink" href="#pre-processing-and-model-architecture" title="Permalink to this headline">¶</a></h4>
<p>SSOAR is a multilingual repository. Therefore, the available abstracts
may vary in language and the language of the abstract may differ from
the language of the article itself. We selected all English abstract
with valid classification as our dataset. Mainly because of the language
of the RCC corpus. However, it should be noted that the multilingual
SSOAR abstract corpus has a skewed distribution of languages with
English and German as the main languages. We count 22,453 English
abstracts with valid classification after filtering. Due to the unequal
distribution of labels in the dataset, we need to guaranty enough
training data for each label. We selected only labels with frequency
over 300 for training the model, which results in a total of 44 out of
154 classification labels representing research fields. For creating
train and test set, 22,453 SSOAR publications with their assigned labels
were split randomly. We used a train/validation/test split of 70/10/20.
We decided to train a text classifier based on a fasttext (Joulin et
al., 2017) model in the author’s implementation. The arguments to use
this model was the speed in comparison to a more complex neural net
architecture and the still comparable to state of the art performance
(e.g.(Wang et al., 2018)). The model is trained with learning rate 1.0
for 150 epochs. Also, the negative sampling parameter is set to 25.</p>
</div>
</div>
<div class="section" id="id8">
<h3>5.3 Evaluation<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>Figure 8.3 shows the performance of the model regarding various evaluation metrics
for different thresholds. A label is assigned to a publication if the
model outputs a probability for the label above the defined threshold.
In multi-label classification, this allows us to evaluate our model from
different perspectives. As illustrated in figure 8.3,
the intersection of the micro precision and the micro recall curves is
at the threshold of 0.1, where the highest micro f1 score is achieved.
By increasing the threshold from this point, the micro-precision score
is increasing, but the micro recall is falling. By decreasing threshold,
these trends are inverted. Also, the default threshold of 0.5 doesn’t
look promising. In spite of micro-precision about 0.75, we have a
problem with the very high number of items without any prediction. In
respect to this observation it is advantageous to select a lower
threshold in a productive environment. The curve named <em>without
prediction</em> shows for a given threshold the share of publications in the
test set without any prediction. If the selected threshold value is
high, the number of publications for which the model cannot predict a
research field increases. For example, a selected threshold value of
0.55 leads to 40% unclassified publications in the test set. The <em>one
correct</em> named curve indicates the quality of the publication wise
prediction. It shows the share of all publications in the test set where
at least one of the predicted research field labels can be found in the
ground truth data. For instance, if a threshold of 0.1 is selected for
75% of the publications in the test set, at least one of the model
predictions are correct. This value decreases with increasing threshold
simmilar to the recall metric. The final micro f1 value on the test set
for our model and a selected threshold of 0.1 is 0.56 (precison 0.55,
recall 0.56).</p>
<p><img alt="_images/fields-precision-recall.png" src="_images/fields-precision-recall.png" />image
Figure 8.3: Performance for different selected probability thresholds (validation
set).</p>
</div>
</div>
<div class="section" id="discussions-and-limitations">
<h2>6. Discussions and Limitations<a class="headerlink" href="#discussions-and-limitations" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id9">
<h3>6.1 Dataset Extraction.<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>For the dataset extraction task, the proposed methods are only tested on
social science related data. The performance measures we have introduced
are based on a hold out data set of our automatically created dataset.
Especially the recall may be biased given that our training as well as
testing data is biased towards known datasets, where datasets not yet
part of our reference set are not considered.</p>
<p>The results of the second phase presented during the RCC workshop[29]
are showing good performance of our approach in comparison to the other
finalist teams with the highest precision 52.2% (second: 47.0%) and
second in recall (ours: 20.5, best: 34.8%). With respect to F1, our
approach provides the second best performing system for this task
(29.5%, 40.0% for first place). The results on the manually created hold
out set underline, that our system performs better in respect to
precision in comparison to the other finalist teams. Given that our
models are supervised through a corpus of social sciences publications,
we anticipate limited generalisability across other disciplines and plan
to investigate this aspect as part of future work. In this context, the
focus of our training data towards survey data, also reflected in
dataset titles such as <em>Current Population Survey</em>, could have biased
the model to detect the survey as a specific type of research datasets
better than other subtypes like e.g. text corpora in the NLP community.
In general, however, our approach to using a weakly labeled corpus
created from a list of dataset names could be applied in other research
domains.</p>
</div>
<div class="section" id="id10">
<h3>6.2 Research method extraction.<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>We consider the extraction of research methods from full text as a
particularly challenging task because the sample vocabulary given by the
RCC organizers covers a large thematic variety of areas. The task itself
was defined as the identification of research methods associated with a
specific publication, which in turn are drawn from a specific research
field. Since no training data has been provided, we created and
annotated a new corpus for the task and trained a CRF model, adding
lexical resources. The qualitative reviews during the two phases of the
competition attested that this approach works fine.</p>
</div>
<div class="section" id="id11">
<h3>6.3 Research field classification.<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p>Our supervised machine learning approach to handle the research field
classification task performs well on the dataset created from social
science publication metadata. A micro F1 measure of above 55% seems to
indicate reasonable performance considering the small dataset with
44 labels and a mean number of keywords of three terms per publication.
As one example of multilabel classification with a comparable size of
labels we would like to mention the classification of texts in the
domain of medicine presented in (Wang et al., 2018). The models tested
by the authors on the task of multilabel prediction from 50 different
labels leads to micro F1 values between 53% and 62%.
Considering the evaulation approach, focused on publications from the
social sciences, the generalisability across other disciplines remains
unclear and requires further research. Even though the used
classification scheme may cover neighbouring disciplines, for instance,
medicine, the numbers of samples of the training data covering other
research fields than the social science is limited. Our pragmatic
approach of basing our classifications on the abstract of the
publications makes it applicable even in scenarios where the full-text
of publications is not accessible.</p>
</div>
</div>
<div class="section" id="conclusion">
<h2>7. Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>This chapter has provided an overview on our solutions submitted to the
Rich Context Competition 2018. Aimed at improving search, discovery and
interpretability of scholarly resources, we are addressing three
distinct tasks all aimed at extracting structured information about
research resources from scientitifc publications, namely the extraction
of dataset mentions, the extraction of mentions of research methods and
the classification of research fields.</p>
<p>In order to address all aforementioned challenges, our pipelines make
use of a range of preprocessing techniques together with
state-of-the-art NLP methods as well as supervised machine learning
approaches tailored towards the specific nature of scholarly
publications as well as the dedicated tasks. In addition, background
datasets have been used to facilitate supervision of methods at larger
scale.</p>
<p>Our results indicate both significant opportunities for automating the
aforementioned three tasks but also their challenging nature, in
particular given the lack of publicly available gold standards for
training and testing. Aggregating and publishing such data has been
identified as important activity for future work, and is a prerequisite
for significantly advancing state-of-the-art methods.</p>
</div>
<div class="section" id="acknowledgments">
<h2>Acknowledgments<a class="headerlink" href="#acknowledgments" title="Permalink to this headline">¶</a></h2>
<p>This work has been partially funded by Deutsche Forschungsgemeinschaft
(DFG) under grant number MA 3964/7-1.
Wolfgang Otto acknowledges the enabling support provided by the
Indo-German Joint Research Project titled ‘Design of a Sciento-text
Computational Framework for Retrieval and Contextual Recommendations of
High-Quality Scholarly Articles’ (Grant No. DST/INT/FRG/DAAD/P-28/2017)
for this work.</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<div id="refs" class="references"><div id="ref-agerri2016robust"><p>Agerri R and Rigau G (2016) Robust multilingual named entity recognition
with shallow semi-supervised features. <em>Artificial Intelligence</em> 238.
Elsevier: 63–82.</p>
</div><div id="ref-altman2007proposed"><p>Altman M and King G (2007) A proposed standard for the scholarly
citation of quantitative data. <em>D-lib Magazine</em> 13(3/4).</p>
</div><div id="ref-conf/jcdl/Backes18"><p>Backes T (2018a) Effective unsupervised author disambiguation with
relative frequencies. In: <em>JCDL</em> (eds J Chen, MA Gonçalves, JM Allen, et
al.), 2018, pp. 203–212. ACM. Available at:
<a class="reference external" href="http://dblp.uni-trier.de/db/conf/jcdl/jcdl2018.html#Backes18">http://dblp.uni-trier.de/db/conf/jcdl/jcdl2018.html#Backes18</a>.</p>
</div><div id="ref-conf/cikm/Backes18"><p>Backes T (2018b) The impact of name-matching and blocking on author
disambiguation. In: <em>CIKM</em> (eds A Cuzzocrea, J Allan, NW Paton, et al.),
2018, pp. 803–812. ACM. Available at:
<a class="reference external" href="http://dblp.uni-trier.de/db/conf/cikm/cikm2018.html#Backes18">http://dblp.uni-trier.de/db/conf/cikm/cikm2018.html#Backes18</a>.</p>
</div><div id="ref-bird2008acl"><p>Bird S, Dale R, Dorr BJ, et al. (2008) The acl anthology reference
corpus: A reference dataset for bibliographic research in computational
linguistics. In: <em>Proceedings of the sixth international conference on
language resources and evaluation (lrec 2008)</em>, 2008. European Language
Resources Association (ELRA).</p>
</div><div id="ref-boland2019distant"><p>Boland K and Krüger F (2019) Distant supervision for silver label
generation of software mentions in social scientific publications. In:
<em>Proceedings of the 4th joint workshop on bibliometric-enhanced
information retrieval and natural language processing for digital
libraries</em>, 2019, pp. 15–27.</p>
</div><div id="ref-boland2012identifying"><p>Boland K, Ritze D, Eckert K, et al. (2012) Identifying references to
datasets in publications. In: <em>International conference on theory and
practice of digital libraries</em>, 2012, pp. 150–161. Springer.</p>
</div><div id="ref-eckle2013automatically"><p>Eckle-Kohler J, Nghiem T-D and Gurevych I (2013) Automatically assigning
research methods to journal articles in the domain of social sciences.
In: <em>Proceedings of the 76th asis&amp;T annual meeting: Beyond the cloud:
Rethinking information boundaries</em>, 2013, p. 44. American Society for
Information Science.</p>
</div><div id="ref-finkel2005incorporating"><p>Finkel JR, Grenager T and Manning C (2005) Incorporating non-local
information into information extraction systems by gibbs sampling. In:
<em>Proceedings of the 43rd annual meeting on association for computational
linguistics</em>, 2005, pp. 363–370. Association for Computational
Linguistics.</p>
</div><div id="ref-ghavimi2016semi"><p>Ghavimi B, Mayr P, Lange C, et al. (2016) A semi-automatic approach for
detecting dataset references in social science texts. <em>Information
Services &amp; Use</em> 36(3-4). IOS Press: 171–187.</p>
</div><div id="ref-gildea2018acl"><p>Gildea D, Kan M-Y, Madnani N, et al. (2018) The acl anthology: Current
state and future directions. In: <em>Proceedings of workshop for nlp open
source software (nlp-oss)</em>, 2018, pp. 23–28.</p>
</div><div id="ref-conf/jcdl/HienertKBZM19"><p>Hienert D, Kern D, Boland K, et al. (2019) A digital library for
research data and related information in the social sciences. In: <em>JCDL</em>
(eds M Bonn, D Wu, JS Downie, et al.), 2019, pp. 148–157. IEEE.
Available at:
<a class="reference external" href="http://dblp.uni-trier.de/db/conf/jcdl/jcdl2019.html#HienertKBZM19">http://dblp.uni-trier.de/db/conf/jcdl/jcdl2019.html#HienertKBZM19</a>.</p>
</div><div id="ref-joulin2017bag"><p>Joulin A, Grave E, Bojanowski P, et al. (2017) Bag of tricks for
efficient text classification. In: <em>Proceedings of the 15th conference
of the european chapter of the association for computational
linguistics: Volume 2, short papers</em>, 2017, pp. 427–431. Association for
Computational Linguistics.</p>
</div><div id="ref-kageura1996methods"><p>Kageura K and Umino B (1996) Methods of automatic term recognition: A
review. <em>Terminology. International Journal of Theoretical and Applied
Issues in Specialized Communication</em> 3(2). John Benjamins Publishing
Company: 259–289.</p>
</div><div id="ref-Krmer2018ADD"><p>Krämer T, Klas C-P and Hausstein B (2018) A data discovery index for the
social sciences. In: <em>Scientific data</em>, 2018.</p>
</div><div id="ref-lewis2003sage"><p>Lewis-Beck M, Bryman AE and Liao TF (2003) <em>The Sage Encyclopedia of
Social Science Research Methods</em>. Sage Publications.</p>
</div><div id="ref-mikolov2013distributed"><p>Mikolov T, Sutskever I, Chen K, et al. (2013) Distributed
representations of words and phrases and their compositionality. In:
<em>Advances in neural information processing systems</em>, 2013, pp.
3111–3119.</p>
</div><div id="ref-nasar2018information"><p>Nasar Z, Jaffry SW and Malik MK (2018) Information extraction from
scientific articles: A survey. <em>Scientometrics</em> 117(3). Springer:
1931–1990.</p>
</div><div id="ref-qasemizadeh2016acl"><p>QasemiZadeh B and Schumann A-K (2016) The acl rd-tec 2.0: A language
resource for evaluating term extraction and entity recognition methods.
In: <em>LREC</em>, 2016.</p>
</div><div id="ref-sahoo2015analysing"><p>Sahoo P, Gadiraju U, Yu R, et al. (2017) Analysing structured scholarly
data embedded in web pages. <em>Lecture Notes in Computer Science</em> 9792.
Springer.</p>
</div><div id="ref-SchwartzH03"><p>Schwartz AS and Hearst MA (2003) A simple algorithm for identifying
abbreviation definitions in biomedical text. In: <em>Pacific symposium on
biocomputing</em>, 2003, pp. 451–462.</p>
</div><div id="ref-tkaczyk2015cermine"><p>Tkaczyk D, Szostek P, Fedoryszak M, et al. (2015) CERMINE: Automatic
extraction of structured metadata from scientific literature.
<em>International Journal on Document Analysis and Recognition (IJDAR)</em>
18(4). Springer: 317–335.</p>
</div><div id="ref-Turian"><p>Turian J, Ratinov L and Bengio Y (2010) Word representations: A simple
and general method for semi-supervised learning. In: <em>Proceedings of the
48th annual meeting of the association for computational linguistics</em>,
Stroudsburg, PA, USA, 2010, pp. 384–394. ACL ’10. Association for
Computational Linguistics. Available at:
<a class="reference external" href="http://dl.acm.org/citation.cfm?id=1858681.1858721">http://dl.acm.org/citation.cfm?id=1858681.1858721</a>.</p>
</div><div id="ref-wang2018joint"><p>Wang G, Li C, Wang W, et al. (2018) Joint embedding of words and labels
for text classification. <em>Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers)</em>.
Association for Computational Linguistics. DOI:10.18653/v1/p18-1216.</p>
</div><div id="ref-journals/semweb/YuGFLRD19"><p>Yu R, Gadiraju U, Fetahu B, et al. (2019) KnowMore - knowledge base
augmentation with structured web markup. <em>Semantic Web</em> 10(1): 159–180.
Available at:
<a class="reference external" href="http://dblp.uni-trier.de/db/journals/semweb/semweb10.html#YuGFLRD19">http://dblp.uni-trier.de/db/journals/semweb/semweb10.html#YuGFLRD19</a>.</p>
</div></div><ol class="simple">
<li><p><a class="reference external" href="https://www.gesis.org/en/institute">https://www.gesis.org/en/institute</a></p></li>
<li><p><a class="reference external" href="https://www.gesis.org/en/institute/departments/knowledge-technologies-for-the-social-sciences/">https://www.gesis.org/en/institute/departments/knowledge-technologies-for-the-social-sciences/</a></p></li>
<li><p><a class="reference external" href="https://www.gesis.org/en/research/applied-computer-science/labs/wts-research-labs">https://www.gesis.org/en/research/applied-computer-science/labs/wts-research-labs</a></p></li>
<li><p><a class="reference external" href="https://www.gesis.org/en/research/external-funding-projects/archive/infolis-i-and-ii">https://www.gesis.org/en/research/external-funding-projects/archive/infolis-i-and-ii</a></p></li>
<li><p><a class="reference external" href="https://search.gesis.org">https://search.gesis.org</a></p></li>
<li><p><a class="reference external" href="https://datasearch.gesis.org/">https://datasearch.gesis.org/</a></p></li>
<li><p><a class="reference external" href="https://coleridgeinitiative.org/richcontextcompetition">https://coleridgeinitiative.org/richcontextcompetition</a></p></li>
<li><p><a class="reference external" href="https://www.ssoar.info">https://www.ssoar.info</a></p></li>
<li><p><a class="reference external" href="https://fasttext.cc/">https://fasttext.cc/</a></p></li>
<li><p><a class="reference external" href="https://www.gesis.org/ssoar/home">https://www.gesis.org/ssoar/home</a></p></li>
<li><p><a class="reference external" href="https://www.gesis.org/en/services/research/tools/thesaurus-for-the-social-sciences">https://www.gesis.org/en/services/research/tools/thesaurus-for-the-social-sciences</a></p></li>
<li><p><a class="reference external" href="https://www.gesis.org/angebot/recherchieren/tools-zur-recherche/klassifikation-sozialwissenschaften">https://www.gesis.org/angebot/recherchieren/tools-zur-recherche/klassifikation-sozialwissenschaften</a>
(in German)</p></li>
<li><p><a class="reference external" href="http://www.openarchives.org">http://www.openarchives.org</a></p></li>
<li><p><a class="reference external" href="https://github.com/CeON/CERMINE">https://github.com/CeON/CERMINE</a></p></li>
<li><p><a class="reference external" href="https://jats.nlm.nih.gov">https://jats.nlm.nih.gov</a></p></li>
<li><p><a class="reference external" href="https://spacy.io">https://spacy.io</a></p></li>
<li><p><a class="reference external" href="http://methods.sagepub.com">http://methods.sagepub.com</a></p></li>
<li><p>apart from those used in traditional NER systems like <em>Person</em>,
<em>Location</em>, or <em>Organization</em> with abundant training data, as
covered in the Stanford NER system(Finkel et al., 2005)</p></li>
<li><p><a class="reference external" href="https://nlp.stanford.edu/projects/project-ner.shtml">https://nlp.stanford.edu/projects/project-ner.shtml</a></p></li>
<li><p><a class="reference external" href="https://www.ssoar.info">https://www.ssoar.info</a></p></li>
<li><p><a class="reference external" href="https://coleridgeinitiative.org/richcontextcompetition">https://coleridgeinitiative.org/richcontextcompetition</a> with a
total of 5,000 English documents</p></li>
<li><p><a class="reference external" href="https://acl-arc.comp.nus.edu.sg/">https://acl-arc.comp.nus.edu.sg/</a></p></li>
<li><p><a class="reference external" href="https://radimrehurek.com/gensim/">https://radimrehurek.com/gensim/</a></p></li>
<li><p>Word embeddings are trained with a skip gram model using embedding
size equal to 100, word window equal to 5, minimal occurrences of a
word to be considered 10. Word embeddings are clustered using
agglomerative clustering with a number of clusters set to <span>500,
600, 700</span>. Ward linkage with Euclidean distance is used to
minimize the variance within the clusters.</p></li>
<li><p>A glossary of statistical terms as provided in
<a class="reference external" href="https://www.statistics.com/resources/glossary/">https://www.statistics.com/resources/glossary/</a> has been added as
well.</p></li>
<li><p>Based on <a class="reference external" href="https://www.statistics.com/resources/glossary">https://www.statistics.com/resources/glossary</a></p></li>
<li><p>Rank: 1,2,2,1,1 for judges 1-5.</p></li>
<li><p>A script to download the metadata of SSOAR can be found in
github/research-field-classifier</p></li>
<li><p>Agenda of the Workshop:
<a class="reference external" href="https://coleridgeinitiative.org/richcontextcompetition/workshopagenda">https://coleridgeinitiative.org/richcontextcompetition/workshopagenda</a>.
The results of the finalists are presented here:
<a class="reference external" href="https://youtu.be/PE3nFrEkwoU?t=9865">https://youtu.be/PE3nFrEkwoU?t=9865</a>.</p></li>
</ol>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Rich Search and Discovery for Research Datasets</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="chap00.html">Contributor Bios</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap01.html">Chapter 1 - Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap01.html#introduction">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap01.html#how-this-book-came-to-be">How this book came to be</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap01.html#book-overview">Book overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap01.html#section-2">Section 2:</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap01.html#section-3">Section 3:</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap01.html#section-4-looking-forward">Section 4: Looking forward</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap01.html#more-resources">More resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap01.html#references">References</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap02.html">Chapter 2 - Bundesbank</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap03.html">Chapter 3 - Digital Science Use Cases</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap03.html#chapter-3-digital-science-use-cases-enriching-context-and-enhancing-engagement-around-datasets">Chapter 3 – Digital Science Use Cases: Enriching context and enhancing engagement around datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap04.html">Chapter 4 - Metadata for Social Science Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap05.html">Chapter 5 - Compettion Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html">Chapter 6 - Finding datasets in publications: The Allen Institute for Artificial Intelligence approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html#introduction">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html#methods">Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html#results">Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html#conclusion">Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html#acknowledgments">Acknowledgments</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html#references">References</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html#footnotes">Footnotes</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html#appendix">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html">Chapter 7 - Finding datasets in publications: The KAIST approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#non-technical-overview">Non-technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#literature-review">Literature Review</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#what-did-you-do">What did you do</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#what-worked-and-what-didnt">What worked and what didn’t</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#summary-of-your-results-and-caveats">Summary of your results and caveats</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#lessons-learned-and-what-would-you-do-differently">Lessons learned and what would you do differently</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#conclusion">Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#what-comes-next">What comes next</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#acknowledgments">Acknowledgments</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html#appendix-description-of-the-code-and-documentation">Appendix: Description of the code and documentation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Chapter 8 - Knowledge Extraction from scholarly publications: The GESIS contribution to the Rich Context Competition</a></li>
<li class="toctree-l1"><a class="reference internal" href="#knowledge-extraction-from-scholarly-publications-the-gesis-contribution-to-the-rich-context-competition">Knowledge Extraction from scholarly publications - The GESIS contribution to the Rich Context Competition</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap09.html">Chapter 9 - Finding datasets in publications: The University of Paderborn approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap09.html#abstract">Abstract</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap09.html#literature-review">Literature Review</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap09.html#project-architecture">Project Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap09.html#preprocessing">Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap09.html#approach">Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap09.html#evaluation">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap09.html#discussion">Discussion</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap09.html#future-agenda">Future Agenda</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap09.html#appendix">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap10.html">Chapter 10 - Finding datasets in publications: The Singapore Management University approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap10.html#finding-datasets-in-publications-the-singapore-management-university-approach">Finding datasets in publications: The Singapore Management University approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html">Chapter 11 - Finding datasets in publications: The University of Syracuse approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html#abstract">Abstract</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html#introduction">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html#the-dataset">The dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html#the-proposed-method">The Proposed Method</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html#results">Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html#conclusion">Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html#acknowledgements-acknowledgements-unnumbered-unnumbered">Acknowledgements {#acknowledgements .unnumbered .unnumbered}</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap12.html">Chapter 12 - The future of context</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap12.html#the-future-of-ai-in-rich-context">The Future of AI in Rich Context</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="chap07.html" title="previous chapter">Chapter 7 - Finding datasets in publications: The KAIST approach</a></li>
      <li>Next: <a href="chap09.html" title="next chapter">Chapter 9 - Finding datasets in publications: The University of Paderborn approach</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, NYU.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/chap08.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>