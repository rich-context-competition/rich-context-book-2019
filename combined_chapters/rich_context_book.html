<h1 id="contributor-bios">Contributor Bios</h1>
<p>Karam Abdulahhad is a postdoctoral at GESIS - Leibniz Institute for the Social Sciences in Germany. He is engaged in the ExploreData project to build an advanced search engine for social science data. He has a Ph.D. degree in computer sciences from Grenoble-Alpes University in France, where he tackled the problem of term-mismatch. He proposed a new IR model by adapting an idea from physics. His research interests include IR theory, logical/conceptual/semantic IR, machine learning, and text mining. Recently, he is studying the profitability of the modern embedding technics in IR. He taught in several universities and developed several tools.</p>
<p>Palakorn Achananuparp is a senior research scientist at Living Analytics Research Centre (LARC), Singapore Management University. He is interested in developing and applying machine learning, natural language processing, and crowdsourcing techniques to solve problems in a variety of domains, including online social networks, politics, and public health.</p>
<p>Daniel Acuna is an Assistant Professor in the School of Information Studies at Syracuse University, Syracuse, NY. He runs the Science of Science and Computational Discovery Lab, supported by grants from NSF, DDHS, and DARPA and featured in Nature Podcast, The Chronicle of Higher Education, NPR, and the Scientist. The goal of his current research is to predict future academic success and remove potential biases that scientists and funding agencies commit during peer review. He has created tools to improve literature search, peer review, and detect scientific fraud.</p>
<p>Bob Allen is developing a model-oriented approach to information organization. His previous work ranged from recommender systems to neural networks. Bob studied at Reed College and UCSD. He joined the Research organization at Bell Laboratories. He then joined to the Bellcore Applied Research group in information science and digital libraries. He was the Editor-in-Chief of the ACM Transactions on Information Systems and later Chair of the ACM Publications Board. Since 1998 he has been a faculty member at number of universities around the world such as Maryland, Drexel, Victoria (NZ), Tsukuba (Japan), and Yonsei (Korea).</p>
<p>Waleed Ammar is a senior research scientist at Google, where he works on NLP-related problems in biomedical and clinical applications. Before joining Google, Waleed was a research scientist at the Allen Institute for Artificial Intelligence where he led the Semantic Scholar research team. In 2016, he received a Ph.D. degree in artificial intelligence from Carnegie Mellon University. Before pursuing the Ph.D., Waleed was a software developer at Microsoft Research, web developer at eSpace Technologies, and teaching assistant at Alexandria University.</p>
<p>Christine Betts is a software engineer working on human computation at Google AI. She graduated with honors in computer science from the University of Washington. While there, she was an intern at The Allen Institute for AI, and before that at Facebook and Google.</p>
<p>Katarina Boland is research associate in the department Knowledge Technologies for the Social Sciences at GESIS - Leibniz Institute for the Social Sciences. She joined GESIS in August 2011 after earning her Magistra Artium degree in Computational Linguistics, Computer Science and Psychology at Heidelberg University. Katarina has been part of the DFG projects InFoLiS I and InFoLiS II which addressed the automatic linking of research data and scientific publications. Katarina’s main research interests lie in the field of Natural Language Processing and Text Mining. Currently, she is primarily involved in research on Information Extraction, NLP &amp; Journalism and automatic fact-checking.</p>
<p>Minh-Son Cao is a Master student in School of Computing at KAIST, under the supervision of Professor Sung-Hyon Myaeng at Information Retrieval and Natural Language Processing Laboratory. Previously, he received his Bachelor Degree from the University of Engineering and Technology, Vietnam National University (VNU-UET) in June 2017. He was a member of Data Mining and Knowledge Technology Laboratory from August 2015 to June 2017, under the supervision of Associate Professor Xuan-Hieu Phan. His research focuses on the application of Deep Learning in Natural Language Processing, mainly on embedding problems.</p>
<p>Stefan Dietze is full professor for Data &amp; Knowledge Engineering at the Institute for Computer Science at Heinrich-Heine-University Düsseldorf, Scientific Director of the department Knowledge Technologies for the Social Sciences at GESIS – Leibniz Institute for the Social Sciences and affiliated member at the L3S Research Center of the Leibniz University Hanover, Germany. His research interests are at the intersection of information retrieval, semantic technologies and artificial intelligence, and in particular, the extraction, fusion and search of knowledge and data on the Web. Stefan’s work has been published at major scientific venues, such as WWW/The Web Conf, SIGIR, CHI or ISWC, where he also frequently serves as PC and/or OC member.</p>
<p>Dimitar Dimitrov is a PostDoc at GESIS - Leibniz Institute for the Social Sciences, Cologne, Germany. He obtained a Ph.D. from the University of Koblenz-Landau, Koblenz, Germany. Before that, he studied Software Engineering at the University of Applied Sciences Konstanz, Konstanz, Germany, where he also obtained his master’s degree in Computer Science. At GESIS, Dimitar Dimitrov is working on the da|ra project aimed to deliver the software infrastructure for assigning DOI names to social and economic datasets. His research focuses on applying statistical and machine learning techniques to study user behavior in web-based systems.</p>
<p>Behnam Ghavimi is a research fellow in WTS department at GESIS – Leibniz Institute for the Social Sciences. He graduated from the University of Bonn with a Master’s degree in Computer Science. His master thesis was about detecting dataset references in texts under the supervision of Prof. Sören Auer and Dr. Philipp Mayr. Since September 2016, he was involved in different projects focused on NLP (text analysis and text mining) and recommender systems. One of his projects was the EXCITE project - jointly run by WeST at the University of Koblenz-Landau and GESIS to extract citations from publications and make more citation data openly available.</p>
<p>Andrew Gordon is Senior Data Engineer at Columbia University Information Technology. Previously, Andrew was Research Information Scientist with the Coleridge Initiative at New York University. There, Andrew served as an information specialist, programmer, and ETL engineer supporting the full research and administrative data lifecycle for ingest, curation, facilitating data discovery, and providing access to sensitive, administrative data for academics and policy analysts. Andrew has a Master of Science degree in Information from the University of Michigan School of Information and a Bachelor of Arts degree in Cultural Anthropology from the University of Michigan.</p>
<p>Suchin Gururangan is a Predoctoral Young Investigator at the Allen Institute for AI (AI2). His research interests involve model evaluation and robustness in NLP, especially under low-resource settings and distant domains. Before joining AI2, Suchin was a master’s student in NLP at the University of Washington, and before graduate school, Suchin was a data scientist at various companies in Boston in Seattle.</p>
<p>Giwon Hong is a master’s student in the School of Computing at KAIST and research assistant in IR&amp;NLP Lab at KAIST. He graduated from Sungkyunkwan University with a degree in Computer Science in February 2018. His research lays in the area of Natural Language Processing, specifically in Question Answering and Relation Extraction</p>
<p>Rricha Jalota is a developer in the Computer Science department of Paderborn University. She works in the areas of data access and knowledge extraction. Her interests lie in the application of Machine Learning/Deep Learning approaches to solve NLP problems in the domain of Question Answering, Conversational AI and Information Retrieval.</p>
<p>Daniel King is a Predoctoral Young Investigator on the Semantic Scholar team at The Allen Institute for AI. He received his B.S. in Computer Science from Harvey Mudd College in May 2018. His research interests are generally in Natural Language Processing and using AI techniques to make useful tools. Outside of research and software engineering, he enjoys playing soccer, bughouse chess, and hiking.</p>
<p>Sebastian Kohlmeier is the Sr. Manager of Program Management and Business Operations at the Allen Institute for AI where he leads program management for applied research, business intelligence and data science and partner development. Prior to joining the Allen Institute for AI, Sebastian worked as a Technical Program Manager and Engineering Manager in a variety of roles at Amazon and Microsoft. Sebastian graduated with honors from Western Washington University in 2007.</p>
<p>Philips Kokoh Prasetyo Philips Kokoh Prasetyo is a principal research engineer at the Living Analytics Research Centre (LARC) in the Singapore Management University. He enjoys analyzing data from many different perspectives, and his current interests include machine learning, natural language processing, text mining, and deep learning. He received Master degree from National Cheng Kung University in Taiwan, and Bachelor degree from Sekolah Tinggi Teknik Surabaya in Indonesia. He received several awards including ACLCLP thesis award in 2009, and DPU scholarship from 2007 to 2009.</p>
<p>Julia Lane is a Professor at the NYU Wagner Graduate School of Public Service, at the NYU Center for Urban Science and Progress, and a NYU Provostial Fellow for Innovation Analytics. She cofounded the Coleridge Initiative, whose goal is to use data to transform the way governments access and use data for the social good through training programs, research projects and a secure data facility. The approach is attracting national attention, including the Commission on Evidence Based Policy and the Federal Data Strategy.</p>
<p>Ekaterina Levitskaya is an Associate Research Scientist at the Coleridge Initiative, New York University. She utilizes computational approaches to the social science research, with special focus on text analysis and natural language processing. Her background is in computational linguistics and applied data science. She is interested in applying computational skills for the projects with social impact and utilizing text as data in a variety of applications for the social science research.</p>
<p>Ee-Peng Lim Dr Ee-Peng Lim is the Lee Kong Chian Professor of Information Systems and Director of Living Analytics Research Center in the Singapore Management University. He received his PhD degree in Computer Science from the University of Minnesota. His research expertise covers social media mining, social/urban data analytics, and information retrieval. He has published more than 90 international journal papers and 280 conference papers, many of them appeared at top ACM and IEEE journals and conference venues. He is the recipient of the Distinguished Contribution Award at the 2019 Pacific Asia Conference on Knowledge Discovery and Data Mining (PAKDD).</p>
<p>Jonathan Morgan is a Doctoral Candidate at the University of Mannheim. Jonathan has worked as a Senior Research Scientist at New York University; a Senior Data Scientist at the United States Census Bureau; a programmer, designer, and product manager for higher education systems integrations and data governance applications at various companies and institutions; and as an online producer for The New York Times and Multiplatform Editor for the Detroit News. He has a Bachelor of Arts in Computer Science from Wittenberg University, a Master of Arts in Journalism from NYU, and was a University Enrichment Fellow at Michigan State University.</p>
<p>Ian Mulvany is head of transformation at SAGE Publishing. He helped setup SAGE’s methods innovation incubator SAGE Ocean following a lean product development approach. Previously he ran technology operations for eLife, was head of product for Mendeley and ran a number of early web2.0 products for Nature Publishing Group. He is passionate about creating digital tools that support the research enterprise. He is interested in the interplay between different stakeholders that can lead to the sustainably of these kinds of tools.</p>
<p>Paco Nathan is a technologist, consultant, and evil mad scientist with deep experience in the areas of machine learning, human in the loop patterns for AI, and natural language work. He is advisor to several tech organizations including: NYU Coleridge Initiative, IBM Data Science Community, Amplify Partners, Recognai, Data Spartan, and Primer AI. He is co-chair for Rev conference by Domino Data Lab.</p>
<p>Axel-Cyrille Ngonga Ngomo is a full professor at the Computer Science department of Paderborn University. In his work, he focuses on the life cycle of knowledge graphs. He has hence been involved in the development of approaches for the extraction, storage, querying, integration and fusion as well as the exploitation of knowledge graphs. One core usage of knowledge graphs he explores is the development of explainable and responsible active machine learning algorithms. Axel is a proponent of open data, open research, and open science with a keen interest in paradigms and frameworks for reproducible scientific research.</p>
<p>Wolfgang Otto is a postgraduate and research associate at GESIS - Leibniz Institute for the Social Sciences in Germany. As part of the Knowledge Technologies for the Social Sciences Department under Stefan Dietze, he applies NLP-techniques on text and data corpora in the Social Sciences. After finishing with a master degree at the NLP Group at Leipzig University (Prof. Dr. Gerhard Heyer), he is part of a team in a third funded project (German Research Fund) to build up a Specialized Information Service for Political Scientists (pollux-fid.de). A Project the State and University Library Bremen (SuUB) is realizing in cooperation with GESIS. During his studies, he collaborated in Projects on Digital Humanities, Applied Text Mining, and Data Science.</p>
<p>Sophie Rand is an Associate Research Scientist working on the Rich Context project at the  Coleridge Initiative. Previously, she was a Public Health Data Analyst at the New York City Department of Health and Mental Hygiene, first in the Bureau of Primary Care and Prevention, where she worked with data from Health Information Exchanges and Electronic Health Records in support of clinical-community public health programs; and in the Division of Disease Control, working with real-time Emergency Department, reportable infectious disease, and school health data.  Sophie holds a Bachelors of Science in Engineering from the Cooper Union and a Master’s in Public Health from the CUNY School of Public Health.</p>
<p>Michael Röder is a research associate and a Ph.D. candidate in the Computer Science department of Paderborn University. His research focuses on data gathering, data analysis and benchmarking of linked data systems. He has been involved in several research projects and reviewed papers for different scientific journals and conferences.</p>
<p>Haritz Puerto San Roman is a master’s student in the School of Computing at KAIST and research assistant in IR&amp;NLP Lab at KAIST. He graduated from the University of Malaga with a degree in Computer Science in July 2017. His research lays in the application of Machine Learning to Natural Language Processing, specifically to solve the problem of Question Answering.</p>
<p>Amila Silva Amila Silva is a graduate from University of Moratuwa, Sri Lanka, with a First-Class Honors degree in Electronics and Telecommunication Engineering, where he was placed second of the graduating class of 110 students. He is currently working towards a Ph.D. degree at the Department of Computing and Information Systems, the University of Melbourne, Australia. He was awarded the Melbourne Graduate Research Scholarship supporting his studies. Besides, he was awarded the Rowden White Scholarship, a prestigious scholarship provided by the University of Melbourne to talented Ph.D. students. His research interests include continual learning, graph analytics, and data mining.</p>
<p>René Speck is a research associate and a Ph.D. candidate in the data processing service center (Research and Development Department II) at Leipzig University. His work and research focus on knowledge extraction, knowledge graphs, natural language processing, and machine learning. René Speck has been involved in several projects at the Leipzig University since 2013. He has been a reviewer for several conferences and journals since then as well.</p>
<p>Nikit Srivastava is a master’s student and a student research assistant in the Computer Science department at Paderborn University. His research mainly focuses on data science chatbots and word embeddings. He has been involved in the development of many proofs-of-concept and prototype demonstrations for different scientific research papers and conferences.</p>
<p>Narges Tavakolpoursaleh is a postgraduate and research fellow at GESIS - Leibniz Institute for the Social Sciences in Germany. At the moment, as a part of a team, she involves in a third-party funded project (STELLA) that aims to create an evaluation infrastructure for search and recommendation services within productive web-based search systems with real users.</p>
<p>Ricardo Usbeck is a senior (guest) researcher at Paderborn University focusing on data extraction and information retrieval. His main interest is the combination of machine learning, statistics, and linked data. Ricardo is leading and executing several national and international research projects concerned with searching large amounts of heterogeneous and small, specific datasets using natural language.</p>
<p>Daniel Vollmers is a research associate and a Ph.D. candidate in the Computer Science department of Paderborn University. His research focuses on Question Answering knowledge extraction and machine learning. He has been involved in several research projects in these domains.</p>
<p>Alex D. Wade is Program Manager for Knowledge Graphs and Open Science at the Chan Zuckerberg Initiative. Alex earned his master’s in library science from the University of Washington and has worked for the libraries at the University of California at Berkeley, the University of Michigan, and the University of Washington. Alex has spent his post-academic career working on problems in information retrieval, knowledge representation, and open science at Microsoft, Amazon, and Facebook, and currently works on the Meta service and the Open Science group at the Chan Zuckerberg Initiative.</p>
<p>Tong Zeng is a Ph.D. candidate in the School of Information Management at Nanjing University and a visiting scholar in the School of Information Studies at Syracuse University, working with Professor Daniel Acuna in the Science of Science and Computational Discovery Lab. Tong’s research interests lie within text mining and scientometrics. In particular, he is interested in applying natural language processing and network science techniques on scientific literature to investigate, understand, and facilitate various aspects of scientific communication. His recent projects involve detecting dataset mentions in full text, assigning credit to datasets, and disambiguating authors at scale.</p>
<p>Andrea Zielinski is a Senior Research Scientist at the Fraunhofer Institute for Systems and Innovation Research (ISI), Karlsruhe, Germany and conducts applied research in Machine Learning and Text Mining at the Innovation System Data Excellence Center (ISDEC). She studied Computer Science with a focus on Artificial Intelligence and Linguistics at the University of Hamburg. In 2002, she received her PhD in Computational Linguistics from Saarland University. Since 2008, she also serves as a lecturer for Text Mining at the Department of Computational Linguistics, Heidelberg University, Germany. Her research interests lie at the intersection of Natural Language Processing and Machine Learning, particularly on areas relating to Text Mining and Semantics.</p>
<h1 id="chapter-1---introduction">Chapter 1 - Introduction</h1>
<p>Rich Context Introductory Chapter</p>
<p>Ian Mulvany, Paco Nathan, Sophie Rand, Julia Lane</p>
<h1 id="introduction">Introduction</h1>
<p>Science is at a crossroads. The enormous growth of access to data coupled with rapid technological progress, has created opportunities to conduct empirical research at a scale that would have been almost unimaginable a generation or two ago. Researchers can now rapidly acquire and develop massive, rich datasets; routinely fit complex statistical models; and conduct their science in increasingly fine-grained ways. Yet there is no automated way to search for and discover what datasets are used in empirical research, leading to fundamental irreproducibility of empirical science and threatening its legitimacy and utility(<em>1</em>, <em>2</em>). There is an enormous interest to change the current manual and ad-hoc system, and incentives are increasingly aligned: while only a fraction of datasets are identified in scientific research, those publications that do cite data are cited up to 25% more than those that do not(<em>3</em>).</p>
<p>Vannevar Bush foreshadowed the issue more than 60 years ago:</p>
<blockquote>
<p>“There is a growing mountain of research. But there is increased evidence that we are being bogged down today as specialization extends. The investigator is staggered by the findings and conclusions of thousands of other workers—conclusions which he cannot find time to grasp, much less to remember, as they appear. … Mendel’s concept of the laws of genetics was lost to the world for a generation because his publication did not reach the few who were capable of grasping and extending it; and this sort of catastrophe is undoubtedly being repeated all about us, as truly significant attainments become lost in the mass of the inconsequential”(<em>11</em>).</p>
</blockquote>
<p>We can do better – and we now have the opportunity to do so.</p>
<p>The core problem that needs to be addressed is automating the search for and discovery of datasets used in empirical data – building an Amazon.com for data. The vast majority of scientific data and outputs cannot be easily discovered by other researchers even when nominally deposited in the public domain. Faced with a never-ending stream of new findings and datasets generated using different code and analytical techniques, researchers cannot readily determine who has worked in an area before, what methods were used, what was produced, and where those products can be found. Resolving such uncertainties consumes an enormous amount of time and energy for many social scientists. A new generation of automated search tools could help researchers discover how data are being used, in what research fields, with what methods, with what code and with what findings —often by passively capitalizing on the accumulated labor of one’s extended research community. And automation can be used to reward researchers who validate the results and contribute additional information about use, fields, methods, code, and findings.(<em>8</em>)</p>
<p>New advances in technology—and particularly, in automation—can now change the way in which social science, and hence other sciences, is done. The place to start is with the social sciences. The great challenges of our time are human in nature - terrorism, climate change, the use of natural resources, and the nature of work - and require robust science to understand the sources and consequences. The lack of reproducibility and replicability evident in many fields(<em>1</em>, <em>4</em>–<em>7</em>) is even more acute in the study of human behavior both because of the difficulty of sharing confidential data and because of the lack of scientific infrastructure. Social scientists have eagerly adopted new technologies in virtually every area of social science research—from literature searches to data storage to statistical analysis to dissemination of results(<em>8</em>). And, in the United States, the recent passage of the Foundations of Evidence-based Policymaking Act(<em>9</em>, <em>10</em>) and the focus on a Federal Data Strategy, mean that there is an important use case for showcasing the value of new approaches.</p>
<p>The knowing how it has been produced and used before: the required elements what do the data <strong><em>measure</em></strong>, what <strong><em>research</em></strong> has been done by what <strong><em>researchers,</em></strong> with what <strong><em>code</em></strong>, and with what <strong><em>results</em></strong>. Acquiring that knowledge has historically been manual and inadequate. The challenge is particularly acute in the case of confidential data on human subjects, since it is impossible to provide fully open access to the source files.</p>
<h1 id="how-this-book-came-to-be">How this book came to be</h1>
<p>This book was born out of a need to solve a very concrete problem. In 2016, the US Congress passed the Commission on Evidence-based Policymaking Act to make a set of recommendations on how to better use data for decision-making. The US Census Bureau was charged with supporting the deliberations of the Commission and asked our team at New York University to build a secure access facility in which data from multiple agencies could be securely hosted.</p>
<p>After we built the facility, and had dozens of users, we realized that putting data in one place, while necessary, was not sufficient for good analytical work to be done. Every user who accessed the data wanted to know what other work had been done with the data, with what assumptions and what results. We were able to provide them with some information, but essentially the information was drawn from our own research experience and was certainly not representative of the entire field. The obvious solution was to see if computer scientists had the technological tools available to automate the discovery of research datasets and the associated research methods and fields in research publications. Our computer science colleagues assured us that, while the technology existed in principle, no single team was known for having developed a solution.</p>
<p>We decided to see what we could to advance the field, and approached Schmidt Sciences, the Alfred P. Sloan Foundation and the Overdeck Family Foundation for support. As part of that support, we ran the competition with the results described in this book. We challenged participants to combine machine learning and natural language processing methods to identify the datasets used in a corpus of social science publications and infer both the scientific methods and fields used in the analysis and the research fields.</p>
<p>The core of the book describes both how the competition was set up, as well as the results achieved by different competing teams. However, as is always the case with exciting research agendas, the competition helped us identify five major scientific challenges that need to be addressed: (i) document corpus development, (ii) ontology development for dataset entity classification, (iii) natural language processing and machine learning models for dataset entity extraction, (iv) graph models for improving search and discovery, and (v) the use of the results to engage the community to both validate the model results, retrain the model and to contribute code and knowledge. So the other chapters in the book provide an overview of what could be done with more resources and talent devoted to this interesting question. The next section provides a more detailed overview of the contribution of each chapter.</p>
<h1 id="book-overview">Book overview</h1>
<p>Section 1 provides an overview of the motivation and approach. Section 2 describes new approaches to develop corpora and ontologies. Section 3 describes the competition results in terms of model development. Section 4 provides a forward looking agenda.</p>
<p>Section 1: Motivation and approach</p>
<p>In Chapter 2, " Where’s Waldo: Conceptual issues when characterizing data in empirical research," researchers from the Research Data and Service Center at the Deutsche Bundesbank show us why better metadata for social science data enables discovery of datasets and research, in ways that surpass what traditional metadata from data producers can support. They present a new modus operandi in the service delivery model of research data facilities, based on the premise that datasets have a measurable value that can be deduced from the relationships between datasets and publications, and the people who author, do research on, and consume them - that is, Rich Context around datasets.</p>
<p>They argue that a major advantage of rich context is that it closes the loop on metadata is closed: a loop initiated by the metadata from the data producer side, is closed by metadata from the data usage side. The authors elucidate why such rich data from the <em>usage</em> perspective is needed to deliver codified knowledge to the research community to guide literature review and new research; without understanding the linkage between datasets and outcomes, we are disabled in shaping new, impactful research.  </p>
<p>The authors identify two primary reasons for this need: first, that metadata on the datasets from the data users perspective helps the data creator to improve upon the quality of the data itself, improving dataset owners’ service delivery (e.g. bundesbank as a service provider, the service being data provision, consulting on dataset usage, creation of new data products, etc); and second, that metadata on the usage of datasets in publications helps us measure impact of datasets in their ability to drive policy-making. With this closed loop, the scope of researchers’ discovery is broadened to include not only literature and datasets, but the interplay between those two - that is, how datasets have been used by whom and how.   The authors discuss a tangible outcome of measuring dataset value - a dataset recommendation system, enabling expedient sharing of available datasets through the research community.</p>
<p>Chapter 3 outlines the operational approach that was taken to develop the <a href="https://coleridgeinitiative.org/richcontextcompetition">Rich Context Competition</a>. The goal of the competition, the results of which are described in Section 2, was to implement AI to automatically extract metadata from research - identifying datasets in publications, authors and experts, and methodologies used. As such, the competition was designed to engage practitioners in AI and NLP to develop models based on a corpus developed at the Interuniversity Consortium of Political and Social Research. The competition attracted 20 teams from around the world and resulted in four finalists presenting their results at NYU on February 15, 2019 (see the <a href="https://coleridgeinitiative.org/richcontextcompetition/workshopagenda">agenda and video here</a>).</p>
<p>The results of the competition provided metadata to describe links among datasets used in social science research. In other words, the outcome of the competition generated the basis for a moderately-sized knowledge graph. the <a href="https://ocean.sagepub.com/blog/an-interview-with-the-allen-institute-for-artificial-intelligence">winning team</a> in the Rich Context Competition was from <a href="https://allenai.org/">Allen AI</a> which is a leader in the field of using embedded models for natural language. Typical open source frameworks which are popular for deep learning research include <a href="https://pytorch.org/">PyTorch</a> (from Facebook) and the more recent <a href="https://ray.readthedocs.io/en/latest/distributed_training.html">Ray</a> (from UC Berkeley RISElab).</p>
<h1 id="section-2">Section 2:</h1>
<p>A major challenge is developing a training corpus that sufficiently represents the population of all documents, and tagging the datasets in the corpus. It is essential to do this well if high quality models are to be developed. There is a literature outlining the issues with developing a "gold standard corpus" (GSC) of language around data being mentioned and used in analysis in academic publications, since creating one is time-consuming and expensive (<em>12</em>) In Chapter 4 “Standardized Metadata, Full Text and Training/Evaluation for Extraction Models”, Sebastian tk and Alex Wade describe the need for, and strategies for collecting, large sets of annotated full-text sources for use as training data for supervised learning models developed in the Rich Context Competition. Dataset Extraction, the NLP task at the core of the Rich Context Competition, relies on a high-quality set of full text sources with metadata annotations. Developing such a corpus must be done strategically, as full-text articles and their metadata are organized inconsistently across their sources. The corpora gathered for use as training data for the Competition required ad-hoc manual labor to compile. Here, authors describe the legal, technological and human considerations in creating a corpus. They dictate the scale of full-text data needed, and the impact that an interdisciplinary (e.g spanning multiple domains) corpus has on that scale. They suggest development of a corpus with open-access text resources, use of human-annotators for labeling of full-text, and attention to the mix of domains that may be in a corpus when developing models. </p>
<p>There is a separate challenge of developing a common understanding of what a dataset is. Developing standard ontologies is a fundamental scientific problem, and one that is often in the domain of libraries and information scientists. Although some measure of linguistic ambiguity is likely to be unavoidable in the social sciences given the complex subject matter, even modest ontologies that minimally control the vocabulary researchers use would have important benefits. In Chapter 5, “Metadata for Administrative and Social Science Data”, Robert B. Allen describes a framework for the application of metadata to datasets, details existing metadata schema, and gives an overview of the technology, infrastructure and human elements that need to be considered when designing a rich metadata schema for describing social science data. </p>
<p>Allen describes three types of metadata - structural, administrative and descriptive; and emphasizes the growth needed in descriptive metadata, which are characterized by semantic descriptions. Allen describes existing metadata schemas which accommodate domain-specific metadata schema, like the W3C DCAT, and the unique semantic challenges faced by social science as opposed to natural sciences - in particular that concepts - e.g. “family”, “crime” -  are less well-defined, and definitions change across sub-domains. He considers data repositories and describes the essential role of metadata in making such repositories searchable and therefore useful. He touches on several prominent data repositories in the social and natural sciences and describes their methods of gathering metadata and how the metadata supports services offered, like search, computing environments, preservation of data for archives, and logging of the history of a dataset and its provenance. Allen describes other challengings in creating and maintaining metadata, prompted by things like changes in technology that yield data streams, and changes in metadata standards. He discusses some of the technology underlying data repositories; in particular data cubes for data storage that facilitate data exploration and retrieval; containerization and cloud computing enabling sharing and reproducibility; and collection management systems which can provide metrics on usage, like number of downloads, maintenance of datasets, etc. </p>
<h1 id="section-3">Section 3:</h1>
<p>Chapter 6, by the Allen AI team, describes their overarching approach. The team used a named entity recognition model to predict dataset mentions. For each mention, they generated a list of candidate datasets from the knowledge base. They also developed a rule based extraction system which searches for dataset mentions seen in the training set, adding the corresponding dataset IDs in the training set annotations as candidates. They then use a binary classiﬁer to</p>
<p>predict which of these candidates is a correct dataset extraction. While this approach was eventually the winning approach given the design of the corpus and the scoring mechanism, it suffers from being too fragile for general application, since it is necessarily corpus dependent. That team did not devote substantial time to identifying fields and methods.</p>
<p>Chapter 7, by the KAIST team, describes a very different approach. They generated their own questions about dataset names and use a machine learning technique to train the model for solving question answering task. In other words, questions suitable for finding dataset names such as “What is the dataset used in this paper?,” are generated and the question answering model is trained to find the answers to those questions from the papers. Furthermore, the resulting answers from the model are filtered by types of each word. For example, if an answer contains words with organization or agency types, then this answer is likely to include the actual dataset names. They also were quite innovative with identifying research fields, by using Wikipedia as the source, and methods by using machine learning techiques</p>
<p>Chapter 8, by the GESIS team, also used a Named Entity Recognition procedure. However, their design was module-based approach and they developed tools that can be used separately but also as parts of a data processing pipeline. For identifying research methods and fields, they exploited the Social Science Open Access Repository maintained at GESIS – Leibniz Institute for the Social Sciences. They also used the ACL Anthology Reference Corpus which is a corpus of scholarly publications about computational linguistics</p>
<p>Chapter 9, by the DICE team at Paderborn University, also used a Named Entity Recognition approach. They trained an Entity Extraction model based on Conditional Random Fields and combined it with the results from a Simple Dataset Mention Search to detect datasets in an article. For the identification of Fields and Methods, they essentially used search string to find embedded words</p>
<p>Chapter 10, by Singapore Management University, was an incomplete submission, with a very interesting approach. They used dataset detection followed by implicit entity linking approach to tackle dataset extraction task. They adopt weakly supervised classification for research methods and fields identification tasks utilizing SAGE Knowledge as an external source and as a proxy for weak labels.</p>
<h1 id="section-4-looking-forward">Section 4: Looking forward</h1>
<p>In Chapter 11, researchers from Digital Science describe the role user engagement plays in creating rich context around datasets, which are take on properties of ‘first class research objects’ (like journal articles) in that they are published as distinct research outputs in their own right.  Authors lay out a set of challenges in the sharing of datasets and dissemination of dataset metadata, and articulate goals in creating infrastructure to answer these challenges. </p>
<p>As technology has yielded ever larger streams of datasets available for social science research, two critical, interrelated elements of infrastructure have not kept apace: information infrastructure, and cultural infrastructure.  Information infrastructure refers to content of interest to the rich context competition models - journal articles, datasets, and their metadata (including details on the data stewards, usage of the datasets in research, and code used to prepare and analyze datasets). Cultural infrastructure refers to the incentives and value propositions in place to encourage individual data stewards, data users and experts to share datasets and contribute metadata on datasets. Cultural infrastructure around datasets do not fit into the existent culture of research publications. </p>
<p>In venturing to build out information infrastructure around datasets, we must consider how concepts like versioning, reproducibility, and peer review carry over to datasets. Further, how do metadata carry over, when there is so much variability in what we mean when we say dataset? Incentives around data sharing, dataset curation, and metadata contribution are even slimmer than in publishing, where there exists a culture of “publish or perish.” This question must be resolved if we wish to enrich the context around datasets to make them more efficiently consumable. </p>
<p>The future agenda is described in the concluding chapter by Paco Nathan and Ian Mulvany</p>
<p>The first step is to create a corpus of research publications to be used for training data during the Rich Context Competition.</p>
<p>The next step will be a formal implementation of the knowledge graph, based primarily on extensions of open standards and use of open source software. That graph is represented as an extension of a DCAT-compliant data catalog. Immediate goals are to augment search and discovery in social science research, plus additional use cases that help improve the knowledge graph and augment research.</p>
<p>In the longer term, the process introduces human-in-the-loop AI into data curation, ultimately to reward researchers and data stewards whose work contributes additional information into the system. With this latter step, in the broader sense Rich Context helps establish a community focused on contributing code plus knowledge into the research process</p>
<h1 id="more-resources">More resources</h1>
<p>General competition information</p>
<p>The competition had two phases. In the first phase, participants were provided with labeled data, consisting of a corpus of 2,500 publications matched to the datasets cited within them. Participants could use this data to train and tune their algorithms. In the second phase, they were provided with a large corpus of unlabeled documents and asked to identify the datasets used in the documents in a test corpus, as well as the associated methods and research fields. The participants were scored on the accuracy of their techniques, the quality of their documentation and code, and the efficiency of the algorithm – and also on their ability to find methods and research fields in the associated passage retrieval.</p>
<p>The timeline was as follows:</p>
<ul>
<li><p><strong>September 30 2018:</strong> Participants submit a letter of intent (see <a href="https://coleridgeinitiative.org/richcontextcompetition#howtoparticipate"><span class="underline">How to Participate</span></a>)</p></li>
<li><p><strong>October 15 2018:</strong> Participants notified and Phase 1 data provided (see <a href="https://coleridgeinitiative.org/richcontextcompetition#phase1participation"><span class="underline">First Phase Participation</span></a>)</p></li>
<li><p><strong>November 15 2018:</strong> Preliminary algorithm submitted (see <a href="https://coleridgeinitiative.org/richcontextcompetition#programreqs"><span class="underline">Program Requirements</span></a>)</p></li>
<li><p><strong>December 1 2018:</strong> 15 finalists selected (see <a href="https://coleridgeinitiative.org/richcontextcompetition#phase1evaluation"><span class="underline">First Phase Evaluation</span></a>) and Phase 2 data provided (see <a href="https://coleridgeinitiative.org/richcontextcompetition#phase2participation"><span class="underline">Second Phase Participation</span></a>)</p></li>
<li><p><strong>January 15, 2019:</strong> The algorithms of up to 6 teams are selected for final submission (see <a href="https://coleridgeinitiative.org/richcontextcompetition#phase2evaluation"><span class="underline">Second Phase Evaluation</span></a>)</p></li>
<li><p><strong>February 15 2019:</strong> Workshop is held in New York for final presentation and selection of winning algorithms (see <a href="https://coleridgeinitiative.org/richcontextcompetition#phase2evaluation"><span class="underline">Second Phase Evaluation</span></a>)</p></li>
</ul>
<p>All the information provided to participants was available here</p>
<p><a href="https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_Coleridge-2DInitiative_rich-2Dcontext-2Dcompetition&amp;d=DwMFaQ&amp;c=slrrB7dE8n7gBJbeO0g-IQ&amp;r=omwcNBUqPba9pikmkXZXk2bFQ7zxZPhI5OH9dd8lFDA&amp;m=jJJRJpvbdwLAeHNwur9CtaqPIY6UXS4q64avAMUSVV0&amp;s=abG_3lYZ3eu8BWs6kkau2rcXOIwLyiymwo0uj6vwGt0&amp;e=">https://github.com/Coleridge-Initiative/rich-context-competition</a></p>
<h1 id="references">References</h1>
<p>1. J. P. A. Ioannidis, Why Most Published Research Findings Are False. <em>PLoS Med</em>. <strong>2</strong>, e124 (2005).2. M. R. Munafò <em>et al.</em>, A manifesto for reproducible science. <em>Nat. Hum. Behav.</em> <strong>1</strong>, 21 (2017).3. G. Colavizza, I. Hrynaszkiewicz, I. Staden, K. Whitaker, B. McGillivray, The citation advantage of linking publications to research data (2019), (available at https://arxiv.org/pdf/1907.02565.pdf).4. C. F. Camerer <em>et al.</em>, Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015. <em>Nat. Hum. Behav.</em> <strong>2</strong>, 637 (2018).5. A. Dafoe, Science deserves better: the imperative to share complete replication files. <em>PS Polit. Sci. Polit.</em> <strong>47</strong>, 60–66 (2014).6. N. Young, J. Ioannidis, O. Al-Ubaydli, Why Current Publication Practices May Distort Science. <em>PLoS Med</em> (2008).7. G. Christensen, E. Miguel, Transparency, reproducibility, and the credibility of economics research. <em>J. Econ. Lit.</em> <strong>56</strong>, 920–980 (2018).8. T. Yarkoni <em>et al.</em>, “Enhancing and accelerating social science via automation: Challenges and Opportunities” (2019), , doi:10.31235/osf.io/vncwe.9. N. Hart, T. Shaw, Congress Provides New Foundation for Evidence-Based Policymaking (2018), (available at https://bipartisanpolicy.org/blog/congress-provides-new-foundation-for-evidence-based-policymaking/).10. Office of Management and Budget, Federal Data Strategy (2019), (available at https://strategy.data.gov).11. V. Bush, <em>Science, the endless frontier: A report to the President</em> (US Govt. print. off., 1945).12. L. Wissler, M. Almashraee, D. M. Díaz, A. Paschke, in <em>IEEE GSC</em> (2014).</p>
<hr />
<h1 id="chapter-2---bundesbank">Chapter 2 - Bundesbank</h1>
<p>﻿# Conceptual issues when characterizing data in empirical research</p>
<h1 id="building-blocks-for-user-centric-data-services-usage-data-to-support-empirical-research">Building blocks for user-centric data-services: Usage data to support empirical research</h1>
<p><strong>Stefan Bender<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, Hendrik Doll<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, Christian Hirsch<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></strong></p>
<p>Empirical economic and social science research uses microdata for analyses to connect theory to socie-tal problems. We present conceptual lessons learned from a machine learning competition held to au-tomate the discovery of datasets, research methods and fields in these research publications. Obtained information from the competition can be used to inform the debate about added value of the used (micro) data. Being able to measure societal benefits of data access is important to put funding decisions on an objective basis, since much research data is generated by publically funded researchers or available from official institutions. The obtained information from the competition can also be used to build a user-centric dataset recommendation system. Both of these outcomes will elevate the current knowledge generating process of empirical research.</p>
<h2 id="introduction-1">Introduction</h2>
<p>Policy makers increasingly recognize that informed decision-making requires data on the characteristics of units of a population, such as individuals, households, or establishments (i.e. microdata). Only microdata can uncover interdependencies between entities and document disparate global develop-ments. Making microdata available for independent research is subject to legal requirements that are designed to prevent the disclosure of information concerning an individual person or business entity. At Deutsche Bundesbank, the Research Data and Service Centre (RDSC) is tasked with making microdata available for independent research while simultaneously ensuring statistical confidentiality.</p>
<p>To strengthen effective quantitative research through optimal microdata usage, the RDSC has engaged in a series of projects that are targeted at enhancing user experience. One specific project currently pur-sued by the RDSC is the development of a microdata recommendation system, which is based on how microdata is being used in empirical research. Describing microdata from the usage in publications dis-tinguished this approach from traditional metadata for researchers, which is largely based on how data is produced.</p>
<p>Empirical research papers are an obvious source of information on dataset usage. A useful microdata recommendation system needs to rely on a corpus of dataset usage, as large as possible. Hand-curating such a sufficiently large corpus is prohibitively labor-intensive and error-prone. Thus, the competition with results described in this book to automate the discovery of datasets, associated research methods and fields in social science research publications lays the groundwork for any future implantation of such a recommendation system.</p>
<p>In this chapter, we present lessons learned from the competition described in this book. We do this from a background of all authors in social science. Readers interested in the more technical aspects of the competition may find Chapters 6 to 11 helpful, where participating teams explain their approaches in more detail. Furthermore, while our lessons learned revolve around datasets, fields, and mentions, Chapter 5 discusses the operational approach as well as lessons learned for designing such a competition. Finally, Chapter 13 presents in more detail a potential framework for implemeting a microdata recommendation system.</p>
<p>Extracting dataset citations from publications is a fairly difficult task because of the variety of dataset ci-tation formats and the absence of training data. Besides empirical research support, the gained infor-mation is the basis to provide value for policy purposes in the G20 context. For example, by providing researchers with information about the use and availability of microdata previously not being available in a systematic way, the results of the competition described in this book and the ensuing microdata recommendation system are a step towards reducing data gaps that have been diagnosed in the aftermath of the financial crisis.</p>
<p>On a broader level, the outcome of the competition contributes to the ongoing digitalization efforts of the Deutsche Bundesbank. Extracting relevant information from research papers as an unstructured data source broadens the value of unstructured, underexplored, data. Thus, the results of the competition presents a well-defined use-case to turn tacit knowledge into codified knowledge by converting text into relatively well-structured information. As a concrete first institutional implementation of competition results, microdata-based research will be supported by turning unstructured information into a useful source of reference for researchers.</p>
<h2 id="insights-from-a-research-data-centre-perspective">Insights from a research data centre perspective</h2>
<h3 id="background-on-research-data-centres">Background on research data centres</h3>
<p>Reseach data centres (RDC) present an established operational approach to facilitate access to confidential microdata for statistical purposes. This approach is based on the theoretical framework of the “Five Safes” which was initially developed by Felix Ritchie at the UK Office of National Statistics in 2003. <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> The first dimension refers to safe projects. This dimension mainly refers to the whether the intended use of the data conforms with the use specified in legislations or regulations. For example, a legislation may specifically allow users to use the data only for independent scientific research.</p>
<p>Safe people, the second dimension of the Five Saves framework, requires data users to be able to use the data in an appropriate way. A certain amount of technical skill or a minimum educational levels may be required to access the data. In contrast, safe data refers to the potential to de-identifying individuals or entities in the data. Safe settings relate to the practical controls on how the data is accessed. Different channels may exist which in turn may depend on the de-identifcation risk. In practice, the lower the de-identifcation risk the more restrictive the setting will be. Lastly, safe output refers to the risk of de-identifcation in publications from confidential microdata.</p>
<p>In response to the increased internal and external demand for microdata and the data confidentiality re-quirements, in 2013 the Bundesbank set up the Integrated Microdata - based Information and Analysis System (IMIDIAS) and established the Research Data and Service Centre (RDSC) (for a detailed moti-vation, refer to Kalckreuth, 2014 and Bender and Staab, 2015). At the RDSC of the Bundesbank, many of the technical and organizational measures put in place to protect confidential microdata follow the Five Safes framework.</p>
<p>A main principle of the RDSC is to give free access to Bundesbank micro data for independent research following the Five Safe approach. Motives for doing so are to stimulate cooperation projects between researchers inside and outside of Bundesbank, get feedback on relevant topics for Bundesbank (use published research results to increase the internal knowledge) and to strengthen evidence-based policy-making on for Bundesbank relevant topics. To fulfill these tasks, the RDSC has to ensure microdata is used effectively by providing excellent services. Implementing potential for structured feedback from researchers back to data production and new research enables an improving empirical knowledge generating process.</p>
<p>The data access provided by the RDSC of Bundesbank and the underlying legal requirements are described in detail by Schönberg (2018). In a nutshell, the requests of users to use microdata are first reviewed pursuant to legal requirements. After the application of a researcher is approved by the RDSC, researchers conduct their research project in a secure environment designed to ensure ongoing compliance with internal data policies and external government regulations. For most microdata this requires researchers to be physically present at the premises of the RDSC in order to analyse the data. Furthermore, only strictly anonymized research outcomes may be used outside of the secure environment.</p>
<p>The RDSC provides access to anonymized datasets on banks, securities, investment funds, enterprises and households, all of which can be accessed at dedicated researcher workstations or for most of the Bundesbank’s surveys – as for the Panel on Household Finances (PHF) study – the RDSC offers so called scientific use files. In addition, the RDSC provides information and advice to researchers on data selection, data content and analytical approaches. Together with the relevant statistical experts, it ensures that the microdata provided are documented in detail and archived. In doing so, the RDSC works according to globally rec-ognized standards and was accredited as a research data centre (RDC) by the German Data Forum (“Rat für Sozial- und Wirtschaftsdaten”).</p>
<h3 id="the-knowledge-generating-process-of-empirical-research-in-the-rdsc">The knowledge generating process of empirical research in the RDSC</h3>
<p>In this section we present the knowledge generating process of empirical research in the RDSC of Bundesbank. Figure 1 depicts this knowledge generating process which can be organized along the four key dimensions (i) data services, (ii) research, (iii) publication, and (iv) (structured) user specific knowledge. For simplification, we assume, that data services effects research and that the outcome of research is a publication. From a publication the RDSC (or Bundesbank) is able to destill user specific knowledge, which can be used for better services (to the “next generation” of users). In the following we discuss these dimensions in more detail and show how leveraging on (structured) user specific knowledge (dimension iv) may elevate the knowledge generating process to a higher level.</p>
<p><img src="./combined_images/20190614_Figure1.jpg" /></p>
<p>Figure 1: The four dimensions of the knowledge generating process of empirical research in the RDSC</p>
<p>The knowledge generating process starts from data sevices which are offered by the RDSC to researchers, who are analyzing data from the RDSC. Data services comprises raw microdata and comprehensive documentation of the data both of which the RDSC compiles together with the data-producing units in Bundesbank. <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> Furthermore, the data services dimension also includes the methodological improvement of microdata through e.g. applying record linkage techniques to facilitate the creation of new datasets for research. Finally, the RDSC also offers advisory services to potential and existing microdata users on topics such as e.g. dataset selection or analytical options.</p>
<p>The second pillar of the knowledge generating process in the RDSC is research. Researchers conduct their research project in the RDSC’s secure environment and produce research outcomes. These outcomes – after statistical disclosure control by the RDSC – sometimes take the form of publications, which present results to the interested public in a form optimized for human consumption as unstructured text. These publications contain knowledge accumulated by researchers about data usage over time (experience), e.g., knowledge about dataset particularities, which in turn could be utilized to inform the debate on how to improve data services.</p>
<p>Examples of user specific knowledge acquired by researchers include: + How data is used (e.g. additional data cleaning, variable transformation, combining datasets, using additional information) + What purposes data is used for (e.g. topics, methodology, research area) + What kinds of analyses or techniques have been tried and are used ultimately + What information about data is most valuable to get to the results, respectively which linkage or data enrichment renders the data most valuable.</p>
<p>Being able to access structured user-specific knowledge through the competition described in this book enables improving data services by making discovery of data and related projects, people, and publications at Bundesbank more comprehensive and efficient. For example, knowledge harvested from publications may be used to enhance services provided by RDSC by allowing standard datasets to be tailored to the needs of re-searchers. Similarly, data producers benefit from feedback on their data, allowing them to improve data quality.</p>
<p>The challenge is to establish such a feedback loop. If effective feedback is given and used, the microdata-based knowledge-generating process restarts with data services, but it is elevated to a higher level. The effect of being able to leverage on information from this feedback loop is depicted in Figure 2. Better data services in turn allow better research, because available microdata is better customized and more effectively used. Better research will be published in higher ranked journals and will generate more relevant user specific knowledge, which can be used again for better services. To sum up, establishing a knowledge generating process as we have shown will lead to improvements in the four key dimensions of this process of empirical research.</p>
<p><img src="./combined_images/20190614_Figure2.jpg" /></p>
<p>Figure 2: Elevating the knowledge generating process of empirical research in the RDSC to a higher level by enabling a feedback loop to data services.</p>
<p>At the moment, this feedback loop is not present in a systematic way. The aim of the competition presented in this book is to identify appropriate procedures to close the gap between publication and data services, which would enable transforming knowledge available in publications into generally re-usable knowledge to inform stakeholders (data producers, RDSC, decision makers at Bundesbank). The results of the competition will thus ultimately enable better data services which in turn will make research outcomes more efficient through the channel of a better data usage.</p>
<h3 id="added-value-of-structured-user-specific-knowledge">Added value of structured user-specific knowledge</h3>
<p>This section details two applications of obtaining dataset usage information from publications that would add value to the data services provided for the RDSC. First, existing applications can be optimized in a user-centric way which would lead to obtaining refined products (e.g. improved researcher recommendations and data documentation). Second, the case for societal investment in free data access can be empirically fortified. Positive externalities (i.e. research as a public good) suggests a less then societally optimal provision of research data and related services. Obtaining a dataset impact factor can then make the case for further investment in microdata provision by concretely showing a dataset’s impact.</p>
<p>The structured user-specific knowledge produced during the competition may be used to inform the design of a dataset proposition system for researchers. By obtaining information on dataset usage in publications, data is for the first time available to construct indices on data set joint usability (and dataset maps to visualize such indices). Such an index connects datasets through actual use by researchers that combined data sets in the past. This enables recommendations, such as, “Researchers, who used dataset A, also used dataset B”.</p>
<p>Going further, the usability index can be expanded into a measure, how well new datasets fit each other. Without needing joint dataset usage in past publications goodness-of-fit measures may be predicted based on dataset usage in the same field, using the same methods or by additional metadata similarity. This can be a valuable accelerator to effectively distribute new datasets in the research community. While both indices can be implemented using only information from the competition, extensions may enhance value to users which are based on other information such as current metadata.</p>
<p>When thinking about user recommendations, the example is set by large online platforms. These online platforms can recommend from two dimensions of information (excluding interaction for simplicity). First, data is available on a large number of observed purchases per customer, which enables statements like “since you like products A and B, you might also like C”. Second, data is present on large numbers of observed customers per product, which enables statements like “users like you also bought”.</p>
<p>In our setting, with the knowledge generating process of empirical research in the RDSC, we consider researchers and datasets. The universe of data users and researchers is decently large (i.e. the first dimen-sion), but per user, we only observe a limited amount of “dataset consumption” (i.e. the second dimen-sion). Hence, we have a decent chance of recommending based on other users behavior. However, we have only limited means of predicting a single users future datasets needs based on his past personal “dataset shopping” behavior.</p>
<p>However, we suspect a simpler underlying behavioral model of “data shopping” compared to shopping through large online platforms, because publishing with one dataset is not a casual purchase. Instead, it implies real commitment relating to being content with the purchase (less cognitive dissonance). Thus, we suspect that, compared to online platforms, less data points per person are needed, in order to make sensible recommendations. Also, in order to gain more of the rare information per user, we can fall back on dataset citations, i.e. “indirect data usage”, as outlined in Chapter 3 of the book.</p>
<p>A challenge in building a data-driven recommendation system is to make sure that recommended da-tasets are indeed feasible to use, i.e. constitute meaningful recommendations. Thus, besides information about datasets, additional information such as fields and methods is needed to be ingested into the system. This additional information essentially constitutes additional links between datasets that helps better align datasets. This is especially true in the finance domain where linking microdata is a common feature in empirical research.</p>
<p>Second, the RDSC as part of a public institution has a responsibility towards its principals, i.e. society. Granting data access free of charge for researchers should be backed by empirically measurable bene-fits of such data provision. Benefits from data usage can justify societal investment in free data access. However, measuring societal benefits through data access is not obvious at first glance. One possible starting point of approximating societal benefits of data access can be to measure the creation of knowledge <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> created by specific datasets.</p>
<p>One can argue that added value of providing administrative microdata is the marginal benefit relative to the second-best comparable commercial database, if such a database exists. Also, one can argue that a dataset, which enables causal evidence, adds more value to societal knowledge, compared to previously available datasets, from which only correlations could be deduced if an important goal is to inform the policy debate. However, both of these methods require identifying which empirical result from a publication can be attributed to which dataset.</p>
<h2 id="lessons-learned-from-competition">Lessons learned from competition</h2>
<h3 id="related-literature">Related literature</h3>
<p>Extracting dataset citations from publications is a fairly difficult task because of the variety of dataset citation formats and the absence of training data (for a recent overview of data retrieval see Koesten et al., 2019). Boland et al (2012) propose a weakly supervised approach, using a pattern induction method for the detection of study references in full texts. They use a corpus of 259 publications from the Social Science Open Access Repository (SSOAR). They use a bootstrapping approach, starting with a small corpus of manually created training instances. The resulting system InfoLink now informs SSOAR.</p>
<p>Boland and Mathiak (2015) describe dataset extraction as a twofold task, finding dataset citation string and following entity resolution (match the string to the correct entity/ DOI). Concerning entity resolution, they report the difficulty of broad survey dataset citations that ignore data variability (such as years, versions, questionnaire variants, etc.), motivating a dataset taxonomy. Named dataset citations are often underspecified allowing identification of the survey but not of the precise dataset (which of multiple sub-samples, aggregation levels, survey modes, etc.).</p>
<p>Zhang et al (2016) use a bootstrapping approach to extract dataset citations from 116 computer science journals publications. Ghavimi et al. (2016) use a similar approach for social science papers finding datasets with well-documented metadata. According to them, only 25% of all dataset citations are given in the references, highlighting the unstructured citation culture for datasets. We advance from these with an environment with less available dataset metadata and a corpus of publications from a variety of fields for our purposes. To tackle this, we continue with a larger hand-curated annotated corpus.</p>
<p>Metadata schemas for datasets are available, such as the DataCite metadata schema and the da|ra metadata schema, which complies with the DataCite schema (Helbig et al. 2014). They offer dataset taxonomies and standardized citation propositions, however their categories do not optimally support automatic search and extraction, if no unique dataset identification (such as a DOI) is used. <a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> In the con-text of central banks that provide microdata, recent progress has been made in the context of INEXDA. A metadata standard (in line with DataCite) has been developed (Bender et al. 2018) and datasets pro-vided by the RDSC are all DOI registered.</p>
<p>Improving dataset citation is high on the scientific agenda in recent years. This notably includes promot-ing widespread usage of persistent and unique dataset identifiers. As available datasets spread across a large number of databases, identification of datasets is important for reproducibility and to credit data creation efforts to incentivize data creation and publication (Lagoze and Vilhuber 2017, McMurry et al. 2017, Mooney and Newton 2012). If unique and persistent dataset identification in publications were available, Ball and Duke (2011) raise the idea of dataset impact factors with such information.</p>
<h3 id="dataset-mentions">Dataset mentions</h3>
<p>This section presents lessons that we learned throughout the duration of the competition described in this book. These lessons originated from our motivation presented in the previous chapter and the role that we assumed. This role was twofold. First, we contributed to the corpus of publications and metadata given to competition teams. Our contribution consisted of a corpus of tagged publications of Bundesbank working papers, some of which use microdata provided by the RDSC. Second, we contributed to both evaluation phases of the competition. <a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<p>We organize this section around the three sets of information that where the main focus of the competition: datasets mentions, research fields, and (statistical) methods used. We begin by describing our a priori expectation of what a dataset is. We did not delve into definitions of a dataset but rather considered it sufficiently defined for our purposes (as empirical social scientists and for the competition).</p>
<p>Since our approach depends on getting to know the user-perspective, we thought it plausible to let usage in empirical papers define a dataset for the purpose of the competition. Having a background in working at a large provider of financial data, we had a vague idea that all datasets would look like those the RDSC provides access to, which consist mostly of collections of structured data in matrix or database form. These datasets typically are defined by a name and with a well-defined scope, thus allowing clear citation, usually including a unique dataset identifier (such as a Digital Object Identifier, DOI).</p>
<h4 id="lesson-1-datasets-fall-into-two-broad-categories">Lesson #1: datasets fall into two broad categories</h4>
<p>Since the corpus of publication used for the competition spanned different domains (like healthcare, education, and others), we quickly realized that our dataset image had an econocentric bias. In social science, we learned, datasets can be categorized into two broad categories for the purposes of extraction. First, there are named datasets, i.e. well defined, usually large-scale and publicized datasets (e.g. Compustat).</p>
<p>Generally, named dataset mentions are short strings in the publications, have commonly used abbrevia-tions (e.g. MMSR), and often containing institution name or name of commercial data vendor. Some-times (rarely, but increasingly) these datasets can be identified by a unique digital object identifier (DOI). These datasets are usually well-defined in scope and time, with formal documentation available. While data is usually collected with a specific purpose in mind, such datasets are be used across multiple pa-pers and research domains.</p>
<p>The second dataset category is what we call created datasets. By created dataset we understand da-tasets usually collected or built by authors of a publication for the purpose of analyzing one specific re-search question. Often, created data comes in the form of small-scale surveys, (structured) interviews, or randomized controlled trials, RCTs. Such data normally does not have a trademark name, but instead one or multiple paragraph descriptions in the publication. Dataset information is blended together with information on data collection and sampling methods. Data reference at its most condensed form then comes in a structure like “we interview a given number of participants in a given region suffering from a given disease and code responses in the following way”.</p>
<p>In contrast to named datasets, created datasets usually are not referred to by a specific string or com-monly used abbreviation. Data collection is usually paper specific, and the universe of existing datasets are not easily searchable. This makes it hard for text mining algorithms to correctly extract strings referring to dataset entities. Specific created datasets are harder to use for follow-up research, and reproducibility is given only if publishers provide data together with the paper. Therefore, the lack of unique identification and search terms renders data collection potentially redundant and dataset spread not optimal.</p>
<h4 id="lesson-2-fractions-of-dataset-category-are-domain-specific">Lesson #2: Fractions of dataset category are domain specific</h4>
<p>Throughout the competition it became clear that the fraction of named and created datasets varies across social science domains. Since different fields of social sciences rely on different identification techniques and differing potentials for conducting RCTs, the predominantly used data sources naturally vary. This has important repercussions for designing a competition, since algorithm performance and later recommendation system performance varies with the input corpus and the application field.</p>
<p>The number of datasets used per empirical paper (linked data) also varies across research areas. This number is also dependent on named vs. created datasets. In fields with widespread use of multiple datasets at once, the added value of recommending additional useful data might be expected to be higher than in fields that create study-specific data every time. Conversely, one could argue that the marginal utility of adding additional datasets is decreasing.</p>
<p>The optimal way forward is to start a data recommendation system for research field with higher expected marginal utility from additional datasets. In our view, these are research areas with widespread usage of named datasets. Named datasets are constructed without the concrete research question in mind. That is why information to answer a particular research question often has to be obtained from more than one data source and is particularly true in empirical economic and finance research.</p>
<h4 id="lesson-3-unique-identification-of-datasets-remains-an-issue">Lesson #3: Unique identification of datasets remains an issue</h4>
<p>From the distinction above, one could make the argument that named datasets are easier to identify than created datasets. However, this is not the case, because the same dataset name can refer to multiple subsamples or waves of same datasets, and it is unclear where to make distinctions between dataset entities. This makes it difficult to identify the mentions referring to the same data points. Issues are, just to name a few, different time periods or subsamples, different states of data and states of knowledge, computational data pre-processing or enrichment steps. These identification issues render the current task of entity resolution of extracted dataset mentions complicated.</p>
<p>Unique dataset identification carries significant repercussions for reproducibility purposes, where identi-fying the exact data used for a study is paramount. For reproducibility purposes, the current solution to this dataset identification problem is the direct data upload to the publisher together with the publication. This is neither storage-efficient for large datasets nor feasible in the case of confidential microdata. A more flexible way to solve this issue is to assign unique identifiers (DOIs) to the datasets.</p>
<p>With a DOI (identifying the exact time frame, sampling universe, data version, wave, aggregations, state of knowledge, etc.), datasets are identified and quantitative research using confidential microdata is re-producible. To make lives easier, DOIs also drastically facilitate the automatized extraction of well-defined datasets from publications (comparable to largely standardized citations of other publications, allowing easy retrieval of publication networks, etc.).[^footnote6]</p>
<p>[^footnote6] An alternative approach to ensure identification of datasets could be providing richer, more systematic metadata for datasets. See Chapter 4 in this book for a detailed discussion of this point.</p>
<p>Summarizing, if we successfully identify datasets and solve the issue of entity resolution, we can link and propose created datasets and thereby enable further research with such data, which takes up a notable fraction of publications in certain fields. While this task is harder than for named datasets, the potential for improvement remains larger as of today. For created datasets, too, DOI usage would be desirable; however encouragement or enforcement to use DOIs is harder in this case, because of a larger target group – authors instead of a limited number of data stewards. Even in case of widespread DOI usage for named datasets, the competition algorithms yield valuable results through the created datasets extraction in order to allow referencing and making available datasets used in the past for further analysis.</p>
<h4 id="lesson-4-datasets-mentions-could-indicate-used-for-analysis-vs.-cited">Lesson #4: Datasets mentions could indicate used for analysis vs. cited</h4>
<p>After a discussion about dataset types and usage in fields, the last lesson that we learned about datasets concerns the mention of datasets in publications. These mentions come in two types. First, datasets used for empirical analysis and second, cited datasets in the literature review or references. Dataset citations (without empirical usage) can generally occur in the literature review section, even in theoretical, methodological papers, e.g. a given paper might report summary statistics based on datasets (“Author Y uses Compustat to…”). Sometimes differences between cited and used datasets are only semantic in nature. In well-written papers, the difference is usually fairly easy to distinguish for humans, but less clear for algorithms.</p>
<p>A key lesson we learned, is to think ahead of time, what the informational need is for the use-case at hand, used or cited datasets. Note that in an optimal setting, if information were available on the universe of datasets used for analysis in papers and on all publication citations, dataset citations would be redundant. This comes from the fact that a dataset citation in one publication is based on a dataset used for analysis in another publication and can be linked via available literature citations.</p>
<p>While literature citations are mostly standardized within research domains and are relatively straightforward to extract (hence publication networks / publications maps exist), information on used datasets in papers remains incomplete (even after the competition). Because of this, for the competition, we asked for used and cited datasets. It is important to note, that extracted dataset citations are always incomplete, since some authors report aggregate statistics from a different paper, but not the data behind (“Smith et al show…”).</p>
<p>If well separated, through extracted dataset citations, one obtains a “dataset map”, thus the “closeness of datasets”, and network measures such as centrality distinguishing important datasets (“nodes”). Through extracted empirical dataset usage on the other hand, one obtains relevant information for our purposes, namely information relating to dataset similarity and joint usage possibilities from the user perspective. However, for our envisioned recommendation system, usage of cited data (“indirect” data usage) is a valuable feature, since it yields more limited data on dataset “purchases” of a user.</p>
<p>As training data for the algorithms it is important to include theoretical literature, essays, etc. in the corpus of publications. [^footnote7] Obviously, this is helpful for algorithms to correctly identify true negatives, i.e. correctly identifying theoretical papers. For this task, distinguishing between cited and used datasets becomes relevant once again, because clearly separating theoretical papers that merely cite data from empirical papers depend on such a distinction.</p>
<p>[^footnote7] Chapter 12 discusses potential strategies for collecting training data in more detail.</p>
<h3 id="fields-and-methods">Fields and Methods</h3>
<p>The competition also asked participants to extract information about research fields and methods used in the publication. We want to gather this information from the user side, because data producers and annotators do not necessarily foresee all usage potential for their data and the point of our envisioned system is to increase user value. One such idea is to construct dataset similarity indices from the usage side, information is relevant not only on existing joint usage by others (“people like you often used dataset Y, too” – hence dataset extraction), but also on new dataset or linkage potentials (“this might also interest you based on your preferences”). For this, information is necessary on the context, how datasets are used.</p>
<h4 id="lesson-5-think-before-you-act-define-fields-and-methods">Lesson #5: Think before you act: define fields and methods</h4>
<p>To obtain the most relevant categories of research fields, we did not provide any thesauri to the competition, on purpose. The rationale behind this was to see the unhindered creativity of teams, which available information sources they would use or not use (e.g. reference datasets, Wikipedia, archive.org, other repositories, thesauri, statistical clustering techniques, etc.). On the other hand, thesauri limit the catalogue of potentially identifiable fields and methods, thus prohibiting new methods and fields to be identified in fast-changing modern research areas. Also thesauri might disturb algorithm performance, since algorithm might be forced to categorize topics and fields to older or less exact categories than necessary.</p>
<p>However, using thesauri does have well-known advantages, as any librarian will confirm. These advantages include easy clustering of similar fields and methods and a manageable category set of predictions. For field predictions, we generally face a fine line between too broad predictions (safe, but unin-formative) and too narrow predictions (narrow, but potentially wrong). A potential way out is backward induction here – we can present differently aggregated predictions for fields to users and get feedback from them (let users rank usability – “Was this helpful to you?”).</p>
<p>Concerning our definition of methods for the purpose of the competition, two questions arise. The first is the definition of statistical methods (i.e. inclusion of sampling methods, qualitative methods, etc.). Secondly, there are multiple statistical methods in a publication (besides the main causal analysis, there can be methods reported for data preparation, sampling, baseline results, robustness checks, descriptive statistics, etc.) and issues of potential weighting of importance of these.</p>
<p>For useful new recommendations to be provided to researchers, we decide to include in statistical methods all methods that describe potential for a merge of datasets / joint usability, hence to include all the above listed. We consider a broad definition of methods, not only including high-level statistical methods, such as ordinary least squares, but also including the observed unit, time period or even re-gression equations. If two papers then use different datasets in the same field using the same methods, there is a relatively high likelihood that those datasets can be linked or used together to create new in-sights.</p>
<h3 id="discussion">Discussion</h3>
<p>Several decades ago, publication citation networks were constructed and to our knowledge no such undertaking has yet been done for datasets. This comes from the fact that no curated training data corpus is readily available in decent quality. Since no such data is available, we manually annotate papers for the competition and now propose to go forward with this in a larger scale.</p>
<p>We would have no need for this competition in a world with universal dataset identifier usage (such as DOIs). In such a scenario, unique identification and standardized citations of datasets would be readily available. Since DOIs only now and slowly gain widespread application for datasets in social science, our task is a 1:n mapping of publications to datasets without unique identifiers. For scientific papers, many journals already provide DOIs for papers.</p>
<p>There are ongoing efforts by journals to have all used data published for reproducibility reasons. Incentivizing researchers to provide unique identification of datasets used in papers is a logical next step. This will ensure reproducibility for confidential microdata and facilitate our use-cases. In the meantime, we show a way forward to learn from the current state of information and analytically use presently available information.</p>
<p>The competition highlights that datasets can be categorized in different dimensions for the purposes of extracting dataset mentions from publications. We propose a binary distinction of datasets into named as opposed to created datasets. As named datasets, we consider formal, large datasets by commercial or official institutions, often referenced in relatively standardized forms as commonly used abbreviations. Created datasets are those created for the specific purpose of one research question in mind. They are generally described in less standardized paragraphs. Usage of named versus created datasets varies across research areas.</p>
<p>Also varying across research areas is the number of datasets used per empirical paper. This number al-so depends on the spread of formal, named datasets as opposed to created datasets for single studies. In fields with widespread use of multiple datasets at once (linked data), the added value of recommending additional useful data might be expected to be higher than in fields that create study-specific data every time. Conversely, one could argue that the marginal utility of adding additional datasets is decreasing. The optimal way forward is to start a data recommendation system for research field with higher expected marginal utility from additional datasets.</p>
<h2 id="conclusions">Conclusions</h2>
<p>In the competition described in this book, we asked teams to extract datasets, fields and methods from a corpus of hand-annotated research publications. The value of the extracted information lies in informing a user-centric dataset recommendation system and thereby enabling optimal and timely spread of available datasets throughout the research community. Furthermore, such information allows us to compute dataset impact factors by obtaining data-driven information on which datasets underlie high-quality research outputs. This in turn is a proxy for societal benefits of data provision by research data centres, thus motivating in-vestment in data access infrastructure.</p>
<p>We introduce a circular model of the knowledge generating process, which increases in levels. From data services, research is conducted, publications are published and user-specific knowledge is generated. Having such knowledge on dataset usage, data services in turn can be improved, which elevates research, publications and user specific knowledge. Thereby, the circle repeats on a higher level. The current competition works on strengthening the knowledge pillar as well as the transmission mechanisms from publications to knowledge to improved data services. <a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p>Automatic processing of generated knowledge in publications becomes increasingly available with modern text analysis tools. Extracting such information is important, because timely and optimal usage of gained results increases the speed, by which findings can be incorporated into data services and thereby next-level research is enabled in turn. To further improve automatic processing, minimum standards for dataset taxonomy are needed. Harmonized metadata schemas for data sets – like the INEXDA metadata schema for central banks and statistical offices (compliant with and building upon DataCite) – offer such an approach.</p>
<p>The competition showcased that information extraction of the necessary information for such systems is possible. The delivered prototype algorithms prove this claim. With the proof of concept, there is a more substantiated case for investing in a larger hand-curated training corpus of annotated research papers. On the road towards a user-centric dataset recommendation and metadata system, the competition forced us to clarify organizational needs and methodological aspects.</p>
<p>For the way forward, it is important to note the importance of the research area on the strategic path towards a unified user-centric microdata recommendation system. The choice of the research domain will greatly influence algorithm performance. Since human effort in creating training data is expensive, one should deliberately pick research domains to start with. This arises because text extraction algorithms (and humans) struggle with informally described created datasets. The low-hanging fruits of prototyping dataset recommendation systems, usability indices etc. are easier to implement for research areas with a largely formalized dataset citation culture (however ultimately potential for benefits may well be larger in other research areas).</p>
<h3 id="references-1">References</h3>
<ul>
<li>Ball, A., and M. Duke (2011): How to cite datasets and link to publications. Digital Curation Centre.</li>
<li>Bender, S., Hausstein, B., &amp; C. Hirsch (2018). An Introduction to INEXDA’s Metadata Schema. Technical Report 2018-02, Deutsche Bundesbank, Research Data and Service Centre.</li>
<li>Bender, S. and P. Staab (2015). The Bundesbank’s Research Data and Service Center (RDSC), Gateway to treasures of microdata on the German financial system. IFC Bulletin 41 (2015).</li>
<li>Boland, K., Ritze D., Eckert, K., &amp; B. Mathiak (2012): Identifying references to datasets in publica-tions. Theory and Practice of Digital Libraries, pp. 150-161. Springer Berlin Heidelberg, http://doi.org/10.1007/978-3-642-33290-6_17</li>
<li><pre><code>  Desai, T., Ritchie, F., &amp; R. Welpton (2016). Five Safes: designing data access for research, Working Papers 20161601, Department of Accounting, Economics and Finance, Bristol Business School, University of the West of England, Bristol</code></pre></li>
<li>Ghavimi, B., Mayr, P., Vahdati, S., &amp; C. Lange (2016). Identifying and improving dataset references in social sciences full texts. arXiv preprint arXiv:1603.01774.</li>
<li>Helbig K., Hausstein B., Koch U., Meichsner J., &amp; A. Kempf (2014): da|ra Metadata Schema. Gesis Technical Reports 2014/17, DOI:10.4232/10.mdsdoc.3.1</li>
<li>Von Kalckreuth, U. (2014). A Research Data and Service Centre (RDSC) at the Deutsche Bundes-bank–a draft concept. IFC-Bulletin No 37, Irving-Fisher Comittee on Central Bank Statistics.</li>
<li>Koesten, L., Mayr, P., Groth, P., Simperl, E., &amp; M. de Rijke (2019): Report on the DATA: SEARCH’18 workshop-Searching Data on the Web. ACM SIGIR Forum (Vol. 52, No. 1, pp. 117-124). ACM.</li>
<li>Boland, K. &amp; B. Mathiak (2015). Challenges in Matching Dataset Citation Strings to Datasets in Social Science. D-Lib Magazine 21, 1/2.</li>
<li>McMurry, J. A., Juty, N., Blomberg, N., Burdett, T., Conlin, T., Conte, N., &amp; A. Gonzalez-Beltran, A. (2017). Identifiers for the 21st century: How to design, provision, and reuse persistent identifiers to maximize utility and impact of life science data. PLoS biology, 15(6), e2001414.</li>
<li>Mooney, H, &amp; M. P. Newton (2012): The anatomy of a data citation: Discovery, reuse, and credit. eP1035-eP1035.</li>
<li>Schönberg, T. (2018): Data Access to Micro Data of the Deutsche Bundesbank. Bundesbank Tech-nical Report 2018-01.</li>
<li>Vilhuber, L. &amp; C. Lagoze (2017): Making Confidential Data Part of Reproducible Research. Chance</li>
<li>Zhang, Q., Cheng, Q., Huang, Y., &amp; W. Lu (2016). A bootstrapping-based method to automatically identify data-usage statements in publications. Journal of Data and Information Science, 1(1), 69-85.</li>
</ul>
<hr />
<h1 id="chapter-3---digital-science-use-cases">Chapter 3 - Digital Science Use Cases</h1>
<h1 id="chapter-3-digital-science-use-cases-enriching-context-and-enhancing-engagement-around-datasets">Chapter 3 – Digital Science Use Cases: Enriching context and enhancing engagement around datasets</h1>
<p>Christian Herzog<sup>1,a</sup>, Daniel W Hook<sup>1,2,3,b</sup>, Mark Hahnel<sup>1,c</sup>, Stacy Konkiel<sup>1,d</sup>, and Duane E. Williams<sup>1,e</sup></p>
<p><sup>1</sup>Digital Science, London, N1 9XW, UK</p>
<p><sup>2</sup>Department of Physics, Washington University in St Louis, St Louis, Missouri, USA</p>
<p><sup>3</sup>Centre for Complexity Science, Imperial College London, London, SW7 2AZ, UK</p>
<p>(<sup>a</sup><a href="https://orcid.org/0000-0002-9983-0033"><span class="underline">https://orcid.org/0000-0002-9983-0033</span></a>, <sup>b</sup><a href="https://orcid.org/0000-0001-9746-1193"><span class="underline">https://orcid.org/0000-0001-9746-1193</span></a>, <sup>c</sup><a href="https://orcid.org/0000-0003-4741-0309"><span class="underline">https://orcid.org/0000-0003-4741-0309</span></a>, <sup>d</sup><span class="underline"><a href="https://orcid.org/0000-0002-0546-8257" class="uri">https://orcid.org/0000-0002-0546-8257</a></span> <sup>e</sup><a href="https://orcid.org/0000-0002-2111-3413"><span class="underline">https://orcid.org/0000-0002-2111-3413</span></a>)</p>
<h2 id="introduction-2">3.1 Introduction</h2>
<p>The relationship between research, researchers and data is changing. Data has always played a critical role in scientific research, however in recent years it has taken centre stage not only for the so-called hard sciences, but also for the social sciences, and it has an increasing role in the humanities (Giuliano, 2019). We assert that this change is, at least in part, driven by two key factors: First, the increasing volume of data that is available to researchers either, for example, through the increasing sensitivity of instruments that aid experimental work, or through the ubiquity of computer systems with which we interact in our daily lives. Second, our ability to process and analyse these data is growing quickly as computers become faster and algorithms become more powerful. While some researchers welcome having more data to work with, others are challenged or marginalised in this new data-rich world (Eijnatten et al., 2013; Grusin, 2014; Leurs and Shepherd, 2017). These effects are often compounded by the tools that researchers must master to connect their research to data.</p>
<p>In the hard sciences, the CERN Open Data Portal contains 131 datasets describing particle collisions, each of which comprise around 300Gb of data at the time of writing (CERN, 2019). In the social sciences, the CISER Data Archive at Cornell is home to more than 1000 different social science data sets (CISER, 2019). These examples are individual instances chosen at random from many that could be used to demonstrate the variety and scale of data available for research. However, even these examples don’t begin to quantify the amount of detailed personal data available to companies such as Facebook. Data of this latter kind has already been used in academic studies as well as in more controversial contexts (Jordan and Weller, 2018; Kamp et al., 2019; Stark, 2018). Clearly, there is an increasing diversity and depth of data available for research from both traditional and from new sources.</p>
<p>Many researchers now work with large volumes of data. Fortunately, many facilitating technologies have become commoditised and are available at a fraction of their original cost: storage is cheap and data transfer is fast. But, Increasing the value of data to researchers is no longer about technology, rather it is about the information and culture around the data.</p>
<p>In this chapter, we take our lead from Chapter 1 in recognising not only that science is at a crossroads but that the whole of research is changing. We discuss two elements of infrastructure that, if enhanced, can make data more useful and valuable to the whole research community: information infrastructure and cultural infrastructure. The Rich Context project supports the development of tools that enrich not only information infrastructure around datasets, but which also enhance the cultural infrastructure. <em>Information infrastructure</em> includes details of the approach to data stewardship, context of usage, code applied to the dataset in its production, as well as code applied to the data to derive further results or translate it for practical uses. All these factors add critical elements to the research infrastructure. <em>Cultural infrastructure</em> includes creating the incentives, triggers and frameworks that encourage the dataset stewards, experts and users to contribute to these critical information elements.</p>
<h2 id="information-infrastructure">3.2 Information Infrastructure</h2>
<p>Information infrastructure can be defined as the collection of processes and artefacts that are foundational to today’s scholarly communications. A simplified model of scholarly communications would have artefacts such as journals, journal articles, article metadata and citations. In this case, the processes would be peer review and scholarly search.</p>
<p>When creating one of the first scientific journals <em>Philosophical Transactions</em> 350 years ago, the members of the Royal Society did not have today’s data-centric world in mind. While a clear line can be drawn from the articles of that time to the articles of today, infrastructures have grown up around research publications in the intervening years that have moved the structures and expectations of the research article forward. These norms are powerful and persistent through their ubiquity. For example, in the large majority of modern research literature, we continue expect articles to be grouped into journals and published on a specific date, and we expect there to be a version of record that constitutes a definitive record of a piece of research.</p>
<p>Data is more fluid than a standard research article: it is produced and updated more frequently and iteratively; it needs to be shared with many in a collaborative context; it is processed and versioned by different colleagues. Data does not fit into the normalised research publication. Research fields that rely on data are beginning to publish data as a distinct output from a research article. Data is becoming a principal research output, while the technological challenges of publishing data are being addressed, the format and necessary fields of the metadata that describe data, the file format in which the data resides, the resource to annotate the data to make it useful to others, the way in which data should be cited in a paper or by another dataset, the description of the processing that has been applied to the data, the details of the ethical review process behind the exercise that gathered the data, and many other norms do not yet exist homogeneously across subjects and geographies. There are not yet strongly established norms that help researchers to have trust in data.</p>
<p>A dataset can change with time for many reasons: data may be added over time, corrections may be issued, and so on. In these cases, it may be appropriate to “version” the dataset (by issuing a persistent identifier for a point-in-time snapshot for the dataset, allowing subsequent changes to receive their own “versions”). But changes to a dataset may have a knock-on effect on the interpretation of the data and may fundamentally alter the research result that was originally reported. Moreover, in many fields “Big Data” is so central that it not only puts pressure on the community to establish an acceptable model of data publication, but also puts significant stress on how we read, interpret, and review research as a whole.</p>
<p>Many datasets are now so vast that we lack the ability as humans to consume them in an easy way. Visualisation technologies and other tools that allow us to interact with and sample data dynamically have received significant attention in recent years, and have helped with the interpretation of data in online environments. But it is simply impossible to reduce some types of data to a single figure or printable table, as would be the case for “traditional” journal publishing. By attempting to do so, we miss the essence of the data and risk failing to communicate data-driven conclusions accurately. This limitation of current publication formats (e.g. static PDF files for articles) is an issue that relates to the reproducibility crisis of modern research.</p>
<p>Peer review is another process that is not easy to apply to data as a “first class” research object. Historically, peer reviewers have ensured that a piece of research is well-communicated and correct in the sense that it is reproducible. This level of peer review is difficult to apply in the context of research data. If data is being published as a primary output, then it may be possible to perform a kind of peer review by applying some statistical tests to a sample of the data, or by using some other appropriate technique. However, it is no longer practical in most cases to set up a parallel experiment to reproduce data, as had been the case in years past. Across all contexts there are good reasons for these challenges: the experiment may be too costly to repeat, or the conditions of the original data collection may not be replicable (for example, surveying stress levels of the populace during a specific political event). In addition, ethical considerations such as the anonymity of those being surveyed may make certain types of data difficult to review. Thus, we need to develop robust and accepted approaches to peer review, not only for data itself but also for those publications that are heavily based on data. Without peer review or some suitable proxy for peer review that makes sense for data, it is difficult to know whether a dataset can be trusted. Without trust, a dataset has no value to a researcher who seeks to build upon it.</p>
<p>Several publishing innovations have made journal articles more discoverable and accessible in recent years, such as preprint servers, the widespread use of Digital Object Identifiers (DOIs), and centralized search engines. However, while some of these infrastructures do enhance a researcher’s ability to find research data, they do not fully translate from the realm of journals to data. There are multiple reasons for this lack of translation, some of the key features include: a) weakness of a homogeneous metadata infrastructure for datasets; b) inhomogeneity in the types of data that can be shared; c) proliferation of different platforms to store data; d) lack of standardised publication practices; e) lack of adoption of standards across fields. When compared with the “shape” of an academic article for which there is a standard structure (e.g. DOI, abstract, title, authors, keywords, etc) specifically designed to facilitate human search, it is clear that datasets are contextualised by an immature information infrastructure.</p>
<p>Datasets are more complex to classify and annotate than articles, yet some progress is being made. The core fields required to create a valid DataCite record are identifier, creator, title, publisher, publication year and resource type (DataCite Metadata Working Group, 2016). All other data fields are optional (e.g. location, funder, subject, contributors) due to the fundamental uncertainty in what might constitute research data in the future. This flexibility limits how data can be discovered. It has taken some years for Web of Science, Google and others to introduce functionality to search for datasets in their discovery systems.</p>
<p>Technological infrastructure for data--or lack thereof--has huge implications for the discovery, peer review, citation practices, interpretation, and availability of data. These challenges are interconnected with challenges we face when thinking about the cultural infrastructure for data, as well.</p>
<h2 id="cultural-infrastructure">3.3 Cultural Infrastructure</h2>
<p>There are two main aspects to cultural infrastructure: incentives and capability. Both aspects affect how researchers engage with research data, and their behaviours relating to sharing it with others and making it available to external scrutiny.</p>
<p>Anecdotally, academics do not typically take up research careers for financial gain. Rather, they choose to dedicate themselves to understanding a specific problem or field partially in the hope of making a discovery. For most researchers, success is not strongly coupled to prize winning, but rather by winning the freedom to determine their own research agenda. Researchers in many fields are promoted by publishing in specific high-impact journals, leading to funding success, which in turn usually leads to greater control of your research.</p>
<p>Sharing data is often not well-aligned with the current model of incentives. Parting with the data that underpins your research gives rise to two concerns. Firstly, that someone may find an error in your work and discredit what you have done. Secondly, that someone else may not share their own data but will gladly reuse yours if you make it available. This is especially the case in fields where success is based on having more data to analyse.</p>
<p>A further level of inequity exists in which data-related jobs are valued by the Academy. If a researcher happens to be particularly talented in working with data curation, data analysis or data processing, there is no track for recognising these talents. They are unlikely to be a first author on a publication in a major journal due to their data wrangling talents, and hence they have less of a chance of career progression than researchers who take a more traditional “publish or perish” path with their work as described above.</p>
<p>This set of perverse incentives means that people with the capability to handle data are often incentivised to leave research. Hence, not only do we have a problem of incentives in sharing and communicating data, but we also have a problem in retaining people who have the capability that we need to structure data so that it can be shared and built upon.</p>
<p>Capability for sharing data is the second aspect of the cultural challenge that academia continues to wrestle with. Making data available to others is generally accepted as a key part of the research communication process. However, there are certain established norms around when the data should be shared, and to what depth it is shared (Linek et al., 2017); for example, in fields where human subjects research is prevalent, there is a much more conservative attitude towards open data than in fields like astronomy where data sharing is widely practiced, given that data can be collected by only a handful of observatories and telescopes worldwide.</p>
<p>In fields that are more applied, ensuring that data generated as a result of a commercial relationship is protected is crucial. In such fields, academics often have a better understanding of copyright, intellectual property rights and licences (Treadway et al., 2016). But outside of this context, there is a general lack of understanding of these issues and hence data are often not shared over concerns for a perceived legal barrier.</p>
<p>Other concerns are ethical—for example, should these data be shared if it might infringe the rights of the subjects of the research? Researchers are beginning to become aware that, through the use of algorithms, some data is not as well anonymised as it may first appear (Siddle, 2014). Anonymisation of data is a research field in and of itself (Li et al., 2007).</p>
<p>The degree and nature of ethical issues and industry-proximity vary greatly between different research fields and give rise to different cultures of data usage and re-usage across fields and even within fields. Some researchers are motivated to engage with the open access community and hence choose approaches to sharing data that include granting permissive licences, association of unique identifiers with data, adherence to data standards and training students to adhere to similar approaches. Other researchers are motivated to ensure that data are not shared due to the information that can be inferred by processing the data.</p>
<p>The power of the newest algorithms, or of algorithms yet to come, mixed with constantly developing ethical nuance means that it is difficult to pre-empt what may or may not be acceptable to share in the future. Hence, some may feel that it is simply better not to share, especially in the social sciences, where many of these issues are more prone.</p>
<p>Other concerns are simply practical—how does one make data available in a way that is meaningful to others? The work associated with making a dataset generically machine-readable is challenging for many researchers, who are not to be experts in data handling. The work associated with making a dataset human-understandable, reproducible and fully contextualised is often significant. Funding constraints may make it impractical to share data or to add useful, valuable or even critical annotations to a dataset. However, funders are beginning to prioritise these activities in their grant programs (Jisc, 2019; NNLM, 2019). All these factors lead to uncertainty exacerbated by different levels of confidence and understanding and consequently an uneven landscape in what is shared, how it is shared and where it is shared.</p>
<h2 id="enriching-context">3.4 Enriching context</h2>
<p>The points discussed above offer some indication of what would be needed to improve the value of research data. Firstly, to address issues of cultural infrastructure, we need to adopt an expanded version CredIT (Allen et al., 2014) that focuses on datasets. This expansion would ensure that all contributors to a dataset’s creation, development and maintenance over time are stored in a machine-readable format. Such a record is central to the facilitation of culture change across research. Only with this structure in place can the activities around datasets be readily recognised and incentives created that would support data sharing. Secondly, to address the deficits in information infrastructure, a set of tools that allow research data to be discovered and contextualised needs to be introduced. In this section, we focus on this second challenge.</p>
<p>The ability to add context any piece of research was a strong driver for the creation of Dimensions (Hook et al., 2018). The idea that all research happens in a particular place, at a particular time, carried out by a set of people, some of whom may be affiliated with a research institution, gives a set of metadata that allows us the “weak context” of a piece of research. By “weak context” we mean that the context being provided gives no deep understanding of the context of an article to a non-expert and is essentially indistinguishable from standard metadata. But with modern data mining approaches, it is possible to add a “strong context”.</p>
<p>Strong contextualisation of research should provide a user with rich information about the research including funding, other research produced as part of the larger project (e.g. related publications, clinical trials, etc), and details of the research that was built on top of it. This information should also fit into, trends and graphical representations that offer a more complete, more rapid understanding of how research fits into the larger field, related fields, or the context of the publishing journal or supporting institution. For example, for a research article, we should be able to quickly understand how many researchers are in a related field, whether the field is growing, how old the field is, how much funding has been deployed in the field, which countries have provided that funding, whether the field has begun the translation to application through patents or clinical trials, or whether it has been used as a basis for the formulation of policy.</p>
<p>Context can also be offered in the data that we provide to understand the reach and influence of research.</p>
<p>Alternative metrics (“altmetrics”) are data from the social web that run orthogonal to classic citation measures, which can be seen to add significant context to an article – extending our understanding of how different cohorts of potential users of the research are engaging with it. For example, altmetrics can be used to understand if an article is being mentioned in the news, in which geographical regions it is being noticed, whether it is being used as part of a teaching syllabus, and many other kinds of public and non-traditional scholarly engagement. These data can then be visualized in creative ways to add instant additional context to engagement with a research article (see Fig. 3.1).</p>
<p><img src="combined_images/chap03_image1.png" style="width:7.27083in;height:2.52778in" /></p>
<p><em>Figure 3.1: Different types of context tracked by Altmetric.com for any research output.<br />
(Reproduced by permission of Altmetric.com)</em></p>
<p>How datasets are used in research more broadly is another important piece of context that data search engines lack that would significantly enhance discoverability of a dataset and that would consequently increase the value of the data. This is where the Rich Context project can add significant value to a broad research community.</p>
<p>Enhanced context for research data and its impacts could be offered to users in the form of in-app badges and other “signposts” that connect data with its larger context. Such a contextualizing badge could bring together existing data, including not only the number of citations that the dataset has received, but also whether the data has been versioned (through Figshare’s repository metadata), discussed online (through Altmetric data), and what kind of tools and insights have been built on top of the data (through rich mining of full-text and citation data available in the ReadCube reference management corpus and in Dimensions).</p>
<p>Correctly developed and accepted by the community, this type of information can make a contribution to solving many of the problems highlighted in this article. If the correct contextual facets can be developed, then recognition would be easier to assign to those who have contributed to the process of creating and maintaining datasets. With greater context around them, datasets become easier to locate, understand and value. This in turn could lead to a broader evaluative environment and more engagement from academics.</p>
<p>Engagement across academia, however, is not uniform. Mechanisms need to be provided to engage data science-focused researchers from whom more details of their tools, scripts and codebooks could be drawn, adding further value to research data. At the same time, engagement tools need to allow data scientists to leverage this information so that it is valuable to them when they are the consumers of search results. These are subtly different use cases from those of standard researchers. By mining ever more open research systems wherein data is being analyzed (e.g. Gigantum, Github, etc), we can start to integrate these other crucial engagement contexts as well.</p>
<figure>
<img src="combined_images/chap03_image2.png" alt="https://lh5.googleusercontent.com/3OunhdD8OksXbG5n9dlxQZ4vHfq3ytZuYahLuNwnXG2oaJ7vSpCkCULgc_8tpapwUqzFLsALR6Xcrhl2ZgyYtvxOhg2kfoXeAJ1MoibnK0liPl2w6xhvoMye-lGtyopkM0ja1xnB" style="width:4.54331in;height:4.77604in" /><figcaption>https://lh5.googleusercontent.com/3OunhdD8OksXbG5n9dlxQZ4vHfq3ytZuYahLuNwnXG2oaJ7vSpCkCULgc_8tpapwUqzFLsALR6Xcrhl2ZgyYtvxOhg2kfoXeAJ1MoibnK0liPl2w6xhvoMye-lGtyopkM0ja1xnB</figcaption>
</figure>
<p><em>Figure 3.2: Mock-up of a research data badge helping to contextualise a set of search results.</em></p>
<figure>
<img src="combined_images/chap03_image3.png" alt="https://lh5.googleusercontent.com/51fefhu7zEe3Y61OD1vbe-Bl9EiBI8IUUj1FOnP7NLLexSqDO7cJrzBeuzmwUR7eC84AQKmcMwDcTKW3trd7-vnNiyelvHvEOdM_Da5OgEoTYh5lvrz8wfxWzTH2_5DJjfgwe9Ed" style="width:4.16316in;height:4.50521in" /><figcaption>https://lh5.googleusercontent.com/51fefhu7zEe3Y61OD1vbe-Bl9EiBI8IUUj1FOnP7NLLexSqDO7cJrzBeuzmwUR7eC84AQKmcMwDcTKW3trd7-vnNiyelvHvEOdM_Da5OgEoTYh5lvrz8wfxWzTH2_5DJjfgwe9Ed</figcaption>
</figure>
<p><em>Figure 3.3: Mock-up of a research data badge helping to contextualise a specific dataset.</em></p>
<p>In Figures 3.2 and 3.3, we have visualised some early concepts for how a contextualized research data badge could look. This visualisation is based on insights from the Rich Context project and uses data that could be mined from articles that use a specific dataset. In particular, we suggest four facets of context that both data science-focused researchers and others could find helpful when viewing a dataset:</p>
<ul>
<li><p><strong>Experts</strong> <strong>who have made use of the data</strong>, sourced from references made to the dataset in a professional context such as an industry whitepaper or policy document</p></li>
<li><p><strong>Academics</strong> that <strong>cite the data</strong>, mined from citation of the dataset or ancillary data in the peer reviewed literature</p></li>
<li><p><strong>End users of the data</strong>, sourced from code book references included in public code repositories</p></li>
<li><p><strong>Enhancements of the data</strong>, vis-à-vis annotations and comments made on the data in public forums.</p></li>
</ul>
<p>In summary, we believe that, if deployed across the many environments in which researchers discover data, the thinking behind the Rich Context project can overcome both the cultural and information-based infrastructure challenges that we highlighted. If these challenges can be overcome by the methods developed, for example in Chapter 13 of this volume, then this will significantly extend the use and discoverability of datasets. The number and variety of datasets in use in academia will certainly expand in the future, and we can only see data becoming even more central to contemporary research efforts. As such, it is critical to invest in robust infrastructures, not only to support the production and sharing of these data, but also to change the culture and evaluative environment around research data. It is only through initiatives such as these that we will be able to solve the vast and complex sociotechnical challenges that face academia today.</p>
<h2 id="references-2">References</h2>
<p>Allen, L., Scott, J., Brand, A., Hlava, M., Altman, M., 2014. Publishing: Credit where credit is due. Nature 508, 312–313. https://doi.org/10.1038/508312aCERN, 2019. CERN Open Data Portal [WWW Document]. URL http://opendata.cern.ch/search?page=1&amp;size=20&amp;subtype=Collision&amp;type=Dataset (accessed 11.30.19).CISER, 2019. CISER. URL https://ciser.cornell.edu/data/data-archive/ (accessed 11.30.19).DataCite Metadata Working Group, 2016. DataCite Metadata Schema 4.0 [WWW Document]. URL https://support.datacite.org/docs/schema-40 (accessed 11.30.19).Eijnatten, J. van, Pieters, T., Verheul, J., 2013. Big Data for Global History: The Transformative Promise of Digital Humanities. BMGN-LCHR 128, 55. https://doi.org/10.18352/bmgn-lchr.9350Giuliano, F., 2019. Humanités numériques et archives : la longue émergence d’un nouveau paradigme. Documentation et bibliothèques 65, 37. https://doi.org/10.7202/1063788arGrusin, R., 2014. The Dark Side of Digital Humanities: Dispatches from Two Recent MLA Conventions. differences 25, 79–92. https://doi.org/10.1215/10407391-2420009Hook, D.W., Porter, S.J., Herzog, C., 2018. Dimensions: Building Context for Search and Evaluation. Front. Res. Metr. Anal. 3, 23. https://doi.org/10.3389/frma.2018.00023Jisc, 2019. Research Data Management Toolkit | Jisc [WWW Document]. URL https://rdmtoolkit.jisc.ac.uk/plan-and-design/data-management-planning/ (accessed 11.30.19).Jordan, K., Weller, M., 2018. Academics and Social Networking Sites: Benefits, Problems and Tensions in Professional Engagement with Online Networking. Journal of Interactive Media in Education 2018, 1. https://doi.org/10.5334/jime.448Kamp, K., Herbell, K., Magginis, W.H., Berry, D., Given, B., 2019. Facebook Recruitment and the Protection of Human Subjects. West J Nurs Res 41, 1270–1281. https://doi.org/10.1177/0193945919828108Leurs, K., Shepherd, T., 2017. 15. Datafication &amp; Discrimination, in: The Datafied SocietyStudying Culture through Data. Amsterdam University Press, Amsterdam. https://doi.org/10.1515/9789048531011-018Li, N., Li, T., Venkatasubramanian, S., 2007. t-Closeness: Privacy Beyond k-Anonymity and l-Diversity, in: 2007 IEEE 23rd International Conference on Data Engineering. Presented at the 2007 IEEE 23rd International Conference on Data Engineering, IEEE, Istanbul, pp. 106–115. https://doi.org/10.1109/ICDE.2007.367856Linek, S.B., Fecher, B., Friesike, S., Hebing, M., 2017. Data sharing as social dilemma: Influence of the researcher’s personality. PLoS ONE 12, e0183216. https://doi.org/10.1371/journal.pone.0183216NNLM, 2019. Data Management Plan | NNLM [WWW Document]. URL https://nnlm.gov/data/data-management-plan (accessed 11.30.19).Siddle, J., 2014. I Know Where You Were Last Summer: London’s public bike data is telling everyone where you’ve been. I Know Where You Were Last Summer. URL https://vartree.blogspot.com/2014/04/i-know-where-you-were-last-summer.html (accessed 11.30.19).Stark, L., 2018. Algorithmic psychometrics and the scalable subject. Soc Stud Sci 48, 204–231. https://doi.org/10.1177/0306312718772094Treadway, J., Hahnel, M., Leonelli, S., Penny, D., Groenewegen, D., Miyairi, N., Hayashi, K., O’Donnell, D., Digital Science, Hook, D., 2016. The State of Open Data Report. Digital Science. https://doi.org/10.6084/M9.FIGSHARE.4036398.V1</p>
<h2 id="biographical-information">Biographical information</h2>
<p>Christian Herzog is CEO of Dimensions and Chief Portfolio Officer at Digital Science. A medical doctor by training, Christian also studied economics and started in 2005 Collexis, a software company focused on text-mining based software applications for the research space. In 2010, Collexis was acquired by Elsevier where Christian spent the following two years as the VP for Product Management SciVal. in 2013, Christian and his co-founders started ÜberResesarch as part of Digital Science which led to the launch of Dimensions as a large-scale research information infrastructure in 2018.</p>
<p>Daniel Hook is CEO of Digital Science. He co-founded Symplectic while studying for his PhD in theoretical physics at Imperial College London in 2003. Symplectic became one of Digital Science’s first investments in 2010. Daniel continues to be an active researcher and holds visiting academic positions at Imperial College London and at Washington University in St Louis. He has written more than 30 academic papers and has co-authored a book on Quantum Theory. Daniel is a Fellow of the Institute of Physics, a Policy Fellow at CSaP in Cambridge and serves on the ORCID board as its treasurer.</p>
<p>Mark Hahnel is the CEO and founder of Figshare, which he created whilst completing his PhD in stem cell biology at Imperial College London. Figshare provides research data infrastructure for institutions, publishers and funders globally. Mark is passionate about open science and its potential to revolutionize the research and has led the community in the development of research data infrastructure. Mark sits on the DataCite board, the DOAJ advisory board, the judging panel for the National Institutes of Health (NIH), Wellcome Trust Open Science prize and acted as an advisor for SpringerNature’s masterclasses.</p>
<p>Stacy Konkiel is the Director of Research Relations at Dimensions and Altmetric. Stacy’s research interests include incentives systems in academia and informetrics, and she has written and presented widely about altmetrics, Open Science, and research data services. Previously, Stacy worked with teams at Impactstory, Indiana University &amp; PLOS. You can learn more about Stacy at <a href="http://www.stacykonkiel.org/">stacykonkiel.org</a>.</p>
<p>Duane Williams is VP of US Government at Digital Science. Duane earned his doctorate in theoretical chemistry from the Quantum Theory Project at University of Florida. His current work focuses on improving strategic research investment decisions through new data sets, new tools and metrics to gain insight into the global research landscape. Prior to joining Digital Science, he served as Senior Scientific Analyst and Project Manager for the IP and Science division of Thomson Reuters (now Clarivate Analytics). There he designed and led custom analyses and software development to facilitate data-driven objective assessments of research programs.</p>
<hr />
<h1 id="chapter-4---metadata-for-social-science-datasets">Chapter 4 - Metadata for Social Science Datasets</h1>
<blockquote>
<p>Metadata for Administrative and Social Science Data</p>
</blockquote>
<p>Robert B Allen</p>
<p>[0000-0002-4059-2587]</p>
<p>rba@boballen.info</p>
<p>Data are valuable but finding the right data is often difficult. This chapter reviews current approaches to metadata about numeric data and considers approaches that may facilitate the identification of relevant data. In addition, the chapter reviews how metadata support repositories, portals, and services. There are many emerging metadata standards, but they are applied unevenly so that there is no comprehensive approach. There has been greater emphasis on structural issues than on semantic descriptions.</p>
<h1 id="introduction-3">INTRODUCTION</h1>
<p>Evidence-based policy needs relevant data (Commission on Evidence-Based Policy, 2018; Lane, 2016). Such data is often difficult to find and use. The FAIR Open Access guidelines suggest that, ideally, data should be Findable, Accessible, Interoperable, and Reusable (FAIR).<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> Broad and consistent metadata can support these needs. Metadata and other knowledge structures could also supplement and ultimately even replace text.</p>
<p>This chapter surveys the state of the art of metadata for numeric datasets, focusing on metadata for administrative and social science records. Administrative records describe details about the state of the world as collected by organizations or agencies. They include governmental, hospital, educational, and business records. By comparison, social science data generally is collected for the purpose of developing or applying theory.</p>
<p>We start by considering data and datasets (Section 2) and basic principles of metadata and their application to datasets (Section 3). Modern metadata is often implemented with Resource Description Framework (RDF) linked data (Section 4). Section 5 introduces ontologies and other semantic approaches. We then move to applications which use metadata. Section 6 examines repositories that hold and distribute collections of datasets. Section 7 describes services and techniques associated with repositories and Section 8 briefly describes the computing infrastructure for repositories.</p>
<h1 id="data-elements-and-datasets">DATA ELEMENTS AND DATASETS</h1>
<p>While data may be incorporated in text, image, or video, here we focus on numeric observations recorded and maintained in machine-readable form. Individual observations are rarely used in isolation. Rather, they are typically collected into datasets.</p>
<p>A dataset is defined in the W3C-DCAT (W3C - Data Catalog Vocabulary)<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> as “a collection of data, published or curated by a single agent”<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> such as a statistical agency. There are many different types of datasets; they differ in their structure, their source, and their use. A given data element may appear in many different datasets and may be numerically combined with other data to form derived data elements which then appear in still other datasets. In some cases, they are single vectors of data; in other cases, they comprise all the data associated with one study or across a group of related datasets. Reference datasets are generally collected and archived because they are of enduring value and can be used for answering many different types of questions. Other datasets, such as an individual’s medical records, are associated with a relatively narrow set of applications.</p>
<p>There is wide variability in the organization and contents of datasets, as well as in the extent to which datasets are validated and curated. Potentially with frameworks such as the SDMX (Statistical Data and Metadata eXchange) Guidelines for the Design of Data Structure Definitions,<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> concise structured descriptions can be developed for how data elements are combined to form datasets.</p>
<h1 id="metadata-schemas-and-catalogs">METADATA SCHEMAS AND CATALOGS</h1>
<p>Many datasets are available; the DataCite repository alone contains over five million datasets. Metadata can support users in finding datasets and enable users to know what is in them. Metadata are short descriptors which refer to a digital object. However, there is tremendous variability in the types of metadata and how they are applied. One categorization of metadata identifies structural (or technical), administrative, and descriptive metadata (Riley, 2017). Structural metadata includes the organization of the files. Administrative metadata describes the permissions, rights, preservation and usage relating to the data.<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> Descriptive metadata covers the contents.</p>
<p>A metadata element describes an attribute of a digital object. The simplest metadata (e.g., a Digital Object Identifier (DOI) or ORCiD<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>) identifies the digital object or its creator.<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> Metadata elements are generally part of a schema, or frame. DCAT<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> is a schema standard for datasets that is used by many repositories such as data.gov. Other structured frameworks for datasets include the DataCite<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> metadata schema and the Inter-university Consortium for Political and Social Research Data Documentation Initiative (ICPSR DDI, see Section 6.1). ISO 19115-1:2014 establishes a schema for describing geographic information and services. <a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a></p>
<p>The schema specifications provide a flexible framework. For instance, DCAT allows the inclusion of metadata elements drawn from domain schema and ontologies. Some of these domain schemas are widely used resources which DCAT refers to as assets. Figure 1 shows a fragment of properties (i.e., metadata elements) from an implementation of the Schema.org<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a> dataset schema to describe gross domestic product.</p>
<p>Figure 4.1: Fragment of GDP properties described by the Schema.org dataset schema.<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a></p>
<p>Metadata terms for an application are often assembled into namespaces from different metadata schemas. Metadata Application Profiles<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> provide constraints on the types of entities that can be included in the metadata for a given application. Moreover, application profiles can be used to validate standards. For instance, the DCAT Application Profile for data portals in Europe (DCAT-AP) supports the integration of data drawn from repositories in different jurisdictions in the EU.<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a></p>
<p>A collection of dataset schema,<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> such as all the datasets in a repository, forms a catalog. For data streams, there needs to be continuity but also the ability to update the records. In some cases, there may be relatively infrequent periodic updates. These could be given version numbers rather than an entirely new DOI.<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a> However, collections of highly dynamic data streams present challenges; most of the data stay the same but some of the data and/or metadata (e.g., number of records) change.</p>
<h1 id="linked-data">LINKED DATA</h1>
<p>RDF (Resource Description Framework) extends XML by requiring triples which assert a relationship (property) between two identifiers: “identifier – property - identifier”. RDF Schema (RDFS) extends RDF by supporting subclass relationships. A graph is formed by linking triples.</p>
<p>Hierarchical classification systems are another knowledge structure with a long history. Indeed, Schema.org is based around a hierarchical ontology. Simple classification relationships are handled by the Simple Knowledge Organization System (SKOS). SKOS represents the hierarchical structure of traditional thesauri with RDFS. Collections of data organized by SKOS are often described as “linked data”.</p>
<p>Depending on the rigor with which they are developed, these collections can support limited logical inference. Many administrative and social-science-related thesauri, such as EDGAR and those of the World Bank and the OECD, have now been implemented with SKOS. A knowledgebase is, primarily, a SKOS graph that links real-world entities. For example, Wikidata<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a> is an effort to develop a knowledgebase based on structured data from Wikimedia projects, and VIVO<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a> is a knowledge graph of scholarship.</p>
<p>But there are also many stand-alone classification schemes. The Extended Knowledge Organization System (XKOS)<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a> was developed to allow classification systems to be incorporated into a SKOS framework.</p>
<h1 id="richer-semantics">RICHER SEMANTICS</h1>
<p>Ontologies provide a coherent set of relationships between entities which cover a given domain. Well-constructed ontologies can support logical inference. Some vocabularies such as Dublin Core, which is implemented in RDF, are said to have an ontology, but they are limited because relationships among the terms are not specified. FOAF (Friend of a Friend) provides a somewhat richer ontology which includes attributes associated with people. Still more extensive ontologies often use OWL (Web Ontology Language) which can support stronger logical inference than RDFS.</p>
<p>One way to coordinate across terms is an upper ontology. Upper ontologies provide top-down structures for the types of entities allowed in domain and application ontologies. One of the best-known upper ontologies is the Basic Formal Ontology (BFO) (Arp, Smith, &amp; Spear, 2015), which is a realist, Aristotelian approach. At the top-level, BFO distinguishes between Continuants (endurants) and Occurrents (perdurants) and also between Universals and Particulars (instances). Many biomedical ontologies based on BFO are collected in the Open Biomedical Ontology (OBO) Foundry.<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a></p>
<p>There are fewer rich ontologies dealing with social science content than for natural science. Social ontology, that is, developing rigorous definitions for social terms, is often a challenge. It is difficult to define precisely what is a family, a crime, or money. In most cases, an operational or approximate definition may suffice when formal definitions are difficult. However, those operational definitions often do not interoperate well across studies.</p>
<h1 id="data-repositories-and-collections-of-datasets">DATA REPOSITORIES AND COLLECTIONS OF DATASETS</h1>
<p>A data repository holds datasets and related digital objects. Ideally, it contains a stable collection selected according to a collection policy. It is organized by metadata and knowledge structures. It provides access to the datasets and typically supports search.</p>
<h2 id="the-inter-university-consortium-for-political-and-social-research-icpsr">The Inter-University Consortium for Political and Social Research (ICPSR)</h2>
<p>ICPSR<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a> is a major repository of public-use social science and administrative datasets derived mostly from questionnaires and surveys. We go into depth about it here because the ICPSR Data Documentation Initiative (DDI)<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a> (e.g., Vardigan, Heus, &amp; Thomas, 2009) is especially well-crafted.<a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a> The DDI codebook saves the exact wording of all the questions and ICPSR provides an index of all variable names. DDI-Lifecycle is an extension that describes the broader context in which the survey was administered as well as the details about the preservation of the file (see Section 7.1). <strong>DDI uses XKOS to provide linked data.</strong> Figure 2 shows the ICPSR DDI metadata schema.</p>
<p>Figure 4.2: ICPSR DDI metadata elements.</p>
<blockquote>
<p>Version</p>
<p>Study Title</p>
<p>Alternate Title</p>
<p>PIs &amp; Affiliation</p>
<p>Funding Agencies</p>
<p>Summary</p>
<p>Subject Terms</p>
<p>Geographic Coverage Areas</p>
<p>Geographic Representation</p>
<p>Study Time Periods and Time Frames</p>
<p>Collection Notes</p>
<p>Study Purpose</p>
<p>Study Design</p>
<p>Description of Variables</p>
<p>Sampling: Sampling Procedure, Sampling Unit, Sampling Notes</p>
<p>Oversampled Group</p>
<p>Time Method</p>
<p>Data Source Type</p>
<p>Mode of Collection</p>
<p>Weight</p>
<p>Response Rates</p>
<p>Scales</p>
<p>Analysis Unit</p>
<p>Unit of Observation</p>
<p>Smallest Geographic Unit</p>
<p>Data Format</p>
<p>Restrictions</p>
<p>Version History</p>
</blockquote>
<p>The ICPSR metadata elements incorporate aspects of the implementation and design of research studies. However, many of the ICPSR metadata elements are not independent; potentially, they could be interlinked with terms such as organizations, locations, individuals, and research designs from other knowledgebases. Moreover, they could be linked with higher-level workflows and mechanisms (see Sections 7.1, 7.5).</p>
<h2 id="additional-examples-of-repositories">Additional Examples of Repositories</h2>
<p>Statistical data collection is a core function of government. Such collections often emphasize social data such as employment, criminal justice, and public health. They also include related indicators such as agricultural and industrial output and housing. Most countries have national statistical agencies such as Statistics New Zealand, and the Korean Social Science Data Archive (KOSSDA). European datasets are maintained in the Consortium of European Social Science Data Archives (CESSDA)<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a> and the European Social Survey.<a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a> Australia has a broad data management initiative, ANDS.<a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a> Many U.S. federal governmental datasets are collected at data.gov. In addition, there are many other social survey repositories<a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a> and many U.S. states and cities have online statistics sites at varying levels of sophistication.</p>
<p>There are also many non-governmental and inter-governmental agencies such as the OECD, the World Bank, and the United Nations that manage datasets. Similarly, there are very large datasets from medical research such as from clinical trials and from clinical practice including Electronic Health Records (EHRs).</p>
<p>Many datasets are produced, curated, and used in the natural sciences such as astronomy and geosciences. Some of these datasets have highly automated data collection, elaborate archives, and established curation methods. Many repositories contain multiple datasets for which access is supported with portals or data cubes (see Section 7.4). For instance, massive amounts of geophysical data and related text documents are collected in the EarthCube<a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a> portal. The science.gov portal is established by the U.S. Office of Science Technology and Policy. NASA supports approximately 25 different data portals. Each satellite in the Earth Observation System (EOS) may provide hundreds of streams of data,<a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a> with much common metadata. Likewise, there are massive genomics and proteomics datasets which are accessible via portals such as UniProt<a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a> and the Protein Data Bank<a href="#fn41" class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a> along with suites of tools for exploring them.</p>
<h2 id="repository-registries">Repository Registries</h2>
<p>There are a lot of different repositories, so it is useful to have a registry with a standard schema structure for describing them. The Registry of Research Data Repositories<a href="#fn42" class="footnote-ref" id="fnref42" role="doc-noteref"><sup>42</sup></a>, which is operated by DataCite, links to more than 2000 repositories each of which holds many datasets. Each of those repositories is described by the re3data.org schema (Rücknagel, Vierkant, et al., 2015).</p>
<h2 id="ecosystems-of-texts-and-datasets">Ecosystems of Texts and Datasets</h2>
<p>Datasets are often associated with text reports, whether they describe the development of the datasets or their use. Ultimately, we would like to be able to move seamlessly from datasets to texts and other related materials. However, as demonstrated by several of the papers in this volume, it is often difficult to extract details about datasets from legacy publications.</p>
<p>Text associated with a dataset may be used to support searching for it. Indeed, Google Dataset Search uses texts marked up with Schema.org JSON-LD (JavaScript Object Notation for Linked Data) micro-data to generate an index.</p>
<p>Going forward, great value can be achieved by persuading editors and authors to clearly cite and deposit datasets. In some cases, a separate data editor may be appointed. The Dryad Digital Repository<a href="#fn43" class="footnote-ref" id="fnref43" role="doc-noteref"><sup>43</sup></a> captures datasets from scholarly publications. It requires the deposit of data associated with scholarly papers accepted for publication. Such datasets are most often used to validate the conclusions of a research publication, but they may also be used more broadly.</p>
<p>Research datasets may be given DOIs<a href="#fn44" class="footnote-ref" id="fnref44" role="doc-noteref"><sup>44</sup></a> and cited in much the same way that research reports are cited. Formal citations can support tracing the origins of data used in analyses and help to acknowledge the work of the creators of the datasets.</p>
<h2 id="information-institutions-and-organizations">Information Institutions and Organizations</h2>
<p>The Open Archival Information System (OAIS) provides a reference model for the management of archives (Lee, 2010). A key part of the model is the inclusion of preservation planning and the requirement for stable administration over time. These attributes are part of all information institutions. Libraries, archives, and museums have formal collection management strategies, metrics, and policies.</p>
<p>In addition to traditional information institutions, there are now many other players. CrossRef<a href="#fn45" class="footnote-ref" id="fnref45" role="doc-noteref"><sup>45</sup></a> and DataCite are DOI registration agencies. CrossRef is a portal to metadata for scholarly articles, while DataCite provides metadata for digital objects associated with research. Schema.org’s primary mission is to provide a structure that improves indexing by search engine companies. Still other organizations such as HL7<a href="#fn46" class="footnote-ref" id="fnref46" role="doc-noteref"><sup>46</sup></a> and KEGG<a href="#fn47" class="footnote-ref" id="fnref47" role="doc-noteref"><sup>47</sup></a> manage controlled vocabularies and frameworks. These organizations are increasingly adopting best practices similar to those of traditional information organizations.</p>
<h1 id="repository-services">REPOSITORY SERVICES</h1>
<h2 id="administrative-metadata-and-related-services">Administrative Metadata and Related Services</h2>
<p>Administrative metadata is one of the three broad categories of metadata. Administrative metadata describes the permissions, rights, preservation, and usage of the data. While the focus of a traditional library is to support access and the focus of an archive is to ensure stability and quality, increasingly, digital repositories must address both access and preservation.</p>
<p><strong>Preservation and Trusted Datasets:</strong> Although data storage prices are declining dramatically, the cost of maintaining a trusted repository remains substantial and we cannot save everything. These challenges are familiar from traditional archives; selection policies typical in archives could help in controlling the many poorly documented datasets in some repositories. Yet, prioritization of what to select is difficult (Whyte &amp; Wilson, 2010)<a href="#fn48" class="footnote-ref" id="fnref48" role="doc-noteref"><sup>48</sup></a>.</p>
<p>Lost data is often irreplaceable. Even if the data is not entirely lost, users need confidence that the validity of stored data has not been compromised. Indeed, some data may become the target of malicious attacks. Trust is a result of both technology and organizational procedures. Technology may include hash-based encoding of data. CLOCKSS<a href="#fn49" class="footnote-ref" id="fnref49" role="doc-noteref"><sup>49</sup></a> is a distributed hash system for web-based scholarly literature. Blockchains provide hashed records of transactions and can be applied to data records.</p>
<p>The OAIS framework has been incorporated into the ICPSR DDI-Lifecycle model. The integrated Rule-Oriented Data System (iRODS)<a href="#fn50" class="footnote-ref" id="fnref50" role="doc-noteref"><sup>50</sup></a> is a policy-based archival management system<a href="#fn51" class="footnote-ref" id="fnref51" role="doc-noteref"><sup>51</sup></a> developed for large data stores. It implements a service-oriented architecture (SOA) to support best practices established by archivists. Further, audits, such as by the Digital Repository Audit Method Based on Risk Assessment (DRAMBORA),<a href="#fn52" class="footnote-ref" id="fnref52" role="doc-noteref"><sup>52</sup></a> may be conducted to assess how well repositories implement trustworthy procedures.</p>
<p>Preservation and provenance metadata schemes such as PREMIS<a href="#fn53" class="footnote-ref" id="fnref53" role="doc-noteref"><sup>53</sup></a> and PROV-O<a href="#fn54" class="footnote-ref" id="fnref54" role="doc-noteref"><sup>54</sup></a> are state-based ontologies that include entities such as actors, events, and digital objects. They record the history of transitions (e.g., changes in format) for digital objects.</p>
<p><strong>Rights Metadata:</strong> For some data, there are many advantages to open publication. The rights for that data can be specified with a Creative Commons License. For other data, there can be strong justifications for limited access, such as privacy and economic factors.</p>
<p>For example, although survey results are generally aggregated across individuals, individual-level data is sometimes very useful. Some repositories of survey data include micro-data, that is data for the responses that individuals gave to survey questions.<a href="#fn55" class="footnote-ref" id="fnref55" role="doc-noteref"><sup>55</sup></a> However, analysis of such micro-data raises privacy concerns and needs to be carefully managed; access should be limited to qualified researchers. Repositories of individual health records raise similar privacy concerns.</p>
<p><strong>Usage Statistics:</strong> The number of visits and downloads for a dataset can give an indication to later users about the likely value of a given dataset. Such usage data are helpful for the managers and funders of repositories to evaluate their service. Citations are indicators for how a dataset is being used and its relationship to other work.</p>
<h2 id="analysis-platforms-and-decision-support-systems">Analysis Platforms and Decision Support Systems</h2>
<p>There is an increasingly rich set of analytic tools. Some of the earliest tools were statistical packages such as SPSS, R, SAS, and STATA. These were gradually enhanced with data visualization and other analytic software. The current generation of tools such as Jupyter<a href="#fn56" class="footnote-ref" id="fnref56" role="doc-noteref"><sup>56</sup></a>, RSpace, and eLab notebooks (ELN) integrate annotations, workflows, raw data, data analysis, and annotations into one environment.</p>
<p>Virtual research environments (VREs) are typically organized by research communities to coordinate datasets with search and analytic tools. For instance, the Virtual Astronomy Observatory (VAO) uses Jupyter to provide users with a robust research environment. WissKI<a href="#fn57" class="footnote-ref" id="fnref57" role="doc-noteref"><sup>57</sup></a> is a platform for coordinating digital humanities datasets which are based on Drupal. Decision Support Systems (DSS) are generally focused on finding optimal solutions in a parameter space. They often draw on data warehouses though recently they have begun to incorporate feeds from unstructured data (e.g., web searches).</p>
<p>Most repositories support search on metadata terms. In addition, some repositories have developed their own powerful data exploration tools such as ICPSR Colectica<a href="#fn58" class="footnote-ref" id="fnref58" role="doc-noteref"><sup>58</sup></a> for DDI and the GSS Data Explorer<a href="#fn59" class="footnote-ref" id="fnref59" role="doc-noteref"><sup>59</sup></a>. The Amundsen data discovery and metadata engine<a href="#fn60" class="footnote-ref" id="fnref60" role="doc-noteref"><sup>60</sup></a> uses metadata elements to provide a table explorer. Potentially, interactive visualization tools such as TableLens (Rao &amp; Card, 1994) could also be employed.</p>
<h2 id="metadata-development-standardization-and-management">Metadata Development, Standardization, and Management</h2>
<p>Metadata, whether for texts or datasets, needs to be complete, consistent, standardized, machine processable, and timely (Park, 2009). Metadata registries provide clear definitions and promote standardization (ISO/IEC 11179). For instance, the Marine Metadata Interoperability Ontology Registry and Repository<a href="#fn61" class="footnote-ref" id="fnref61" role="doc-noteref"><sup>61</sup></a> records usage of different metadata terms. A registry may interoperate with editing tools for developers (Goncalves, O’Connor, et al., 2019). These tools may suggest candidate metadata terms. One of the keys to the development of good metadata is the involvement of a community that cares about the results.</p>
<h2 id="data-cubes-data-warehouses-and-data-exchanges">Data Cubes, Data Warehouses, and Data Exchanges</h2>
<p>An organization such as a large business often has many different databases. The data in the databases will likely have different formats and definitions and can be organized in a multidimensional cube. Some of cube’s cells may be well-populated with data that appears across many of the databases, but there will also be sparsely populated regions and cells. Online Analytical Processing (OLAP) users can generate different views of the data by drilling-down, rolling-up, and slicing-and-dicing across cells. To facilitate retrieval, there can be a rich pre-coordinated index for common queries. Other queries can be implemented with slower methods such as hashing or B-trees.</p>
<p>While many organizations now have integrated enterprise data management systems, data cubes are still useful for warehousing data and for exchanging it across organizations. For instance, the W3C Data Cube<a href="#fn62" class="footnote-ref" id="fnref62" role="doc-noteref"><sup>62</sup></a> standard is applied in inter-organizational projects such as EarthCube<a href="#fn63" class="footnote-ref" id="fnref63" role="doc-noteref"><sup>63</sup></a>. <em>SDMX</em><a href="#fn64" class="footnote-ref" id="fnref64" role="doc-noteref"><sup>64</sup></a> <em>enables data exchange among statistical agencies in the EU.</em></p>
<h2 id="production-workflows-research-workflows-and-research-objects">Production Workflows, Research Workflows, and Research Objects</h2>
<p>Entities change over time, yet many knowledge representation frameworks do not model change. To represent change, models need to represent transitions, processes, and other sequential activities. Such modeling is closer to state machines, Petri Nets, process ontologies, the Unified Modeling Language (UML) or even programming languages than to traditional knowledge representation.</p>
<p>One way to document a research project is by saving files developed during the study (Borycz &amp; Carroll, 2018). Data files (e.g., Excel files) are just one type of artifact from a research program; other research objects include workflows. Workflows are a natural fit for describing research methods and analyses (Austin, et al., 2017). The Taverna<a href="#fn65" class="footnote-ref" id="fnref65" role="doc-noteref"><sup>65</sup></a> workflow tool has been used for the MyExperiment<a href="#fn66" class="footnote-ref" id="fnref66" role="doc-noteref"><sup>66</sup></a> project. It provides a framework for capturing and posting Taverna and other types of research workflows and incorporates simple ontologies such as FOAF. Workflows can also be used to specify and document statistical analyses; several of the analysis platforms described in Section 7.2 support them. Sequential activities in the management of repositories are often tracked with workflows. For instance, the Generic Statistical Information Model (GSIM)<a href="#fn67" class="footnote-ref" id="fnref67" role="doc-noteref"><sup>67</sup></a> specifies workflows for the production of datasets by statistical agencies.</p>
<h2 id="semantic-modeling-and-direct-representation">Semantic Modeling and Direct Representation</h2>
<p>Semantic models attempt to represent entities. They could support unified descriptions of functionality, transitions of complex continuants, and sequential activities (Allen, 2018). Changes in semantic models are a form of qualitative simulation. While traditional knowledge representation is usually implemented with ontologies, models which allow transitions are more like programming languages. Although semantic modeling might be implemented by process ontologies, we have focused on the use of an object-oriented programming language which supports threads to allow parallel concurrent event streams and potentially to develop a “unified temporal map”. Such semantic simulations may be useful for modeling historical events. For instance, a community described in a newspaper may be cast into a “community model”. These go beyond social ontology to model social mechanisms (Ylikoski, in press).</p>
<p>In addition, Allen (2015, 2018) has proposed rich semantic modeling of entire research reports and datasets. Structured evidence and argumentation about claims might then be applied for the evaluation of the models. Ultimately, such “direct representation” may replace text as the primary representation for research and scholarship.</p>
<h1 id="infrastructure">INFRASTRUCTURE</h1>
<h2 id="repository-servers">Repository Servers</h2>
<p>Semantic representations may be implemented with triplestores. Triplestores facilitate logical inference, but retrieval may be more efficient with relational databases. Many metadata catalogs are implemented with relational databases. Thus, they use SQL and are often characterized by UML Class Diagrams. Information models (e.g., National Information Exchange Model (NIEM)<a href="#fn68" class="footnote-ref" id="fnref68" role="doc-noteref"><sup>68</sup></a>) which could be used for metadata registries may be implemented as data dictionaries.</p>
<p>Some repositories are federated with the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH)<a href="#fn69" class="footnote-ref" id="fnref69" role="doc-noteref"><sup>69</sup></a>. This allows the “harvesting” of metadata from separate repositories. OAI-PMH is increasingly used as an API to allow external users to query and interact with the federated set of metadata.</p>
<h2 id="cloud-computing">Cloud Computing</h2>
<p>We are well into the era of cloud computing (Foster &amp; Gannon, 2017), allowing flexible allocation of computing, networking and storage resources, which facilitates Software as a Service (SaaS). The compatibility of the versions of software packages needed for data management is often a challenge. Containers, such as those from Docker, allow compatible versions of software to be assembled and run on a virtual computer. A cloud-based virtual machine can hold datasets, workflows, and the programs used to analyze the data, which can be a complete digital preservation package.</p>
<p>Highly networked data centers facilitate the Internet of Things (IoT) which generates massive and dynamic data. Increasingly, cloud computing is supporting edge computing and append-only stores which can capture streaming data. These technologies will provide the foundation of smart cities and have implications for the kinds of questions we may ask about social behavior.</p>
<h1 id="conclusion">CONCLUSION</h1>
<p>Many datasets, especially legacy datasets, are difficult to find and access. Some of the biggest issues for the retrieval of datasets concern information organization, which helps to provide context. Metadata supports the discovery and access to datasets.</p>
<p>More attention to metadata would also further support evidence-based policy. We need richer, more systematic, and more interoperable metadata standards. We need to improve the metadata associated with existing datasets. And we need to aggressively upgrade the application of high-quality metadata and knowledge organization systems to datasets as they are created.</p>
<h1 id="acknowledgments" class="ListParagraph">ACKNOWLEDGMENTS</h1>
<p>Julia Lane and members of NYU’s Center for Urban Science and Progress provided useful advice and comments.</p>
<h1 id="references" class="ListParagraph">REFERENCES</h1>
<p>Allen, R.B. (2015) Repositories with direct representation, <em>Networked knowledge representation systems,</em> arXiv: 1512.09070</p>
<p>Allen, R.B. (2018) <em>Issues for using semantic modeling to represent mechanisms</em>, arXiv:1812.11431</p>
<p>Allen, R.B., &amp; Kim, YH. (2017/2018) Semantic modeling with foundries, arXiv:1801.00725</p>
<p>Arp, R., Smith, B., &amp; Spear, A.D. (2015) <em>Building ontologies with basic formal ontology</em>, MIT Press, Cambridge. MA.</p>
<p>Austin, C.C., Bloom, T., Dallmeier-Tiessen, S., Khodiyar, V.K., Murphy, F., Nurnberger, A., et al. (2017). Key components of data publishing: Using current best practices to develop a reference model for data publishing. <em>In<a href="https://link.springer.com/journal/799">ternational Journal on Digital Libraries</a>, 18</em>(2) 77–92, doi:10.1007/s00799-016-0178-2</p>
<p>Borycz, J., &amp; Carroll, B. (2018) Managing digital research objects in an expanding science ecosystem: 2017 conference summary. <em>Data Science Journal</em>, 17, doi: http://doi.org/10.5334/dsj-2018-016</p>
<p>Commission on Evidence-Based Policymaking (2018) <em>The Promise of Evidence-Based Policymaking</em>, https://www.cep.gov/cep-final-report.html</p>
<p>Foster, I., &amp; Gannon, D.B. (2017) <em>Cloud computing for science and engineering</em>, MIT Press, Cambridge, MA.</p>
<p>Gonçalves, R.S., O'Connor, M.J., Martínez-Romero, M., Egyedi, A.L., Willrett, D., Graybeal, J., &amp; Musen, M.A. (2019) <em>The CEDAR workbench: an ontology-assisted environment for authoring metadata that describe scientific experiments</em>. arXiv: 1905.06480</p>
<p>InterPARES2 Project (2008) A framework of principles for the development of policies, strategies and standards for the long-term preservation of digital records, http://www.interpares.org/public_documents/ip2(pub)policy_framework_document.pdf</p>
<p>Lane, J. (2016) Big data for public policy: The quadruple helix, <em>Journal of Policy Analysis and Management, 35</em>(3), doi: <a href="https://doi.org/10.1002/pam.21921"><strong>10.1002/pam.21921</strong></a></p>
<p>Lee, C.A. (2010) Open Archival Information System (OAIS) reference model. <em>Encyclopedia of library and information sciences</em> (3<sup>rd</sup> Edition). CRC Press, doi: 10.1081/E-ELIS3-120044377</p>
<p>Park, J-R., (2009) Metadata quality in digital repositories: A survey of the current state of the art. <em>Cataloging &amp; Classification Quarterly, 47</em>, 213–228, 2009, doi: 10.1080/01639370902737240</p>
<p>Rao, R., &amp; Card, S.K. (1994) The Table Lens: Merging graphical and symbolic representations in an interactive focus+context visualization for tabular information, <em>ACM SIGCHI</em>, 318-322, doi: 10.1145/191666.191776</p>
<p>Riley, J. (2004) <em>Understanding metadata: What is metadata, and What is it for?: A primer</em>, NISO Press, Bethesda, MD. ISBN: 978-1-937522-72-8</p>
<p>Rücknagel, J., Vierkant, P., Ulrich, R., Kloska, G., Schnepf, E., Fichtmüller, D. et al. (2015) <em>Metadata schema for the description of research data repositories: version 3.0</em> (29), doi: 10.2312/re3.008</p>
<p>Vardigan, M., Heus,P., &amp; Thomas, W. (2008) Data documentation initiative: Toward a standard for the social sciences. <em>International Journal of Digital Curation. <strong>3</strong></em><strong>(1). doi:</strong> <a href="https://doi.org/10.2218/ijdc.v3i1.45">10.2218/ijdc.v3i1.45</a></p>
<p>Whyte, A., &amp; Wilson, A. (2010) How to appraise and select research data for curation. <em>DCC How-to Guides. Edinburgh</em>: Digital Curation Centre. http://www.dcc.ac.uk/resources/how-guides</p>
<p>Wilkinson, M.D., <a href="https://www.nature.com/articles/sdata201618#auth-2">Dumontier</a>, M., Aalbersberg, I.J.J., Appleton, G., Axton, M., Baak. A., et al. (2016) The FAIR guiding principles for scientific data management and stewardship, <em>Scientific Data, 3</em>, 160018. doi: 10.1038/sdata.2016.18</p>
<p>Ylikoski, P. (to appear) Social mechanisms. <em>The Routledge handbook of mechanisms and mechanical philosophy</em>, Routledge, edited by S. Glennan and P. Illari</p>
<hr />
<h1 id="chapter-5---compettion-design">Chapter 5 - Compettion Design</h1>
<p>By Andrew Gordon, Ekaterina Levitskaya, and Jonathan Morgan - New York University</p>
<p>Table of Contents</p>
<p><a href="#introduction">Introduction 3</a></p>
<p><a href="#context">Context 3</a></p>
<p><a href="#specific-challenges">Specific Challenges 4</a></p>
<p><a href="#competition-design">Competition Design 5</a></p>
<p><a href="#data">Data 6</a></p>
<p><a href="#publications">Publications 7</a></p>
<p><a href="#publication-dataset---phase-1">Publication Dataset - Phase 1 7</a></p>
<p><a href="#publication-dataset---phase-2">Publication Dataset - Phase 2 8</a></p>
<p><a href="#converting-pdf-files-to-plain-text">Converting PDF files to plain text 8</a></p>
<p><a href="#finding-data-sets">Finding Data Sets 9</a></p>
<p><a href="#data-set-mention-annotation-process">Data Set Mention Annotation Process 11</a></p>
<p><a href="#methods-and-fields">Methods and Fields 13</a></p>
<p><a href="#submission-process">Submission Process 14</a></p>
<p><a href="#building-and-submitting-a-model">Building and Submitting a Model 15</a></p>
<p><a href="#model-api">Model API 16</a></p>
<p><a href="#running-a-submitted-model">Running a Submitted Model 17</a></p>
<p><a href="#notes-on-the-submission-process">Notes on the Submission Process 17</a></p>
<p><a href="#evaluation">Evaluation 18</a></p>
<p><a href="#phase-1-evaluation">Phase 1 Evaluation 18</a></p>
<p><a href="#mentions-methods-and-fields">Mentions, Methods and Fields 18</a></p>
<p><a href="#data-set-citations">Data Set Citations 19</a></p>
<p><a href="#phase-2-evaluation">Phase 2 Evaluation 19</a></p>
<p><a href="#data-set-citations-1">Data Set Citations 22</a></p>
<p><a href="#capturing-data-references">Capturing Data References 22</a></p>
<p><a href="#finding-related-mentions-and-citations">Finding Related Mentions and Citations 23</a></p>
<p><a href="#scoring-the-results">Scoring the Results 25</a></p>
<p><a href="#discussion">Discussion 25</a></p>
<p><a href="#conclusion">Conclusion 27</a></p>
<p><a href="#references">References 27</a></p>
<h1 id="introduction-4">Introduction</h1>
<p>The rich context competition was designed to inspire computer scientists to automate the discovery of research datasets and the associated research methods and fields in social science research publications. Participants were asked to use any combination of machine learning and data analysis methods to identify the datasets used in a corpus of social science publications and infer both the scientific methods and fields used in the analysis and the research fields.</p>
<p>The competition had the potential to draw on existing work. The IARPA FUSE program had funded research teams to develop automated methods that would identify technical emergence using information found in published scientific, technical, and patent literature(<em>1</em>, <em>2</em>), and resulted in recommendation systems like meta.com(<em>3</em>). Google Dataset Search had developed search technologies that would find datasets in data repositories across the Web(<em>4</em>) Academic social network sites like ResearchGate and Academia.edu had developed platforms whereby researchers provided feedback about their scientific activities(<em>5</em>) And there is a well established tradition of competitions in computer science, particularly natural language processing(<em>6</em>).</p>
<h3 id="context">Context</h3>
<p>Social scientists might define rich context as a dataset search and discovery process: what does the data <strong>measure</strong>, what <strong>research</strong> has been done by which <strong>researchers</strong>, with what <strong>code</strong>, and with what <strong>results</strong>. Computer scientists might define rich context as knowledge graph representation and recommender systems. Others might define rich context as promoting datasets to be a first class entity. But the core idea of the competition was to incentivize computer scientists to build automated tools that find datasets mentioned in scientific publications and build an associated community of interest. The results could then be used to recommend datasets to empirical researchers and encourage researchers to provide feedback about the value of the recommendations.</p>
<p>The innovation literature provided some guidance. At a high level, most systematic incentives for innovation can be classified as one of two types: up-front support for research (“push programs”) or commitments to reward successful results (“pull incentives”)(<em>7</em>), with a given incentive evaluated on its balance between positive and negative outcomes. The patent system of protecting intellectual property for a period of time, for example, is an incentive that balances the benefit to the creator of time-limited exclusive use of a patented innovation with the cost of restriction on broader use(<em>8</em>).</p>
<p>Prizes are another common pull incentive, offering direct reward for an innovation that arises from competition among innovators. Innovation prizes offer an immediate benefit that can be a powerful incentive for development and diffusion of innovations, but the design of the contest that awards them is important to maximizing innovation benefits, and effective evaluation is difficult. For prizes to encourage innovations that are of high quality, desirable, and more production-ready, the contests that offer them need to be designed carefully to include additional evaluation requirements or incentives, with the benefits to participants carefully balanced so that the rewards make the additional requirements worth their cost(<em>9</em>, <em>10</em>) .</p>
<p>The literature informing the development of a community of practice in a domain of work where knowledge is cumulative emphasizes the advantages. Successful communities can develop knowledge-sharing and dissemination mechanisms, common norms of sharing and cooperation, and broad agreement on technical paradigms and jargon(<em>11</em>). As open source software communities show, however, they must be carefully incentivized and nurtured to grow participation(<em>12</em>) and managed well to maintain resources and quality of output over time(<em>13</em>).</p>
<h3 id="specific-challenges">Specific Challenges</h3>
<p>There were a number of challenges associated with developing a natural language processing (NLP) competition applied academic publications. Access to scientific publications is typically limited. In addition, there are no existing annotated data sets or standards for annotations and existing solutions are not easily reusable. A similar project focused on text analysis for clinical studies reported that NLP research teams do not traditionally collaborate closely, and models and systems that result tend to not be designed or implemented to be easy to use or to scale up for production use(<em>14</em>). However, in the NLP domain, Ian Soboroff at the National Institutes of Standards and Technology (NIST) has developed a series of competition patterns designed to inspire disparate groups of researchers to help to carry out information tasks against text data. These include basic competitions where data is provided to groups and they are allowed to train and then submit a number of runs of their models against a subset of evaluation data (<em>6</em>). More elaborate competitions include ones organized around an “incident”, where groups are given training data and model specifications and allowed to train a model, game out an incident where an event occurs in a previously unseen language and then they have to quickly adapt their model to the new language and submit results(<em>15</em>).</p>
<p>We also wanted to encourage researchers to develop a generalized model to identify datasets that was not overly dependent on the use of formal titles of data sets, because many research datasets do not have such formal titles. Thus the problem was much more complicated than a named entity recognition problem, because competitors needed to be able to characterize the language of discussing and using data to recognize where data is discussed in a particular article and then identify which data sets.</p>
<p>This chapter describes the implementation of the competition, particularly focusing on the lessons learned.</p>
<h1 id="competition-design">Competition Design</h1>
<p>The goal of the competition was to use any combination of machine learning and data analysis methods to identify the datasets mentioned in a corpus of social science publications and infer both scientific methods used in the analysis and the publication’s research fields<a href="#fn70" class="footnote-ref" id="fnref70" role="doc-noteref"><sup>70</sup></a>.</p>
<p>The competition had two phases.</p>
<p>In the first phase, participating teams were provided with a listing of datasets and a labeled corpus of 5,000 publications with an additional dev fold of 100 publications. Each publication was labeled to indicate which of the datasets from the master list were referenced within and what specific text was used to refer to each dataset. The teams used this data to train and tune algorithms to detect mentions of data in publication text and, when a data set in our list is mentioned, tie each mention to the appropriate data set. A separate corpus of 5,000 labeled publications was held back to serve as an evaluation corpus. Each team was allowed up to 2 test runs against this evaluation corpus before final submission. The final models of each group were run against this holdout corpus and the results were used to evaluate submissions, along with a random qualitative review of the mentions, methods, and fields detected by the team’s model. Submissions were primarily scored on the accuracy of techniques, the quality of documentation and code, the efficiency of the algorithm, and the quality and novelty of the methods and research fields inferred for each of the publications.</p>
<p>Four finalist teams were selected to participate in the second phase, the teams from: Allen Institute for Artificial Intelligence, United States; GESIS at the University of Mannheim, Germany; Paderborn University, Germany; and KAIST in South Korea.</p>
<p>In the second phase, finalists were provided with a new training corpus of 5000 unlabeled publications and asked to discover which of the datasets from the first phase’s data catalog were used in each publication, as well as infer associated research methods and fields. As in the first phase, teams were scored on the accuracy of their techniques, the quality of their documentation and code, the efficiency of their algorithm, and the quality and novelty of the methods and research fields inferred for each of the publications.</p>
<p>At the end of each phase, competing teams packaged their models into a docker container using a model packaging framework designed and built for the competition by NYU, and the containers were installed on AWS servers and run by the competition organizers against the holdout to generate predictions that were used to evaluate the teams.</p>
<h1 id="data">Data</h1>
<p>For training and evaluation data, our goal was to lay the foundations for developing a “gold standard corpus” (GSC) of academic populations tagged with the semantic context within which datasets are mentioned used in analysis. A GSC corpus is one that is manually tagged and reviewed for quality, usually for a particular domain and task. Creating one is time-consuming and expensive(<em>16</em>) because it involves selecting a corpus to annotate, then implementing a manual annotation and review scheme<a href="#fn71" class="footnote-ref" id="fnref71" role="doc-noteref"><sup>71</sup></a>.</p>
<p>While our goal was not to make a GSC, we used our data creation to begin to assess data needed for high quality data detection models and to test potential methods for creating a GSC. To create our competition training and evaluation data, we started with data set citation data from the ICPSR data catalog (<a href="https://www.icpsr.umich.edu/icpsrweb/"><span class="underline">https://www.icpsr.umich.edu/icpsrweb/</span></a>), then used methods that originated in quantitative content analysis of communication artifacts(<em>17</em>) combined with software designed to reduce and simplify the work of human coders to increase reliability (<em>18</em>).</p>
<p>In each of the two phases, competing teams were given text and metadata for 5,000 publications and single set of metadata on 10,348 data sets of interest, shared between the two phases, for use in training and testing their models. Separate 5,000-publication samples were provided for each phase. The corpus of 10,348 data sets included data maintained by Deutsche Bundesbank and the set of public data sets hosted by the Inter-university Consortium for Political and Social Research (ICPSR). In addition, a single 100-publication development fold was provided separate from the training and testing data to serve as a test for packaging of each team’s model, and as a quick test of their model and the quality of its output<a href="#fn72" class="footnote-ref" id="fnref72" role="doc-noteref"><sup>72</sup></a>.</p>
<p>In each phase, an additional separate set of 5,000 publications were held back and used to evaluate the models. After the 1st phase, the phase 1 holdout was also provided to phase 2 competitors to serve as additional training and testing data.</p>
<p>In phase 1, both the train-test publications and the holdout publications were broken into 2,500 publications each that used one or more of the data sets of interest for analysis, as compiled by ICPSR and Bundesbank staff, and 2,500 publications that had not been annotated and had been filtered to not contain data. The data set citations were captured in a separate data set citations JSON file. The citations for the phase 1 train-test publications were provided to competition teams to use as training data, while the citations in the phase 1 holdout were used to test the quality of each team’s model in phase 1, and given to teams as additional training data in phase 2.</p>
<p>In phase 2, teams were provided with the phase 1 holdout for additional annotated training data, and then provided with an additional un-annotated set of 5,000 publications to assess their model’s behavior on un-curated data. The phase 2 holdout of 5,000 publications was also unannotated, and evaluation of data set detection was based on hand-coded data set reference data revised to make the data more representative of what the models were asked to detect.</p>
<h2 id="publications">Publications</h2>
<p>All publication text provided to teams was either open access, and so freely available, or licensed from the publisher for use in the contest by contest participants. In each phase of the competition, a set of publications was provided to the participants and a separate set of publications was held out and kept in reserve so it could be used to evaluate the teams’ models. For each publication, participants were provided with PDF and plain text versions of each publication along with basic metadata (pub_date; unique_identifier - DOI or equivalent; text_file_name; pdf_file_name; and publication_id - the unique identifier from our internal system used to manage the data, metadata, and underlying relationships between publications and data sets for the competition).</p>
<p>One particular challenge was that copyright and licensing around research publications limited what publications could be accessed, licensed, and distributed for the competition, and so our universe of publications was limited to publications that were either open access, or published by Sage Publications.</p>
<h3 id="publication-dataset---phase-1">Publication Dataset - Phase 1</h3>
<ul>
<li><p>2500 labeled training publications</p></li>
<li><p>2500 unlabeled/no-dataset training publications</p></li>
<li><p>100 publication development fold</p></li>
<li><p>2500 labeled holdout publications</p></li>
<li><p>2500 unlabeled/no-dataset holdout publications</p></li>
</ul>
<p>In phase 1, 5,000 publications were provided to participants as a train-test data set, 5,000 publications were held back for evaluation, and 100 publications were provided as a separate development fold, for basic model testing and evaluation. The train-test and evaluation holdout each contained 2,500 publications that cited at least one data set, and 2,500 publications that had not been cited by ICPSR as using their data, and had been filtered to not have obvious markers of using data.</p>
<p>The annotated portion of these two sets of publications were drawn from a set of publications provided by Bundesbank that referenced their data and the publications captured in the ICPSR catalog annotated as having used a particular data set for analysis. These publications were collected in a database application designed to facilitate a mix of human and automated content analysis of publications. They were then filtered into two sets: those that were open access, and so could be shared publicly, and those that were not open access, but that were available from our publisher partner (Sage Publications, or “Sage”). Of the 5,100 total publications with annotated data citations provided to phase 1 participants, the 2,550 publications in the train-test corpus (2,500) and development fold (50) were randomly selected from the open access set, so they could be distributed freely to all participants. The 2,500 in the holdout were randomly selected from the remainder of the open access set plus those available from Sage. The un-annotated publications used in phase 1 were all published by Sage - the 2,550 non-annotated publications in the train-test corpus (2,500) and development fold (50) were open access publications from Sage journals. The 2,500 un-annotated publications used in the holdout evaluation corpus were sampled from across Sage Publications’ journal holdings including non-open access journals.</p>
<h3 id="publication-dataset---phase-2">Publication Dataset - Phase 2</h3>
<ul>
<li><p>The main publication corpus for phase 2 of the competition was 10,000 unlabeled publications evenly distributed between 6 key topic areas (Education, Health care, Agriculture, Finance, Criminal justice, and Welfare), nicknamed the “wild corpus”.</p></li>
<li><p>5,000 of these 10,000 were given to teams to work with in phase 2 (randomly selected from within each of the 6 key topic areas to maintain even distribution across topic areas).</p></li>
<li><p>The other 5,000 publications were held out to serve as an evaluation corpus.</p></li>
<li><p>In addition, teams were given the same 100 publication development fold as in phase 1.</p></li>
<li><p>Teams were given the 5,000 publication evaluation corpus from phase 1 to serve as further train-test data.</p></li>
</ul>
<p>In phase 2, we worked with Sage to find publications in six key topic areas of interest for partners and future projects (Education, Health care, Agriculture, Finance, Criminal justice, and Welfare). For 28,769 matches, Sage provided PDFs for each and we parsed the text (see details below), removing any that did not parse, or that resulted in file sizes smaller than 20KB, reducing the size of the sample to 25,888. We looked at publication year and type to see if we needed to filter out older publications or non-academic publications, but there were few enough of each class (644 pre-2000 publications and 3,115 non-research articles) that we decided we’d keep all in to preserve as much potential for heterogeneity as possible. From these 25,888 publications, we then randomly selected a total of 10,000 with the goal to keep the distribution across the 6 topic areas equal (so 1666 randomly selected in 2 topic areas, 1667 randomly selected in the other 4). Then, we split the phase 2 corpus to give half to participants and keep half back for evaluation, maintaining equal distribution between the topic areas within each set of 5,000 publications.</p>
<h3 id="converting-pdf-files-to-plain-text">Converting PDF files to plain text</h3>
<p>The plain text provided for each publication was derived from that publication’s PDF file by the competition organizers. It was not intended to be a gold standard, but to serve as an option in case a team preferred not to allocate resources to PDF parsing.</p>
<p>The articles were converted from PDF to text using the open source “pdftotext” application, an Xpdf text extraction system. The basic conversion used the “raw” mode of “pdftotext”:</p>
<p>pdftotext -raw &lt;path_to_pdf.pdf&gt; &lt;path_to_txt.txt&gt;</p>
<p>There are many approaches and tools available for this task. The rationale behind this simplified process for converting pdfs to texts:</p>
<ol type="1">
<li><p>To render the most usable txt files from available pdfs without over engineering for any specific types of pdf files (e.g., single column vs. multi-column).</p></li>
<li><p>To have a process that is easily reproducible across different machines for free. That is, not all PDFs convert the same way. Some are more error prone than others. More advanced OCR techniques might have been able to compensate where Xpdf might have fallen short, but relying on more sophisticated and perhaps costly text conversion processes would have made the conversion pipeline more expensive to reproduce and less portable across different applications.</p></li>
</ol>
<p>Because of the basic approach, there were some limitations to note:</p>
<ul>
<li><p>Many artifacts from PDF formatting were left behind in the text.</p></li>
<li><p>We had to tweak our processing to get multi-column layouts to output text in order in a linear, single-column text output, and the method we ended up using to achieve this precluded more nuanced processing of other elements of the PDFs.</p></li>
<li><p>Example: tables and charts were not converted in any way to text.</p></li>
</ul>
<p>Competition participants were encouraged to try their own conversion process if this text did not meet their needs. If participant teams chose to use another means for converting PDF files to plain text, we asked that they supply us with documentation for installing and running their conversion process so we could start to build up a set of PDF processing strategies that could be reused in the future.</p>
<h2 id="finding-data-sets">Finding Data Sets</h2>
<p>Competitors were provided with two sets of data related to detecting data sets: 1) a catalog of all of the data sets of interest that models were tasked with finding in publications, including basic metadata for all and a list of verbatim mention text snippets for those that were cited in the train-test data; and 2) a subset of these data sets that were actually specifically annotated as having been used for analysis in a given publication.</p>
<p>The data set catalog, provided to participants in the JSON file data_sets.json, contained metadata for all public datasets in the ICPSR data repository and a subset of public data sets available from Deutsche Bundesbank. It includes all data sets sited in the train-test and evaluation corpora, plus many others not cited in either. The data was provided in JSON format for ease of use, a JSON list of JSON objects, each of which contains:</p>
<ul>
<li><p>subjects - list of terms associated with the dataset, based on the <a href="https://www.icpsr.umich.edu/icpsrweb/ICPSR/thesaurus/subject"><span class="underline">ICPSR subject thesaurus.</span></a></p></li>
<li><p>additional_keywords - System keyword for where dataset originated.</p></li>
<li><p>citation - Preferred dataset citation.</p></li>
<li><p>data_set_id - Integer ID for dataset from our internal data store of publications, data sets, and relations. This is the identifier used in the data_set_citations.json file to identify relationships between datasets and publications.</p></li>
<li><p>title - Canonical title for dataset.</p></li>
<li><p>name - Canonical title for dataset.</p></li>
<li><p>description - Dataset description, if available.</p></li>
<li><p>unique_identifier - Original unique identifier for dataset, normally a DOI if available.</p></li>
<li><p>methodology - Methodology for dataset, if available.</p></li>
<li><p>date - Date when dataset was published, if available.</p></li>
<li><p>coverages - Geographic coverages, if available.</p></li>
<li><p>family_identifier - Internal system ID, roughly captures datasets that have multiple years but are the same dataset. Inconsistently applied, should not be used in analysis.</p></li>
<li><p>mention_list - Array of strings for annotated mentions as identified by human reviewers. Not an exhaustive list of mentions for any given dataset, and only populated for those data sets cited in the phase 1 train-test corpus.</p></li>
</ul>
<p>The mention list is the superset of all unique mention strings associated with each data set across all of that data set’s citations where mention data was created. Mention data was only created for data sets cited in the phase 1 train-test corpus.</p>
<p>ICPSR captured when a given data set was used in analysis within a particular publication, but it did not capture particulars on how that determination was made. To provide better data for participants, we implemented a human content analysis protocol to capture mention text for each data set-publication pair included in our train-test corpus (see <a href="#_23ckvvd"><span class="underline">Data Set Mention Annotation Process</span></a> below). Since we manually created this data, given limited time and resources, we initially only did this work for data sets that the teams would be using for training and testing in phase 1. In future work, we intend to provide this kind of information for all data sets of interest, and to refine the protocol to capture the exact position in the text of each mention along with the verbatim text.</p>
<p>Citations of data sets by publications within our phase 1 corpora were captured in separate data_set_citations.json files for each of the train-test and evaluation corpora. Each of these JSON files contains a JSON list of JSON objects, each of which specifics a single relationship between a data set and a publication. This JSON format is also used by models to output detected citations. Each citation contains:</p>
<ul>
<li><p>citation_id - A unique ID for the relationship between one dataset and one publication</p></li>
<li><p>publication_id - Unique ID for a publication which is the same ID for the publication in publications.json</p></li>
<li><p>data_set_id - Unique ID for a dataset which is the same ID for the dataset in the data_sets.json file.</p></li>
<li><p>mention_list - Optional array of strings for alternative references for the dataset in the specific publication (only present in citations included in train-test corpus, and even then, could still be empty).</p></li>
<li><p>score - Confidence score for the dataset being found in the related publication. In ICPSR-specified citations, the score will be 1.0. In model-created files, will depend on the model.</p></li>
</ul>
<p>Even citations from the phase 1 train-test corpus could have an empty mentions list. A given publication could, for example, have been tagged with a dataset by the curator (either at Bundesbank or ICPSR) based on knowledge of the publication and dataset, but a human coder without this knowledge was not subsequently able to find specific mentions within the publication, or the human coder could simply have missed the references. An empty mentions list is not a guarantee that the data set in question was not mentioned.</p>
<p>The list of data sets cited in a particular publication is also not exhaustive. There is the possibility that other data sets from our catalog of data sets of interest were used in analysis within a paper but not captured. The ICPSR data did not include mentions where data was not used in analysis, even of other ICPSR data sets. And named data sets not within our catalog of data sets of interest could also have been used in analysis within a given publication.</p>
<h3 id="data-set-mention-annotation-process">Data Set Mention Annotation Process</h3>
<p>The ICPSR data contains many explicit ties between publications and data sets that would have been hard to come by otherwise, but the lack of any indication of which parts of the publication indicated the citation relationship made it difficult to identify the linguistic context within the publication that captured the relationship.</p>
<p>To make it easier for participants in the competition to efficiently and systematically engage with the language used to discuss data, we developed a content analysis protocol and accompanying web-based coding application so human coders could examine all of the data set citations in our train-test corpus and capture mention text for each. This required human workers to examine each data set citation in the context of its publication (there were X citations in 2500 training publications) to identify and mark locations in the text where each data set was referenced.</p>
<p>Because of the manual effort required, we only did this for the 2,500 train-test publications that referenced data provided to the teams. We did not manually annotate mention text in the 2,500 publications in the phase 1 holdout, and this made that data a little less useful for teams when it was given to them in phase 2.</p>
<p>Our team of coders was spread across the United States, and so we used a web-based application with a central database store to allow our distributed team of coders to work in parallel. The basic unit of work was a publication-data set pair (so a given publication would be examined as many times as it had different data sets cited within it).</p>
<p>The ICPSR data set repository is very fine-grained in definition of a data set, so each year of an ongoing survey, for example, might have its own data set. To save time, we eventually created the concept of a data set family for these types of data sets and assigned coding for any one instance in a family to all other instances from that family within a given publication. So, for example, multiple years of the same survey or longitudinal data collection were related to each other in a family, and then coding for one year within a paper was used for all other years cited in that paper.</p>
<p>The general process:</p>
<ul>
<li><p>each user was assigned a list of citations to code.</p></li>
<li><p>Once the user logged in to the coding tool, they were presented with a list of the coding tasks assigned to them that included a status of each, so they could track which they had already completed, and a link for each to the coding page.</p></li>
<li><p>Once the user loads a particular citation for coding, they are presented with the following coding page, and are asked to follow the coding instructions in the codebook/documentation for the annotation tool (<a href="https://docs.google.com/document/d/1xuZL_-z1re6TO3Sv8_9tdFk7z6ovyqTwDVgc1bYO3Ag/edit"><span class="underline">https://docs.google.com/document/d/1xuZL_-z1re6TO3Sv8_9tdFk7z6ovyqTwDVgc1bYO3Ag/edit</span></a>):</p></li>
</ul>
<blockquote>
<p><img src="combined_images/chap05_figure1.png" style="width:6.5in;height:3.48611in" /></p>
<p><em>Figure 1. The interface of a given publication and a mention capturing process in the coding tool. The left pane contains a full text of an article to code. The right pane contains the coding interface at the top. The “Data Set Info” section contains basic metadata on the data set (title, date of collection, formal identifiers), as well as a list of synonyms gathered so far from publications where the data set is cited.</em></p>
</blockquote>
<p>Coders were instructed to find terms that relate to mentions of the dataset and avoid general synonyms of those terms (for example, tagging “<span class="underline">ANS survey</span>” instead of only “<span class="underline">survey</span>”). If the phrase provides additional information about collection of the dataset, the mention is tagged twice. For example, in the case of “<span class="underline">ANS survey collected/conducted by X</span>”, “<span class="underline">ANS survey</span>” is captured first, and then “<span class="underline">ANS survey collected/conducted by X</span>”. At the same time, we tried to avoid including too much descriptive information of the dataset - the task is just to code the specific mentions of a particular dataset, including alternate names (e.g. abbreviations, etc.), rather than trying to capture full text in which the data set is discussed.</p>
<p>For more details, including an FAQ that provides guidance on specific issues that arose during coding (like how to deal with data sets that span multiple years), see the content analysis protocol: <a href="https://docs.google.com/document/d/1xuZL_-z1re6TO3Sv8_9tdFk7z6ovyqTwDVgc1bYO3Ag/edit"><span class="underline">https://docs.google.com/document/d/1xuZL_-z1re6TO3Sv8_9tdFk7z6ovyqTwDVgc1bYO3Ag/edit</span></a></p>
<p>In total, a team of 5 coders, with a background in text analytics for policy research and computational linguistics, completed the task (Emily Wiegand, Neil Miller and Jenna Chapman from Chapin Hall at the University of Chicago, Mengxuan Zhao, Marcos Ynoa and Ekaterina Levitskaya from the CUNY Graduate Center, Computational Linguistics program). The results were then used to re-render data_sets.json and the data_set_citations.json file for the phase 1 train-test data to include mentions.</p>
<p>This combined protocol and tool were developed in-house. Considerations behind building in-house:</p>
<ul>
<li><p>From previous work, we had an open-source tool that did what we would need with minor tweaks, so were able to leverage substantial existing work, though we did have to pay for the work to customize it as well as the AWS t2.large instance on which we hosted it.</p></li>
<li><p>This tool includes templates for human-coding application pages like the one we used, but it is also designed to be used to build up data about publications from multiple sources and this data is straightforward to query and interact with. This allowed us to use the underlying database and application code as the competition dataset database, not just a place to handle mention coding.</p></li>
<li><p>We looked at off-the-shelf text annotators and Qualitative Analysis tool such as lighttag.io, tag.works, NVivo, Atlas.ti, MAXQDA. Unfortunately, given a tight timeline and relatively complex requirements, we didn’t have the time to come up to speed with any of these tools. In addition, we needed the tool to be usable by a distributed team, and that precluded some tools above that did not support distributed workflows.</p></li>
<li><p>For future coding work, we would love to be able to outsource coding tool development, and so are looking at distributed coding applications like lighttag.io and tag.works.</p></li>
</ul>
<h2 id="methods-and-fields">Methods and Fields</h2>
<p>For the task of detecting methods and fields for a given publication, our goals were broader than simply providing a vocabulary for each and asking the teams to classify publications against them. We want to encourage development of models that not only can determine when a given publication is a part of an existing field or uses an existing method, but that also understands enough about fields and methods such that they can be used to detect new fields and methods as they emerge, and can then be used to look back through time for traces of these new fields and methods to track their growth and evolution.</p>
<p>To support this goal, we did not give any formal set of either methods or fields that participants needed to train models to classify from. Instead, we provided examples of taxonomies of methods and fields that Sage Publications uses to classify their publications<a href="#fn73" class="footnote-ref" id="fnref73" role="doc-noteref"><sup>73</sup></a>, and we directed participants to use them as an example, but to try to make models that would be more creative and potentially able to find new, emerging, or novel fields rather than just fit a publication to a term from a predefined taxonomy.</p>
<p>In practice, this decision to forego any kind of fitting to an existing taxonomy showed the complexity of the problem of understanding fields and methods well enough to detect them based on linguistic context, rather than classifying to an existing vocabulary. Some teams limited themselves to the vocabularies we defined, and the results were uninspiring. Some teams tried to detect based on text, but ended up with a lot of noise and few relevant terms.</p>
<p>In addition, we also learned that there is complexity in “methods” that lumping all methods together did not account for: methods could mean many things, and we started to find sub-categories that we wish we had broken this into: statistical methods, analysis methods, data collection and creation methods, etc.</p>
<p>For future work, for each of these types of information, we intend to first work to decide what exactly we mean by “fields” and “methods”, then find or develop one or more taxonomies to precisely capture what we mean. Once we have these taxonomies, we’ll focus separately on building models to classify publications to them, and making models to extend and update them.</p>
<h1 id="submission-process">Submission Process</h1>
<p>The primary goals of the submission process developed for our competition were:</p>
<ul>
<li><p>to balance the effort needed for a particular group of participants to package their model for submission with the effort needed from the competition organizers to configure, run, and troubleshoot submissions once they were received.</p></li>
<li><p>to begin development of a model packaging strategy that could be used to distribute and allow reuse of any model that uses it.</p></li>
</ul>
<p>More specifically, we had the following requirements:</p>
<ul>
<li><p>Create submission infrastructure to make it as straightforward and easy as possible for a team to package their model for submission, including minimizing the understanding needed to use technologies chosen for packaging and deployment and having a built-in way to automatically run the model over the dev fold to validate processing of standard input formats and creation of required output formats.</p></li>
<li><p>Minimize the installation and configuration work needed on part of competition organizers to replicate computing environments as part of model submission process.</p></li>
<li><p>Maximize our ability to see and be able to test how each submission environment is set up, and so avoid accepting a blackbox that could contain anything (including malicious code or sneaky/clever tricks).</p></li>
</ul>
<h2 id="building-and-submitting-a-model">Building and Submitting a Model</h2>
<p>Our approach for participants building and submitting a model combines Box.com, docker, a git repo for code to implement and support infrastructure, and shell scripts. The central workspace for competition participants was a Box folder that contained example docker files, a copy of the dev fold, and shell scripts that implemented the basic steps of packaging, building, running, and testing a model. The git repository (<a href="https://github.com/Coleridge-Initiative/rich-context-competition"><span class="underline">https://github.com/Coleridge-Initiative/rich-context-competition</span></a>) was integral to our framework, but was not used directly by participants. Its code repository was solely used as a home for the code, scripts, and files that made up our submission framework. We did, however, host documentation for participants in the repository’s main README and its wiki (<a href="https://github.com/Coleridge-Initiative/rich-context-competition/wiki"><span class="underline">https://github.com/Coleridge-Initiative/rich-context-competition/wiki</span></a>).</p>
<p>To get started, participants downloaded a compressed archive of the Box folder and extracted it onto a system with a bash shell. Windows systems were supported, but we recommended that participants with Windows machines work inside a linux virtual machine.</p>
<p>This work folder contained:</p>
<ul>
<li><p>the script “rcc.sh” and its accompanying configuration “config.sh”, that implements all of the basic actions needed to manage docker for a model.</p></li>
<li><p>A set of scaffold files and folders that demonstrate how to hook a model into a docker container, including a Dockerfile with examples of installing OS packages and python packges in a docker container and an example “project” folder with a “code.sh” shell script that is called by default when the docker container is run, pre-configured to call a provided example python file named “project.py”.</p></li>
<li><p>A copy of the git repo, for use by the scripts.</p></li>
<li><p>A copy of the dev fold, in the standard data folder structure.</p></li>
</ul>
<p>The set of scaffold files provided out of the box could be used along with “rcc.sh” to create a simple docker container to test one’s local install of docker (including reading from and writing to a data folder configure in “config.sh”, running a script in the work folder, and creating output).</p>
<p>Participants were then instructed to work within the “project” folder in their work folder, get their code working first on their local machine, then set up a docker container using the provided example files and get the model running there, to isolate problems with docker from problems with their model.</p>
<p>When participants were ready to submit, they were asked to compress their work folder and upload it to the root of their group’s project folder and send an email to the organizers.</p>
<p>Participants were allowed 2 test submissions before the final submission, and most groups took us up on those test submissions in phases 1 and 2. All groups were able to work within the “code.sh” and “project.py” files in “project” to get their model to run, so no further customizations were needed.</p>
<h2 id="model-api">Model API</h2>
<p>Our submission framework used a file-system based API for giving the model input and accepting output. We interaction through the file system to keep the configuration and implementation simple.</p>
<p>Each time the docker container for a model is run, it is configured to work in a particular data folder.</p>
<p>This data folder has a standard directory structure:</p>
<p>data<br />
| _input<br />
| | _files<br />
| | _text<br />
| | _pdf<br />
|_output</p>
<p>All input information is stored in the “data/input” folder. All output is expected to be stored in the “data/output” folder.The input folder will contain a "publications.json" file, with the same contents as described above in the “Data → Publications” section of this chapter, that lists the articles to be processed in the current run of the model. Publication plain text is stored in “data/input/files/text”, one text file to a publication, with a given publication's text named "&lt;publication_id&gt;.txt". The original PDF files are stored in “data/input/files/pdf”, one PDF file to a publication, with a given publication's text named "&lt;publication_id&gt;.pdf".</p>
<p>The output folder starts out empty, and is where the model is expected to place 4 output files after each run of the model:</p>
<ul>
<li><p><strong>data_set_citations.json</strong> - A JSON file that contains publication-dataset pairs for each detected mention of any of the data sets provided in the contest data_sets.json file. The JSON file should contain a JSON list of objects, where each object represents a single publication-dataset pair.</p></li>
<li><p><strong>data_set_mentions.json</strong> - A JSON file that should contain a list of JSON objects, where each object contains a single publication-mention pair for every data set mention detected within each publication, regardless of whether a gvien data set is one of the data sets provided in the contest data set file.</p></li>
<li><p><strong>methods.json</strong> - A JSON file that should contain a list of JSON objects, where each object captures publication-method pairs.</p></li>
<li><p><strong>research_fields.json</strong> - A JSON file that should contain a list of JSON objects, where each object captures publication-research field pairs.</p></li>
</ul>
<h2 id="running-a-submitted-model">Running a Submitted Model</h2>
<p>Once a model was submitted, the competition organizers followed a standard script for running the model and processing its output for analysis:</p>
<ul>
<li><p>For each submission, an AWS instance was spun up from a standard image pre-configured to run models built using our submission framework.</p></li>
<li><p>The evaluator connected to the instance and started a screen session, so work would not be disrupted if connection to server was lost.</p></li>
<li><p>The model was downloaded to the server and extracted.</p></li>
<li><p>The submission container was built on the server using the provided Dockerfile and “rcc.sh”, and then the container was run over the dev fold to test basic functionality of the container and the model, and to give an estimate of time needed to complete.</p></li>
<li><p>Once the dev fold was successfully processed, “config.sh” was reconfigured to point at the evaluation corpus, and the model was run over the evaluation corpus.</p></li>
<li><p>Once the model completed, standard evaluation Jupyter notebooks in the git submission framework repository were configured to the current projects output and run to generate materials for judges to evaluate the submission.</p></li>
<li><p>Output and results were copied to a central storage area, and the instance used to run the model was terminated.</p></li>
</ul>
<p>Throughout this process, the evaluator communicated any problems with the participant team and worked with the team to address problems and turn around a new version of the model as quickly as possible. If a team’s model performed poorly on the standard size machine, we also would sometimes try different sizes of server to give them an idea of whether their problem was related to needing more compute power, or was a limitation of their approach independent of available resources.</p>
<h2 id="notes-on-the-submission-process">Notes on the Submission Process</h2>
<p>We chose Box.com because we have unlimited space there through NYU, and so we were able to accommodate not only whatever data participants needed to provide to make their models work, but also all of the data we provided to participants for training and testing. To minimize confusion, we pre-configured and shared each team’s Box folder with them, so they did not have to do any setup.</p>
<p>To setup the infrastructure in each folder, we created a git repository (https://github.com/Coleridge-Initiative/rich-context-competition) that contained all of the files, shell scripts, and templates needed to: 1) configure a new instance of a team folder, for use by competition staff setting up team folders; 2) develop, package and test deployment of a model (participants); and 3) support building, running, and evaluating the models once they were submitted.</p>
<p>We considered using github to store participant submissions, but chose Box because of its unlimited storage.</p>
<p>We considered using an external service like CodaLab or Kaggle, but an initial assessment of each suggested that they would not meet our needs without substantial changes to the design of our competition:</p>
<ul>
<li><p>Codalab looked promising, but its documentation was sparse and our time frame was short enough that we weren’t comfortable we could get up to speed with it quickly enough to make a reliable, easy-to-use competition with it.</p></li>
<li><p>Kaggle seemed designed for more basic competition designs (our evaluation steps were fuzzy, so couldn’t just take their outputs and make scores - this is not entirely a knock on them - it would be great to get our tasks to the point where they fit in this framework, we just don’t have the data yet), and there were also licensing complications we weren’t comfortable sorting out. We also needed control over manual evaluation and were concerned there that their submission and evaluation system wouldn’t support the bespoke nature of our submissions.</p></li>
<li><p>For both, we also simply weren’t comfortable that we’d be able to get up to speed on the platform in time to make the experience of participating in the competition as pleasant and painless as possible.</p></li>
</ul>
<p>We also wanted to have the flexibility to run many models in parallel and give models substantial resources if needed, to see how they performed with different magnitudes of computing resources and to allow us to try to throw raw compute power at a model if it was running too slowly, to get it to complete so we could give as good of feedback as possible. We not only wanted groups to be able to do preliminary submissions, but we wanted to make sure we could give as much feedback as possible. This led us toward a container-based approach where we did what we could to abstract and simplify the running of models, and allowed for flexibility and configurability in the instances that we spun up to run the models.</p>
<h1 id="evaluation">Evaluation</h1>
<p>In both phases of the competition, we evaluated raw mentions, research fields, and research methods separate from citation of named data sets.</p>
<h2 id="phase-1-evaluation">Phase 1 Evaluation</h2>
<h3 id="mentions-methods-and-fields">Mentions, Methods and Fields</h3>
<p>In phase 1, expert social science judges evaluated mentions, methods, and fields in two ways: 1) we randomly selected 10 publications to manually examine each team’s output against, and made notes of good and bad for each team, then ranked the teams within each publication; and 2) we generated distributions of all values found across all publications within each type of value, counted the occurrences of each, compared the distributions across teams, and ranked the teams based on how their distributions compared. To create overall rankings, the judges met, compared notes and individual rankings, and then agreed on an overall ranking of the teams.</p>
<h3 id="data-set-citations">Data Set Citations</h3>
<p>To evaluate data set citations in phase 1, we used the ICPSR citation data as our evaluation baseline for creating a confusion matrix based on how each team’s citation findings compared to ICPSR’s baseline, and we calculated precision, recall, and F1 scores from the confusion matrix to compare across teams. To create the confusion matrix for each team, we started with a list of all of the data set-publication pairs found either in ICPSR’s baseline or the team’s output. We created found-or-not (1 or 0) vectors for every publication-data set pair for the baseline, and for the team. Then, for each data set-publication pair, we compared the values between the baseline vector and the team vector to decide how to update the confusion matrix for that pair: if a team agreed with ICPSR on presence of a data set, that was counted as a true positive (TP). If the team found a data set that ICPSR did not, that was counted as a false positive (FP). If a team missed a data set ICPSR indicated was present, it was counted as a false negative (FN). We did not develop a way to capture true negatives since the metrics we used to evaluate did not require it. In addition, as part of the processing to create the overall confusion matrix, we created per-publication confusion matrices for each publication, so we could track average false positives and false negatives per publication, and highlight publications that were higher than the average, for more detailed evaluation.</p>
<p>We also deferred figuring out “mentioned” vs. “used in analysis” in our initial competition, to make the initial task more manageable. This decision, combined with the traits of the ICPSR data, caused substantial noise in the phase 1 precision/recall/F1 scores. For example, even models that figured out that a longitudinal data set was present sometimes got many false positives and false negatives because they got the years wrong, and models that correctly found ICPSR data sets used in discussion had those counted as false positives because ICPSR had only captured data sets used in analysis.</p>
<h2 id="phase-2-evaluation">Phase 2 Evaluation</h2>
<p>In evaluating phase 2, we kept the division between mentions, fields, and methods and citations, but we refined our evaluation methods in based on what we’d learned in the first phase.</p>
<p>Mentions, Methods and Fields</p>
<p>For mentions, methods, and fields in phase 2, we kept the basic strategy of: 1) comparing the values created by each team’s model in the context of a set of selected publications and 2) reviewing the overall distributions of values for each team.</p>
<p>We expanded the number of publications across which we compared values to make the sample reviewed more representative, though, and created a web-based tool to help judges deal with the added work from more publications to review. We also selected publications differently for data mentions from fields and methods, choosing publications with different levels of agreement between the teams on whether data was present or not, to start to evaluate the different model’s ability to detect data at all, in addition to comparing the results when they thought a publication contained data.</p>
<p>For fields and methods (and data set citations), we selected 20 publications for each of our 6 topic areas of interest (Education, Health care, Agriculture, Finance, Criminal justice, and Welfare) with a few extras (2 extra in finance and 1 extra in criminal justice), for a total of 123 publications to compare values across. Within the 20 publications per topic area, we worked through a random selection of articles picking publications to add to our sample to fill out a rough ratio within each topic area of 5:4:1 between publications with titled data sets (5); data described, but not titled (4); and no data (1).</p>
<p>To make it easier for the judges to work through this increased number of publications, we also created a tool that collected the output for each team side-by-side per publication along with a link to each publication’s PDF, and had a place for the judge to score each team’s output for a given publication from among “-1”, “0”, and “1”. Once judges scored all output, we then created rankings based on the sum of each team’s scores.</p>
<p><img src="combined_images/chap05_figure2.png" style="width:6.5in;height:5.01389in" /></p>
<p><em>Figure 2: The interface given to judges to evaluate data set mentions, research fields, and research methods.</em></p>
<p>For manual evaluation of data set mentions, we used the same tool described above, but we chose a different sample of 60 publications based on agreement between the output of the different participant team models as to whether publications had data mentions. To generate this sample, we first loaded all of the output from each team’s model into our work database. We then made a list of all of the publications in our phase 2 holdout and, for each publication, the count of teams that had data set mentions for that publication. We then sampled to get 60 publications:</p>
<ul>
<li><p>10 publications where all teams agreed there was no data.</p></li>
<li><p>10 publications where all teams agreed there was data.</p></li>
<li><p>40 publications where the teams disagreed on whether there was data.</p></li>
</ul>
<p>For the 40 publications with disagreement, we selected publications with 1 team, 2 teams, and 3 teams agreeing data was present proportional to the distribution of each level of agreement in the broader sample:</p>
<ul>
<li><p>17 from 1 (1439/5000 = 0.2878; 0.2878 * 60 = 17.268)</p></li>
<li><p>20 from 2 (1741/5000 = 0.3482; 0.3482 * 60 = 20.892)</p></li>
<li><p>13 from 3 (1080/5000 = 0.216; 0.216 * 60 = 12.96)</p></li>
</ul>
<p>We then asked a separate pair of qualitative judges to use the tool to compare and evaluate the data set mentions generated by the teams across these publications.</p>
<h3 id="data-set-citations-1">Data Set Citations</h3>
<p>Our analysis of data set citations in phase 2 required a more substantial rethinking since we did not have any starting point for presence or absence of data like the ICPSR corpus. We implemented a method of creating a confusion matrix that could be used to generate precison, recall, and F1 scores more closely aligned with the task we’d assigned the teams to implement - finding mentions of data and data sets within publications.</p>
<p>To implement this, we started with the sample of 123 publications used for evaluating mentions and fields above and:</p>
<ul>
<li><p>Captured all “data references” within each of those publications using a new human coding protocol. This included external titled data sets either discussed or used in analysis, external data without a title that was discussed or used in analysis, and data created by the researcher for a given study.</p></li>
<li><p>For each data reference, we compared all mentions and citations created by each team for the publication to the information on the data reference within that publication and marked any that were “related” to the data reference.</p></li>
<li><p>Finally, we used the list of references as a baseline and built a confusion matrix based on whether each team had found mentions or citations “related” to each of the data references, along with a “false positive” record where the baseline was always 0 and the team was assigned a 1 if they had one or more mentions or citations that were not “related” to any data reference.</p></li>
</ul>
<h4 id="capturing-data-references">Capturing Data References</h4>
<p>To capture data references in our sample of publications, we created a basic protocol for an initial round of data creation (<a href="https://docs.google.com/document/d/1aFPEtT4hd93kcsOEzocyB6-a4Hu8WcemKTld-98Q25k/edit#heading=h.f3u3kdbg87s4"><span class="underline">https://docs.google.com/document/d/1aFPEtT4hd93kcsOEzocyB6-a4Hu8WcemKTld-98Q25k/edit#heading=h.f3u3kdbg87s4</span></a>), then evaluated the results throughout the rest of the process. We used a single data reference coder to encourage consistency in output. Our data reference coder worked within a spreadsheet to, for each publication in our sample:</p>
<ul>
<li><p>Flag all paragraphs where data was mentioned.</p></li>
<li><p>Cluster mentions together that refer to a single dataset.</p></li>
<li><p>Give each cluster of mentions a row in the spreadsheet. These are our “data references”.</p></li>
<li><p>Then, for each data reference:</p>
<ul>
<li><p>Collect all mentions that refer to the reference.</p></li>
<li><p>decide if the data set is simply cited (“cited”), or if it is one used in analysis (“analysis”) in the publication</p></li>
<li><p>Capture words or phrases that are key to identification as “key terms”.</p></li>
<li><p>Also capture any broader contextual text in “Context”, so it could be used to better understand the nature of the “data reference”.</p></li>
<li><p>If data set title is present, capture it.</p></li>
<li><p>Try looking up the data set in the database, and if it is there, store its data set ID.</p></li>
</ul></li>
</ul>
<p>We tried to capture detailed context on each reference for a couple reasons: 1) To make it easier for reviewers of this data to evaluate the quality of each data reference; 2) To give more context for judges deciding if mentions and citations for a given team were “related” to a given data reference.</p>
<h4 id="finding-related-mentions-and-citations">Finding Related Mentions and Citations</h4>
<p>After the data references were captured, a team of coders then looked at each data reference related to the selected publications for each team to see if data set citations and mentions by the team were “related” to the data reference.</p>
<p>The coders, subject matter experts in the different key topic areas, looked at each “data reference” in publications in their area of expertise. For each, they evaluated it against the mentions and citations output by the model of each team that found mentions or citations in the selected publication. For each reference-team pair, the coder flagged any mentions or citations they deemed “related to” the current data reference.</p>
<p>In our protocol (<a href="https://docs.google.com/document/d/1Hi13N6gfiRz9nfwCoUQrey8v_ozY7fKHMtHV4GgX2ys/edit"><span class="underline">https://docs.google.com/document/d/1Hi13N6gfiRz9nfwCoUQrey8v_ozY7fKHMtHV4GgX2ys/edit#</span></a>), we describe the coding task as “When you are judging data mentions, we want to mark mentions on the right as”exists" if they are related to the data referenced on the left, and make sure to not mark any mentions as "exists" that are not related.“, balanced with”If in doubt, don’t mark a given mention as related."</p>
<p>The definition of “related to” is purposely fuzzy. Our goal was to give credit for finding language related to a dataset even if it wasn’t a perfect, formal reference, but to also make sure to not mark things that are obviously unrelated. To help to flesh this distinction out, we gave examples and analogies and training, and we had coders work through a few data references on their own then discuss their decisions.</p>
<p>An example from the protocol: “Think of it as a fuzzy match - we want to give the models the benefit of the doubt if they get close, especially if they detect some but not all key terms or phrases or find a mention of the basic type of data a named data set represents (”wage data" for IDES Unemployment Wage Records, for example), but we also want to make sure to reject things that are obviously not related."</p>
<p>Coders used a web-based coding tool that listed out their assigned coding tasks and pulled together all of the information so they just had to scan the page, open the associated PDF if they had questions, and then mark related items and Submit to save their coding:</p>
<p><img src="combined_images/chap05_figure3.png" style="width:6.5in;height:5.01389in" /></p>
<p><em>Figure 3: The interface given to judges to evaluate whether a given team’s data set mentions and citations were related to a given data reference.</em></p>
<p>As one would expect, while we got coders on the same page, each had subtly different ideas about what was or was not “related to”. To remove some of this variability from our final data, we then had a sole experienced researcher who understood what we were trying to do review all coding and, when he saw coding that obviously did not fit his understanding, either: revise to fit his understanding of “related to”; or flag as one he was unsure of and note his thoughts.</p>
<p>This experienced researcher also served as a final reviewer of the data references that were collected, marking any that did not actually refer to data as needing to be removed from our final analysis.</p>
<p>Finally, the protocol designer reviewed all removed data references, corrections, and ambiguities flagged for additional review, and made a final set of corrections.</p>
<h4 id="scoring-the-results">Scoring the Results</h4>
<p>To create a “related to” confusion matrix for each team, we started with a list of all of the data references that our final reviewers indicated should be included in our analysis (165 total). We created found-or-not (1 or 0) vectors with a value for every reference set to 1 for the baseline, and then set based on our coding for each team. For each publication, we also included a false positive item that was always 0 for the baseline, and that was set to 1 for a given team if they had any mentions or citations that were not “related to” a data reference from that publication.</p>
<p>To build a given team’s vector, for each data references, we checked to see if any of the team’s mentions or citations had been marked as “related to” that reference. If one or more of the team’s mentions or citations was marked as “related to”, we gave that reference a “1” for that team. If not, we gave it a “0”. Then, for each publication’s false positive item, if the team had 1 or more mentions and/or citations that were not “related to” any data reference, the team got a “1” for that entry. If not, they got a “0”.</p>
<p>To build out a confusion matrix, we went reference by reference: If the team found mentions and/or citations related to the reference, that was counted as a true positive (TP). If a team did not have any mentions or citations related to a given data reference, it was counted as a false negative (FN). Then, for the publication, if the team had 1 or more mentions and/or citations that were not “related to” any data reference, this was counted as a false positive (FP).</p>
<p>We did not develop a way to capture true negatives since the metrics we used to evaluate did not require it.</p>
<h1 id="discussion-1">Discussion</h1>
<p>Given the time and resources available to put the competition together, the competition’s design was effective, but required some iteration within each of the phases. We modified and updated both training data and model submission infrastructure in response to participant feedback, and the participants were generally quite positive about the experience.</p>
<p>The docker-based model submission process worked well for the competition, but subsequent use of the models by Digital Science and Bundesbank has revealed a need to more precisely design how the models work within their docker container and the APIs they provide so packaged models implement a more re-usable API. For example, to be readily able to be used within an existing environment, the model needs to be able to be invoked from a simple unit of code (a python function, for example), rather than needing to spin up an instance of a container each time you want results.</p>
<p>To facilitate re-use, we need much more detailed specification of how the participants should implement their models. For example:</p>
<ul>
<li><p>If a submission is implementing multiple tasks, each should be broken into its own separate API so it can be used separately (so separate services for mention detection, field detection, and data detection).</p></li>
<li><p>We need to better specify how we expect the models to be re-trained, in particular elements of the model we expect to be easily changed and which we expect would require a full retraining to tune. For example, we hoped to be able to easily switch out the data sets of interest that are detected specifically without needing to retrain on a full corpus referring to those data sets, but we didn’t mention this, and none of the models worked this way.</p></li>
</ul>
<p>In terms of community building, we inspired participation and the workshop and discussions after the competition lead to collaborations between pairs of sponsors and participants and collective work on making a gold standard corpus that could be used to develop better models in the future (a great step toward higher quality models), but we need to continue to work to nurture and grow this community.</p>
<p>The data for the competition was a great start, but trying to use it to detect data mentions and then start to get at whether data was simply discussed or actually used in analysis revealed how much work remains to make high quality training data. The base ICPSR data did not include mention text where we did not create it, and so for the majority of data sets, the only text available for characterizing a data set was the title and a paragraph of description, no examples of how the data would be discussed within a publication. It also did not capture non-ICPSR data sets, nor did it include data sets mentioned but not used in analysis. We need to be able to work with imperfect data, but the complexity of this task makes it a good fit for better training data. We also found that our definition of a data set was too specific – ICPSR is granular down to the year of some of their formal data collections. Data signatures of interest in the real world might just be clusters of key terms without a formal title, and our data and models need to account for this.</p>
<p>Our evaluation approaches were effective given the time we had, but they also had significant limitations. In phase 1, the ICPSR data was great for a model that finds named data sets used in analysis, but it was not as good a fit for evaluating models trying to detect data citations in general. For example, some high quality models were scored with many false positives that, on review, were actually correct, but for non-ICPSR data sets.</p>
<p>In phase 2, our design and evaluation data creation attempted to account for the limitations of phase 1 - to move from just looking at titled ICPSR and Bundesbank data sets used for analysis and begin to look at all the ways data is discussed in academic papers, and how much of that discussion the combined mentions and citations of each team was aware of. Its effectiveness depended on how well we designed and carried out each of these three steps.</p>
<p>We are comfortable with the quality of the resulting data, but it should be noted that given the time and resources available to us, we had to make a choice between quality of data and reproducibility. In a perfect world, content analysis is the discipline of reliably being able to use a well-designed protocol to create content of comparable quality regardless of who does the coding. Given this project’s relatively tight timelines and limited resources, in this process we prioritized quality of data over reproducibility. We created relatively detailed coding protocols for each step of the process and we designed review and refinement into our processes, but we did not have time to go through multiple rounds of training and evaluation to make each of the protocols reliable and reusable. At the end, we introduced consistency by having experienced researchers familiar with our goals review all the output and either correct problems or flag items that should not be used in analysis. We believe that this created a reasonable level of consistency and quality in our output, but intend to refine these protocols for use in the future,</p>
<h1 id="conclusion-1">Conclusion</h1>
<p>Given the time and resources available, we consider the competition design to have been effective. The design attracted letters of intent from 20 teams from 8 countries and 12 teams actually submitted code. The models were interesting and some of the solutions were novel and surprisingly effective given their novelty. A nascent community of practice was also formed. Discussions after the competition led to collaborations between participants and collective work on making a gold standard corpus that could be used to develop better models in the future – an important step toward higher quality models. The models also ended up being re-usable as they are, though in a limited scope, and at least one sponsor has been able to run them and get useful output.</p>
<p>Additional work is continuing in three directions. The first is developing better corpora: that is discussed in the Appendix to this chapter by Alex Wade and Sebastian Kohlmeier. The second is developing a community of practice: a recent workshop (<a href="https://coleridgeinitiative.org/richcontextworkshop" class="uri">https://coleridgeinitiative.org/richcontextworkshop</a>) took the next step to doing so. The third is to further develop the machine learning and natural language processing tools through broader based competition: this is discussed in more detail in the concluding chapter in this book.</p>
<h1 id="appendix---standardized-metadata-full-text-and-trainingevaluation-for-extraction-models">Appendix - Standardized Metadata, Full Text and Training/Evaluation for Extraction Models</h1>
<p>Key challenges when working on an NLP task like dataset mention extraction that requires access to scholarly literature include the proliferation of metadata sources and sourcing of full text content. For example, each metadata source has their own approach for disambiguation (e.g. recognizing that A. Smith and Anna Smith are the same author) or de-duplication of content (clustering pre-prints and final versions into a single record). As a result competition organizers and NLP researchers currently use ad-hoc processes to identify metadata and full text sources for their specific tasks which results in inconsistencies and a lack of versioning of input data across competitions and projects.</p>
<p>One way these challenges can be addressed is by using a trustworthy metadata source like <a href="http://api.semanticscholar.org/corpus/"><span class="underline">Semantic Scholar’s open corpus</span></a> developed by the Allen Institute for Artificial Intelligence (AI2) or <a href="https://docs.microsoft.com/en-us/academic-services/graph/reference-data-schema"><span class="underline">Microsoft’s Academic Graph</span></a> that make it easy to access standardized metadata from an openly accessible source. In addition, both Semantic Scholar and the Microsoft Academic Graph provide topics associated with papers which makes it easy to narrow down papers by domain. If full text is needed we recommend tying the metadata to a source of open access full text content like <a href="https://unpaywall.org/data-format"><span class="underline">Unpaywall</span></a> to ensure that the full text can be freely redistributed and leveraged for model development.</p>
<p>To gather the data we recommend collecting a sufficiently large set of full text papers (3,000-5,000 minimum) with their associated metadata and providing participants with a standardized format of the full text. More data might be required if data is split across many scientific domains. For example for a task like dataset extraction, reference formatting is often inconsistent across domains and dataset mentions can potentially be found in different sections (e.g. background, methods, discussion, conclusion or the reference list) throughout the text. Once a decision has been made on the full text to include, the PDF content can be easily converted into text in a standardized format using a PDF to text parser like <a href="https://github.com/allenai/spv2"><span class="underline">AI2’s ScienceParse</span></a> (which handles key tasks like metadata, section heading and references extraction).</p>
<p>Once the metadata and full text dataset has been created it can be easily versioned and used again in future competitions. For example, if updated metadata is needed it’s easy to go back to the original metadata source (for example by using Semantic Scholar’s <a href="http://api.semanticscholar.org/"><span class="underline">API</span></a>) to get the latest metadata.</p>
<p><strong><span class="underline">Annotation Protocols to Produce Training &amp; Evaluation Data</span></strong></p>
<p>A common approach to machine learning known as <strong>supervised learning</strong> uses labelled, or annotated, data to train a model what to look for. If labelled data is not readily available, human annotators are frequently used to label, or code, a corpus of representative document samples as input into such a model. Different labelling tasks may require different levels of subject domain knowledge or expertise. For example, coding a document for different parts of speech (POS) will require a different level of knowledge than coding a document for mentions of upregulation of genes. The simpler the labelling task, the easier it will be for the coders to complete the task, and the more likely the annotations will be consistent across multiple coders. For example, a task to identify a <em>mention of a dataset</em> in a document might be far easier than the task of identifying only the<em>mentions of</em> <em>datasets that were used in the analysis phase of research</em>.</p>
<p>In order to scale the work of labelling, it is usually desirable to distribute the work amongst many people. Generic crowdsourcing platforms such as Amazon’s Mechanical Turk can be used in some labelling exercises, as can more tailored services from companies such as TagWorks and Figure-Eight. Whether the labelling is done by one person or thousands, the consistency and quality of the annotations needs to be considered. We would like to build up a sufficiently large collection of these annotations and we want to ensure that they are of a high quality. How much data needs to be annotated depends on the task, but in general, the more labelled data that can be generated the more robust the model will be.</p>
<p>As mentioned above, we recommend 3000-5000 papers, but this begs the question of how diverse the subject domains are within this corpus. If the papers are all within from the finance sector, then a resulting model might do well in identifying datasets in finance, but less well in the biomedical domain since the model was not trained on biomedical papers. Conversely, if our 3000-5000 papers are evenly distributed across all domains, our model might be more generically applicable, but might do less well over all since it did not contain enough individual domain-specific examples.<br />
<br />
As a result, we recommend labelling 3000-5000 papers within a domain, but we plan to do so in a consistent manner across domains so that the annotations can be aggregated together. In this manner, as papers in new domains are annotated, our models can be re-trained to expand into new domains. In order to achieve this, we intend to publish an open annotation protocol and output format that can be used by the community to create additional labelled datasets.</p>
<p>Another factor in deciding the quantity is the fact that the annotations will be used for two discrete purposes. The first is to <em>train</em> a machine learning model. This data will inform the model what dataset mentions look like, from which it will extract a set of features that the model will use and attempt to replicate. The second use of the annotations is to <em>evaluate</em> the model. How well a model performs against some content that it has never seen before. In order to achieve this, labelled data are typically split randomly into training and evaluation subsets.</p>
<p>One way to evaluate how well your model performs is to measure the <strong>recall</strong> and <strong>precision</strong> of the model’s output, and in order to do this we can compare the output to the labelled evaluation subset. In other words, how well does our model perform against the human annotations that it was not trained on and has never seen. Recall is the percentage of right answers the model returned. For example, if the evaluation dataset contained 1000 mentions of a dataset, and the trained model returned 800 of them, then the recall value would be .80. But what if the model returned everything as a dataset, then it would get all 1000, plus a whole bunch of wrong answers. Obviously, the precision of the model is important too. Precision is the percentage of answers returned that were right. So, continuing the example above, if the model returned 888 answers, and 800 of those were right, then the precision of the model would be ~.90. But again, if the model returned only one right answer and no wrong ones, the precision would be perfect. So, it is important to measure both precision and recall. In summary, the model in this example, got 80% of the right answers, and 90% of the answers it returned were right. The two measures of recall and precision can be combined into an F1 score of ~.847.<br />
<br />
If we then make modifications to our model, we can re-run it against the evaluation dataset and see how our F1 score changes. If the score goes up, then our new model performed better against this evaluation data. If we want to compare several different models to see which one performed best, we can calculate an F1 score for each of them. The one with the highest F1 score has performed the best. Consequently, the quality of the annotations are critical for two reasons: first, the accuracy of a <em>model</em> will only be as good as the data upon which it was trained. And secondly, the accuracy of the <em>evaluation</em> (in this case the F1 score) can be affected by the quality of the data it is evaluated against.</p>
<h1 id="references-3">References</h1>
<p>1. D. A. Murdick, Foresight and understanding from scientific exposition (FUSE). <em>Retrieved June</em>. <strong>21</strong>, 2015 (2011).2. D. Murdick, “Finding Patterns of Emergence in Science and Technology” (OFFICE OF THE DIRECTOR OF NATIONAL INTELLIGENCE WASHINGTON DC INTELLIGENCE ADVANCED RESEARCH PROJECTS ACTIVITY, 2012).3. J. Perrie <em>et al.</em>, Implementing Recommendation Algorithms in a Large-Scale Biomedical Science Knowledge Base. <em>arXiv Prepr. arXiv1710.08579</em> (2017).4. D. Brickley, M. Burgess, N. Noy, in <em>The World Wide Web Conference</em> (ACM, 2019), pp. 1365–1375.5. S. Ovadia, ResearchGate and Academia. edu: Academic social networks. <em>Behav. Soc. Sci. Librar.</em> <strong>33</strong>, 165–169 (2014).6. I. Soboroff, I. Ounis, C. Macdonald, J. J. Lin, in <em>TREC</em> (Citeseer, 2012), vol. 2012, p. 20.7. M. Kremer, H. Williams, Incentivizing innovation: Adding to the tool kit. <em>Innov. policy Econ.</em> <strong>10</strong>, 1–17 (2010).8. B. D. Wright, The economics of invention incentives: Patents, prizes, and research contracts. <em>Am. Econ. Rev.</em> <strong>73</strong>, 691–707 (1983).9. H. L. Williams, Innovation Inducement Prizes: Connecting Research to Policy. <em>J. Policy Anal. Manag.</em> <strong>31</strong>, 752–776 (2012).10. T. Kalil, <em>Prizes for technological innovation</em> (Brookings Institution Washington, DC, 2006).11. K. Boudreau, K. Lakhani, How to manage outside innovation. <em>MIT Sloan Manag. Rev.</em> <strong>50</strong>, 69 (2009).12. J. Mateos-Garcia, W. E. Steinmueller, The institutions of open source software: Examining the Debian community. <em>Inf. Econ. Policy</em>. <strong>20</strong>, 333–344 (2008).13. B. M. Sadowski, G. Sadowski-Rasters, G. Duysters, Transition of governance in a mature open software source community: Evidence from the Debian case. <em>Inf. Econ. Policy</em>. <strong>20</strong>, 323–332 (2008).14. W. W. Chapman <em>et al.</em>, Overcoming barriers to NLP for clinical text: the role of shared tasks and the need for additional creative solutions (2011).15. A. Tong <em>et al.</em>, Overview of the NIST 2016 LoReHLT evaluation. <em>Mach. Transl.</em> <strong>32</strong>, 11–30 (2018).16. L. Wissler, M. Almashraee, D. M. Díaz, A. Paschke, in <em>IEEE GSC</em> (2014).17. D. Riff, S. Lacy, F. Fico, B. Watson, <em>Analyzing media messages: Using quantitative content analysis in research</em> (Routledge, 2019).18. S. C. Lewis, R. Zamith, A. Hermida, Content analysis in an era of big data: A hybrid approach to computational and manual methods. <em>J. Broadcast. Electron. Media</em>. <strong>57</strong>, 34–52 (2013).</p>
<hr />
<h1 id="chapter-6---finding-datasets-in-publications-the-allen-institute-for-artificial-intelligence-approach">Chapter 6 - Finding datasets in publications: The Allen Institute for Artificial Intelligence approach</h1>
<h1 id="introduction-5"><a href="#sec:intro">Introduction</a></h1>
<p>The Allen Institute for Artificial Intelligence (AI2) is a non-profit research institute founded by Paul G. Allen with the goal of advancing artificial intelligence research for the common good. One of the major undertakings at AI2 is to develop an equitable, unbiased software platform Semantic Scholar<sup>1</sup> for finding relevant information in the scientific literature. Semantic Scholar extracts meaningful structures in a paper (e.g., images, entities, relationships) and links them to other artifacts when possible (e.g., knowledge bases, GitHub repositories), hence our interest in the rich context competition (RCC). In particular, we participated in the RCC in order to explore methods for extracting and linking datasets used in papers. At the time of this writing, Semantic Scholar comprehensively covers the computer science and biomedical literature, and we plan to expand our coverage in 2019 to other scientific areas, including social sciences.</p>
<p>In the following sections, we describe our approach to the three tasks of the RCC competition, which are described in more detail in Chapter 5: 1. extracting the datasets used in publications, 2. predicting the field of research of publications 3. extracting the methods used in publications</p>
<h1 id="methods"><a href="#sec:methods">Methods</a></h1>
<h2 id="dataset-extraction-and-linking"><a href="#sec:datasets_methods">Dataset Extraction and Linking</a></h2>
<p>This task focuses on identifying datasets used in a scientific paper. Datasets which are merely mentioned but not used in the research paper are not of interest. This task has two sub-tasks:</p>
<ol type="1">
<li><p>Citation prediction: extraction and linking to a provided knowledge base of <em>known datasets</em>, and</p></li>
<li><p>Mention prediction: extraction of both <em>known and unknown</em> dataset mentions.</p></li>
</ol>
<h3 id="provided-data">Provided Data</h3>
<p>The provided knowledge base of known datasets includes approximately 10K datasets used in social science research. The high textual similarity between different datasets in the knowledge base informs our approach for linking dataset mentions to their dataset in the knowledge base. Approximately 10% of the datasets in the knowledge base were linked one or more times in the provided corpus of 5K papers. To attempt to generalize mention discovery beyond those present in the knowledge base, we train a named entity recognition (NER) model on the noisy annotations provided by the labeled mentions in the knowledge base.</p>
<p><img src="combined_images/datasets.png" alt="image" /> Figure 6.1: A high-level overview of our appraoch to dataset mention detection and linking.</p>
<p>We provide a high-level overview of our approach in Figure 6.1. First, we use an NER model to predict dataset mentions. For each mention, we generate a list of candidate datasets from the knowledge base. We also developed a rule based extraction system which searches for dataset mentions seen in the training set, adding the corresponding dataset IDs in the training set annotations as candidates. We then use a binary classifier to predict which of these candidates is a correct dataset extraction.</p>
<p>Next, we describe each of the sub-components in more detail.</p>
<h3 id="mention-and-candidate-generation">Mention and Candidate Generation</h3>
<p>We first constructed a set of rule based candidate citations by exact string matching mentions and dataset names from the provided knowledge base. We found this to have high recall and low precision, both on the provided development fold and our own development fold that we created. High recall and low precision was the desired outcome for this candidate generation step. However, after our test submission, it became clear that there were many datasets in the actual test set that did not have mentions in the provided knowledge base. If the provided development fold had been representative of the test set (rather than the train set) in terms of what datasets were mentioned in it, we could have discovered this issue sooner. In this case, it would have been more representative of the test set if it included more datasets that did not have example mentions in the provided knowledge base. The importance of reliable evaluation and training data is discussed further in Chapter 12.</p>
<p>To address this limitation, we developed an NER model to predict additional dataset mentions. For NER, we use a bi-LSTM model with a CRF decoding layer, similar to Peters et al., 2018, and implemented using the AllenNLP framework<sup>2</sup>. In order to train the NER model, we automatically generate mention labels by string matching mentions in the provided annotations against the full text of a paper. This results in noisy labeled data, because it was not possible to find all correct mentions this way (e.g., some dataset mentions were not annotated), and the same string can appear multiple times in the paper, while only some are correct examples of dataset usage.</p>
<p>We limit the percentage of negative examples (i.e., sentences with no mentions) used in training to 50%, and use 40 words as the maximum sentence length. We use 50-dimensional Glove word embeddings (Pennington et al., 2014), 16-dimensional character embeddings with 64 CNN filters of sizes (2, 3, 4). The CNN character encoder outputs 128-dimensional vectors. We optimize model parameters using ADAM (Kingma and Ba, 2014) with a learning rate of 0.001. Training the model took approximately 12 hours on a single GPU.</p>
<p>In order to generate linking candidates for the NER mentions, we score each candidate dataset based on TF-IDF weighted token overlap between the mention text and the dataset title. For a given mention, many dataset titles can have a non-zero overlap score, so we take the top 30 scoring candidates for each mention as the linking candidates for that mention.</p>
<h3 id="candidate-linking">Candidate Linking</h3>
<p>The linking model takes as input a dataset mention, its context, and one of the candidate datasets in the knowledge base, and outputs a binary label. We use a gradient boosted trees classifier using the XGBoost<sup>3</sup> implementation. The model takes as input the following features:</p>
<ul>
<li>prior probability of entity, estimated based on number of occurrences in the training set (float between 0 and 1)</li>
<li>prior probability of entity given mention, estimated based on number of occurrences in the training set (float between 0 and 1)</li>
<li>prior probability of mention given entity, estimated based on number of occurrences in the training set (float between 0 and 1)</li>
<li>whether the same year appears both in the mention context and in the dataset title (binary)</li>
<li>mention length (int)</li>
<li>mention sentence length (int)</li>
<li>whether the mention is an acronym, computed by checking if it is one token that is all upper case (binary)</li>
<li>estimated section title of the mention, computed by searching backwards from the mention for the nearest section header (binary one-hot)</li>
<li>count of overlapping words between the mention context and dataset keywords provided in the knowledge base (int)</li>
</ul>
<p>We note that it is possible to predict zero, one or multiple dataset IDs for the same mention, and each dataset candidate is scored independently.</p>
<p>We performed a randomized hyperparameter search with 100 iterations over the following hyperparameters and ranges and used a learning rate of 10^-1:</p>
<ul>
<li><code>max_depth</code> : <code>range(2, 8)</code></li>
<li><code>n_estimators</code> : <code>range(1, 50)</code></li>
<li><code>colsample_by_tree</code> : <code>numpy.linspace(0.1, 0.5, 5)</code></li>
<li><code>min_child_weight</code> : <code>range(5, 11)</code></li>
</ul>
<p>Each model took a negligible amount of time to train, and the entire hyperparameter search took a few minutes to train on a machine with 8 CPUs.</p>
<h2 id="research-area-prediction"><a href="#sec:areas_methods">Research Area Prediction</a></h2>
<h3 id="data-1">Data</h3>
<p>The second task of the competition is to predict research areas of a paper. The task does not specify the set of research areas of interest, nor is training data provided for the task. After manual inspection of a subset of the papers in the provided test set, the SAGE taxonomy of research, and the Microsoft Academic Graph (MAG) (Shen et al., 2018), we decided to use a subset of the fields of study in MAG as labels. In particular, we included all fields related to social science or papers from the provided training corpus. However, since the abstract and full text of papers are not provided in MAG, we only use the paper titles for training our model. The training data we ended up with included approximately 75K paper titles along with their fields of study as specified in two levels of the MAG hierarchy. We held out about 10% of the titles for development data. The coarse level (L0) has 7 fields while the more granular one (L1) has 32. Fields associated with less than 100 papers were excluded.</p>
<h3 id="methods-1">Methods</h3>
<p>For each level, we trained a bi-directional LSTM which reads the paper title and predicts one of the fields in this level. We additionally incorporate ELMo embeddings (Peters et al., 2018) to improve performance. In the final submission, we always predict the most likely field from the L0 classifier, and only report the most likely field from the L1 classifier if its prediction exceeds a score of 0.4. It takes approximately 1.5 and 3.5 hours for the L0 and L1 classifiers to converge, respectively.</p>
<h2 id="research-method-extraction"><a href="#sec:methods_methods">Research Method Extraction</a></h2>
<h3 id="data-2">Data</h3>
<p>The third task in the competition is to extract the scientific methods used in the research paper. Since no training data was provided, we started by inspecting a subset of the provided papers to get a better understanding of what kind of methods are used in social science and how they are referred to within papers. This limitation, and the difficulty of working on an undefined task, is discussed in Chapter 2.</p>
<h3 id="methods-2">Methods</h3>
<p>Based on the inspection, we designed regular expressions which capture common contextual patterns as well as the list of provided SAGE methods. In order to score candidates, we used a background corpus to estimate the salience of candidate methods in a paper. Two additional strategies were attempted but proved unsuccessful: a weakly-supervised model for named entity recognition, and using open information extraction (openIE) to further generalize the list of candidate methods.</p>
<h1 id="results"><a href="#sec:results">Results</a></h1>
<h2 id="dataset-extraction-and-linking-1"><a href="#sec:datasets_results">Dataset Extraction and Linking</a></h2>
<p>First, we report the results of our NER model in Table 6.1. Since it is easy for the model to memorize the dataset mentions seen at training time, we created disjoint train, development, and test sets based on the paper–dataset annotations provided for the competition. In particular, we sort datasets by the number of papers they appear in, then process one dataset at a time. For each dataset, we choose one of the train, development, or test splits at random and add the dataset to it, along with all papers which mention that dataset. When there is a conflict, (e.g., a paper <em>p</em> has already been added to the train split when processing an earlier dataset <em>d<sub>1</sub></em>, but it is also associated with a later dataset <em>d<sub>2</sub></em>), the later dataset <em>d<sub>2</sub></em> along with all papers associated with it are added to the same split as <em>d<sub>1</sub></em>. For any further conflicts, we prefer to put papers in the development split over the train split, and the test split over the development split.</p>
<p>We also experimented with adding ELMo embeddings (Peters et al., 2018), but it significantly slowed down training and decoding which would have disqualified our submission due to the runtime requirements of the competition. As a result, we decided not to include ELMo embeddings in our final model. If the requirements for the competition had permitted the use of a GPU at evaluation time, neural network based embeddings like ELMo could be leveraged.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>prec.</th>
<th>recall</th>
<th>F1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dev set</td>
<td>53.4</td>
<td>50.3</td>
<td>51.8</td>
</tr>
<tr class="even">
<td>test set</td>
<td>50.7</td>
<td>41.8</td>
<td>45.8</td>
</tr>
</tbody>
</table>
<p>Table 6.1: NER precision, recall and F1 performance (%) on the development and test sets.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>prec.</th>
<th>recall</th>
<th>F1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>baseline</td>
<td>28.7</td>
<td>58.0</td>
<td>38.4</td>
</tr>
<tr class="even">
<td>+ p(d|m), p(m|d)</td>
<td>39.6</td>
<td>42.0</td>
<td>40.7</td>
</tr>
<tr class="odd">
<td>+ year matching</td>
<td>35.1</td>
<td>57.0</td>
<td>43.5</td>
</tr>
<tr class="even">
<td>+ aggregated mentions, tuning, and other features</td>
<td>72.5</td>
<td>45.0</td>
<td>55.5</td>
</tr>
<tr class="odd">
<td>+ dev set examples</td>
<td>77.0</td>
<td>47.0</td>
<td>58.3</td>
</tr>
<tr class="even">
<td>+ NER mentions</td>
<td>56.3</td>
<td>62.0</td>
<td>59.0</td>
</tr>
</tbody>
</table>
<p>Table 6.2: End-to-end precision, recall and F1 performance (%) for citation prediction on the development set provided in phase 1 of the competition.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>prec.</th>
<th>recall</th>
<th>F1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>phase 1 holdout</td>
<td>35.7</td>
<td>19.6</td>
<td>25.3</td>
</tr>
<tr class="even">
<td>phase 2 holdout</td>
<td>39.6</td>
<td>18.8</td>
<td>25.5</td>
</tr>
</tbody>
</table>
<p>Table 6.3: End-to-end precision, recall, and F1 performance (%) for dataset prediction on the phase 1 and phase 2 holdout sets. Note that the phase 1 holdout results are for citation prediction, while the phase 2 holdout results are for mention prediction.</p>
<p>We report the end-to-end performance of our approach (on the development set provided by the organizers in the first phase) in Table 6.2. This is the performance after using the linking classifier to predict which candidate mention–dataset pairs are correct extractions. We note that the development set provided in phase 1 ended up having significantly more overlap with the training data than the actual test set did. As a result, the numbers reported in Table 6.2 are not indicative of test set performance. End to end performance from our phase 2 submission can be seen in Table 6.3. This performance is reflective of our focus on the linking component of this task. Aside from the competition development set, we also used a random portion of the training set as an additional development set. The initial model only uses a dataset frequency feature, which gives a baseline performance of 38.4 F1. Adding p(d <span class="math inline">∣</span> m) and p(m <span class="math inline">∣</span> d), which are the probability of entity given mention and probability of mention given entity improves the performance (<span class="math inline"><em>Δ</em> = 2.3</span> F1). Year matching helps disambiguate between different datasets in the same series, which was found to be a major source of errors in earlier models (<span class="math inline"><em>Δ</em> = 2.8</span> F1). Aggregating mentions for a given dataset, adding mention and sentence length features, adding an is acronym feature, and further hyper-parameter tuning improve the results (<span class="math inline"><em>Δ</em> = 12.5</span> F1). Adding examples in the development set while training the model results in further improvements (<span class="math inline"><em>Δ</em> = 2.8</span> F1). Finally, adding the NER-based mentions significantly improves recall at the cost of lower precision, with a positive net effect on F1 score (<span class="math inline"><em>Δ</em> = 0.7</span> F1).</p>
<p>Two clear limitations of our model are its difficulty in generalizing to unseen datasets, and its inability to effectively distinguish between datasets that are used in a publication and datasets that are merely referenced. These limitations are the main causes of the low recall (due to difficulty generalizing to unseen datasets) and low precision (due to difficulty distinguishing between used datasets and referenced datasets). An interesting approach to improving recall is presented in Chapter 7, and could potentially be leveraged in future work.</p>
<h2 id="research-area-prediction-1"><a href="#sec:areas_results">Research Area Prediction</a></h2>
<p>To select a model, we performed a 100 trial random search across model hyper-parameters, evaluated on a held out development set of papers from the Microsoft Academic Graph. Our final model contained 512 hidden dimensions, 2 layers and 0.5 dropout prior to classification. The top performing classifier achieved 84.4% accuracy on our development set on L0 fields, and 65.2% accuracy on our development set on L1 fields. The main limitation of using MAG for this problem is that our model cannot find new fields of research, and is limited to those provided by MAG. Additionally, our method performs classification based only on the titles of papers, while there are other pieces of information about the paper that would be useful for classifying the field of research. Other resources that could have been used to help with this task are presented in Chapters 7 and 8.</p>
<h2 id="research-method-extraction-1"><a href="#sec:methods_results">Research Method Extraction</a></h2>
<p>We evaluated performance by manually evaluating the output of our extractor for a subset of 50 papers from the provided test set to compute precision. Since evaluating recall requires a careful annotation, we resorted to using yield as an alternative metric. Our final submission for method extraction has 95% precision and yield of 1.5 methods per paper on the manually inspected subset of papers. Similarly to research area prediction, the main limiation here is the difficulty our model has finding new methods, as it is limited to the SAGE ontology and a few hand-crafted patterns. One potential way to alleviate this issue is to leverage external resources, as presented in Chapter 8.</p>
<h1 id="conclusion-2"><a href="#sec:conclusion">Conclusion</a></h1>
<p>This report summarizes the AI2 submission at the RCC competition. We identify dataset mentions by combining the predictions of an NER model and a rule-based system, use TF-IDF to identify candidates for a given mention, and use a gradient boosted trees classifier to predict a binary label for each candidate mention–dataset pair. To identify research fields of a paper, we train two multi-class classifiers, one for each of the top two levels in the MAG hierarchy for fields of study. Finally, to extract research methods, we use a rule-based system utilizing a dictionary and common patterns, followed by a scoring function which takes into account the prominence of a candidate in foreground and background corpora.</p>
<p>We now provide some possible directions of improvement for each component of our submission. For dataset extraction, the most promising avenue of improvement is to improve the NER model, and the most promising avenue to improve the NER model is to collect less noisy data. We effectively have distantly supervised training data for the NER model, and the first thing to try would be directly annotating papers with dataset mentions to provide a clearer signal for the NER model. As mentioned previously and discussed in Chapters 5 and 8, the dataset mentions provided are not located in the text, and are simply extracted strings. Given these strings, labels in the actual text can be created by searching for the provided string. However, this is a noisy process, as the string may occur multiple times in the document, and all occurrences may or may not be correct dataset mentions. This is especially problematic when the string is a common word (e.g. “time”). Therefore, directly annotating the strings in the full text (i.e. providing character offsets for the strings) would help to reduce the noise in the NER data. For research area prediction, it would help to include signals beyond just the paper title for predicting the field of study. The difficulty here is finding labeled training data that includes richer signals like abstract text and paper keywords. For method prediction, exploring the use of open information extraction is a potential avenue of future research. Additionally, it would be helpful to clarify what exactly is meant by a method, as it is currently unclear what a successful method extraction looks like. The main lesson learned is that, when presented with noisy, distantly supervised, real-world data, to produce a production-quality system, it becomes very important to (1) have a high-confidence evaluation dataset, and (2) look for other data sources that are similar enough to the task at hand to be useful. Taking steps towards both of these objectives are promising avenues of future work.</p>
<p>As discussed in Chapter 1 and throughout, the dataset extraction discussed in this report is intended to be part of a broader effort to create infrastructure and tools that will aid in the discovery and usage of datasets in social science research. This is critical to enable reproduciblity, collaboration, and effective use of data. We look forward to seeing what comes out of this project as a whole, and how AI techniques can be leveraged to have positive impact in the social sciences.</p>
<h1 id="acknowledgments-1"><a href="#sec:acknowledgements">Acknowledgments</a></h1>
<p>We would like to thank the competition organizers for their tireless efforts in preparing the data, answering all our questions, doing the evaluations, and providing feedback. We also would like to thank Zhihong (Iris) Shen for helping us use the MAG data.</p>
<h1 id="references-4"><a href="#sec:references">References</a></h1>
<p>Diederik P. Kingma and Jimmy Ba. 2014. Adam:A method for stochastic optimization.CoRR,abs/1412.6980.</p>
<p>Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning. 2014. Glove: Global vectors forword representation. InEMNLP.</p>
<p>Matthew E. Peters, Mark Neumann, Mohit Iyyer,Matt Gardner, Christopher Clark, Kenton Lee, andLuke S. Zettlemoyer. 2018. Deep contextualizedword representations. InNAACL 2018.</p>
<p>Zhihong Shen, Hao Ma, and Kuansan Wang. 2018.A web-scale system for scientific knowledge explo-ration. InACL</p>
<h1 id="footnotes"><a href="#sec:footnotes">Footnotes</a></h1>
<p>1: www.semanticscholar.org</p>
<p>2: https://github.com/allenai/allennlp/blob/master/allennlp/models/crf_tagger.py</p>
<p>3: https://xgboost.readthedocs.io/en/latest/</p>
<p>4: https://github.com/allenai/coleridge-rich-context-ai2</p>
<h1 id="appendix"><a href="#sec:appendix">Appendix</a></h1>
<p>The code for the submission can be found on GitHub<sup>4</sup>. There is a README with additional documentation at this github repo.</p>
<hr />
<h1 id="chapter-7---finding-datasets-in-publications-the-kaist-approach">Chapter 7 - Finding datasets in publications: The KAIST approach</h1>
<h1 id="non-technical-overview">Non-technical Overview</h1>
<p>The KAIST’s approach for retrieving datasets is to generate questions about datasets like <em>what is the dataset used in this publication?</em> and use a machine-learning system that can read a publication and a question, and give the answer to it. This machine-learning system retrieves a list of candidate answers among which one of them should be the name of the dataset. To remove those wrong candidate answers, they proposed to filter them out by their entity types. For example, if the entity type of a candidate answer is <em>organization</em>, it is likely to be a dataset name because datasets are created by organizations.</p>
<p>For research field retrieval, they proposed to compare publications with Wikipedia articles to discover the research fields. First, they crawled Wikipedia articles that correspond to the list of research fields. Then, they retrieved the research fields of the publications by measuring the similarity between the papers and the crawled Wikipedia documents. For example, they crawled the Wikipedia article <em>economic history</em> which corresponds to the research field <em>economic history</em>. If the similarity between a publication and the article <em>economic history</em> is high enough, it is determined that the publication belongs to the research field <em>economic history</em>. They proposed to use TF-IDF as similarity measure, which is based on term frequency and document frequency, but others could be applied too.</p>
<p>For the research methods retrieval, they modeled the task as a named-entity recognition problem. They considered research methods as named entities, real-world objects that can be denoted with a proper name, and trained a machine learning model to identify and retrieve them.</p>
<h1 id="literature-review">Literature Review</h1>
<p>Although <em>Information Retrieval</em> is a well-established research field, only a few attempts have focused on the task of dataset extraction form publications. <span class="citation" data-cites="ghavimi2016identifying">[@ghavimi2016identifying]</span> tackled this problem using heuristics and dictionaries but encountered several problems. Firstly, they gave too much weight to acronyms. For example, <em>NYPD (New York Police Department)</em> is detected as a dataset name. Secondly, they gave too much weight to the year of publication of the datasets because they assumed that dataset names are usually followed by the year of publication. However, this may only apply to Social Sciences publications. For example, Computer Science datasets do not appear followed by the publication year so this heuristic cannot detect all possible types of dataset mentions.</p>
<h1 id="what-did-you-do">What did you do</h1>
<p>In this section, a detailed explanation of the used models for dataset names, research field, and research methods retrieval is provided.</p>
<h2 id="datasets-retrieval">Datasets Retrieval</h2>
<p>The followed approach to retrieve dataset names is based on Machine Reading Question Answering (MRQA). First, given a publication, a list of candidate paragraphs in which the dataset is mentioned is selected. Then, using a query generation module, a specific query for each paragraph is created. After that, each pair of paragraph-query is input into the MRQA model. This model creates a list of candidate answers that is further processed using a feed-forward neural network. This network takes as input pairs of candidate answers and their entity types. The types of the answer candidates are obtained using an entity typing module. The output of this feed-forward neural network is the final list of dataset names found in the publication. Figure 7.1 shows an overview of the pipelined system. In the following subsections, the MRQA, query generation, and entity typing models are explained in detail.</p>
<figure>
<img src="combined_images/OverallModel.png" alt="image" /><figcaption>image</figcaption>
</figure>
<p><em>Figure 7.1: Overall architecture for dataset retrieval</em></p>
<h3 id="document-qa">Document QA</h3>
<p>MRQA models are neural networks that find answers for given queries according to a text. These answers must appear explicitly in the text. Since the dataset retrieval task is about finding explicit dataset mentions from publications, MRQA models are suitable for this task.</p>
<p>The MRQA model used in this work is Document QA <span class="citation" data-cites="clark2017simple">[@clark2017simple]</span>. It uses Bi-GRU, bi-attention, and self-attention mechanism. In addition, Document QA performs a paragraph selection that pre-filters and selects the <em>k</em> most relevant paragraphs using TF-IDF similarity between the query and paragraphs of the text. The model was trained on SQuAD v1.1, a common dataset to train MRQA models <span class="citation" data-cites="rajpurkar2016squad">[@rajpurkar2016squad]</span>. For details about the implementation and computing resources to train it, they refer to the original publication.</p>
<p>The KAIST team had the hypothesis that MRQA models do not need the full publication to find datasets. Rather, the MRQA models only need to process the paragraph where the answer appears. Among the literature of MRQA, the model used in this work stands out because of its paragraph selection stage. Using this model, it is possible to select a list of candidate paragraphs where the answer may appear, and then use the MRQA model to process them and retrieve the datasets.</p>
<h3 id="query-generation-module">Query Generation Module</h3>
<p>Queries that are suitable for finding datasets are required to utilize an MRQA model for the dataset retrieval task. However, defining a general query for retrieving datasets is not trivial, since the dataset mentions appear in various forms like surveys, datasets, or studies. Therefore, they devised a query generation module to generate multiple specific queries instead of a single general query.</p>
<p>To create a list of important query terms that the queries should include, they used a query generation model proposed by <span class="citation" data-cites="yuan2017machine">[@yuan2017machine]</span> that creates a query given a text and an answer. All the queries generated by this model are too specific and cannot be used for other publications or other dataset names. However, they can be utilized to create a list of query terms to generate more specific queries for other publications. To create this list, they extracted query terms that are frequent in the list of queries and at the same time are not frequent in sentences that do not include a mention to a dataset. Because of this, these query terms have the discriminative power to retrieve dataset mentions since 1) queries are generated to extract mentions and 2) the query terms do not appear in the sentences without dataset mentions. This list is used two times in their pipelined system. First, concatenating the first <em>k</em> query terms a general query is built. This query is employed by the paragraph selection stage of Document QA, as shown in Figure 7.1. Then, the query generation module generates specific queries for each paragraph concatenating the query terms that appear in the paragraph and on the list.</p>
<h3 id="entity-typing-model">Entity Typing Model</h3>
<p>Ultra-Fine Entity Typing <span class="citation" data-cites="Choi:2018:ACL">[@Choi:2018:ACL]</span> can predict a set of free-from phrases like <em>criminal</em> or <em>skyscraper</em> given a sentence with an entity mention. For example, in the sentence: <em>Bob robbed John and he was arrested shortly afterward</em>, Bob is of type <em>criminal</em>. In the task of the present book, candidate answers proposed by the MRQA model and the sentence in which they appear are input into Ultra-Fine Entity Typing. This system can predict 10k different entity types among which <em>dataset</em> is included. However, after a few experiments, they observed that most of the entity types obtained from the dataset names are not <em>dataset</em> but <em>organization</em>, <em>agency</em>, and similar types. This is due to the fact that datasets are usually created by organizations and thus, they include the name of the organization in the name of the dataset. Since these entity types are consistent, it is possible to use them as a feature for their candidate answer classifier. In this work, the KAIST team used the pre-trained model that was released with the original publication. For details about the implementation and computing resources to train it, they refer to the original publication.</p>
<h3 id="candidate-answer-classifier">Candidate Answer Classifier</h3>
<p>Using the score given by the MRQA model for each candidate answer and the entity types given by the Entity Typing model for each candidate answer, a neural network classifier that filters the wrong candidate answers provided by the MRQA model is used. The intuition of this classifier is that a candidate answer with a high score given by the MRQA model and whose entity type is <em>organization</em> or something similar is highly likely to be a correct dataset name. Due to this pattern, they were able to create a neural network classifier to filter out candidate answers.</p>
<p>The classifier has the following architecture:</p>
<ol type="1">
<li><p>Input size: 10332 (10331 labels from Ultra-Fine Entity Typing and one from the Document QA score)</p></li>
<li><p>1 hidden layer with 50 neurons</p></li>
<li><p>Output size: 2</p></li>
</ol>
<p>The training set consists of 25172 examples and the test set of 6293 examples obtained from the training set provided by the competition. To train the network, Adam optimizer was used with cross entropy as the loss function.</p>
<h2 id="research-fields-retrieval">Research Fields Retrieval</h2>
<p>Their approach to obtain research fields is based on comparing the publications with Wikipedia articles using TF-IDF similarity. First, using the list of research fields provided by the competition, a set of Wikipedia articles about different research fields was obtained using the Python library MediaWiki. The list provided by the competition has three levels of hierarchy as shown in the example (Figure 7.2). The leaf nodes of that hierarchy were searched in Wikipedia to retrieve specific research fields instead of general ones. For example, they were aiming to retrieve <em>Neurosurgery</em> instead of <em>Medicine</em>. Then, using Scikit-learn <span class="citation" data-cites="scikit-learn">[@scikit-learn]</span>, a TF-IDF matrix of all the publications and Wikipedia articles of research fields were computed. The research field and all its superior nodes in the hierarchy associated with the Wikipedia article most similar to the publication were returned along with the similarity in the range [0,1]. The overall architecture can be seen in Figure 7.3.</p>
<p><img src="combined_images/fieldshierarchy2.png" alt="Research fields hierarchy" /><em>Figure 7.2: Research fields hierarchy</em></p>
<p><img src="combined_images/researchfields2.png" alt="Overall architecture for research fields retrieval" /><em>Figure 7.3: Overall architecture for research fields retrieval</em></p>
<h2 id="research-methods-retrieval">Research Methods Retrieval</h2>
<p>For the research methods retrieval task, they modeled it as a named-entity recognition (NER) problem. Research methods are considered to be named entities and because of this, they can be tagged as research method label (RS) instead of common NER labels such as <em>location</em>, and <em>people</em>. Figure 7.4 shows the main architecture of the model proposed by <span class="citation" data-cites="lample2016neural">[@lample2016neural]</span> and used in this task.</p>
<p><img src="combined_images/bi-lstm.png" alt="BiLSTM-CRF architecture" /><em>Figure 7.4: Paragraph selection for DocQA in research method retrieval</em></p>
<p>The representation of a word using the model is obtained considering its context. The KAIST team had the assumption that research methods have dependencies and constraints with words that appear in their surrounding context. Therefore, the conditional random field <span class="citation" data-cites="lafferty2001conditional">[@lafferty2001conditional]</span> layer in this model is suitable for detecting research methods by jointly tagging the whole sentence, instead of independently tagging each word.</p>
<p>For this task, the research method phrases that appeared in the training set were marked using as reference the list of research methods provided by the competition. Then, the training set was represented in CoNLL 2003 format <span class="citation" data-cites="tjong2003introduction">[@tjong2003introduction]</span>, using IOB tag (Inside, Outside, Beginning) to train the model. Every token was labeled as B-RS if the token is the beginning of a research method, I-RS if it is inside a research method but not the first token, or O otherwise. Training the model took approximately one day using a CPU AMD Ryzen 7 2700, a GPU Nvidia GeForce GTX 1050 Ti, and 8GB RAM.</p>
<h1 id="what-worked-and-what-didnt">What worked and what didn’t</h1>
<p>The KAIST team tried different ideas to extract dataset names. First, they tried to extract the dataset names using hand-crafted queries in the MRQA model. However, they noticed that these manually generated queries do not have sufficient discriminative power. Therefore, they tried to generate a general query with enough discriminative power to retrieve datasets’ names. To this end, they converted the sentences containing the dataset into queries and then clustered the converted queries to get general queries. However, they found that each of the resulting clusters did not reflect the semantics of the desired general queries. Hence, they had to create specific queries for each publication as explained in the previous section. These specific queries helped to increase the recall but at the same time affected negatively to the precision. The use of entity typing worked well to remove the wrong candidate answers proposed by the MRQA model. Thanks to this entity-type filtering, they were able to improve the recall using the query generation module without sacrificing the precision.</p>
<p>They also tried to use the section names as a feature for the paragraph selection module of Document QA. However, the use of section names degraded the overall performance. In their analysis, they stated that the heuristics that they used to extract them generated noise that affected the performance. For example, <em>total</em>, <em>variable</em>, and <em>funding</em> were detected as section names but clearly, they are not.</p>
<p>Finally, their idea to compare publications with Wikipedia articles to retrieve research fields yielded successful results. But on the other hand, their first idea to retrieve research methods was not successful. It was based on identifying the context words of the research methods by using the frequency of those words. The reason for the bad performance was due to the lack of discriminative power of the most common words that co-occur with the research methods. Therefore, they tried to model it as a NER problem, where they consider each research method that appeared in a publication as a named-entity. By modeling the problem in this way, they could use existing NER models to extract research methods from publications. However, this approach did not achieve satisfactory results either.</p>
<h1 id="summary-of-your-results-and-caveats">Summary of your results and caveats</h1>
<p>Due to the difficulty of performing a quantitative analysis on a not extensively labeled dataset, a qualitative analysis was made. Several random publications were chosen and manually labeled to check the quality of the model and discover the strong and weak points.</p>
<h2 id="datasets-retrieval-1">Datasets Retrieval</h2>
<p>To analyze the effects of the query generation module and entity typing module, they performed analyses on 100 publications from the dev set of the first phase using three different settings:</p>
<ol type="1">
<li><p>Document QA only</p></li>
<li><p>Document QA + Query Generation Module</p></li>
<li><p>Document QA + Query Generation Module + Entity Typing Module</p></li>
</ol>
<h3 id="document-qa-only">Document QA only</h3>
<p>Figure 7.5 shows the results from three publications from the list of analyzed publications using <em>Document QA only</em>. Compared to the other settings, <em>Document QA only</em> setting retrieves high-quality answers (dataset mentions). However, the number of retrieved answers is notably small. For example, the result from <em>153.txt</em> publication is empty as shown in Figure 7.5. In fact, using this setting the model can only retrieve 260 answers (predictions) from the list of analyzed publications.</p>
<p><img src="combined_images/OnlyDocQA.png" alt="Results from Document QA only" /><em>Figure 7.5: Results from Document QA only. Right answers from the model in blue.</em></p>
<p>These results were expected due to the difficulty of defining general queries as explained in section <a href="#query-generation-module"><em>Question Generation Module</em></a>. Without a query generation module, it is hard to make a representative enough query to retrieve various forms and types of the dataset mentions.</p>
<h3 id="document-qa-query-generation-module">Document QA + Query Generation Module</h3>
<p>Figure 7.6 shows the results from three publications from the list of analyzed datasets using Document QA and the Query Generation Module. Because of the addition of the Query Generation Module, a larger number of answers were retrieved. For example, the result from <em>153.txt</em> publication contains several answers including the right one, <em>Micro Database Direct Investment</em>. Therefore, this and the retrieval of more than 2,000 answers from the list of analyzed datasets proves that the Query Generation Module improves recall of the entire dataset retrieval model. On the other hand, compared to the <em>Document QA only</em> setting, there is a considerable amount of wrong candidate answers. For instance, in Figure 7.6, <em>empirical</em>, <em>Table 1</em>, and <em>Section 4</em> are not dataset mentions.</p>
<p><img src="combined_images/DocQA_QG.png" alt="Results from Document QA + Query Generation Module" /><em>Figure 7.6: Results from Document QA + query generation module. Right answers from the model in blue.</em></p>
<p>They believe that the reason for this noise is that some query terms may cause the retrieval of wrong answers. For example, the query term <em>study</em> can help to retrieve dataset mentions such as <em>ANES 1952 Time Series Study</em>. However, this term can also retrieve wrong answers such as <em>empirical study</em>. These types of query terms are still needed to retrieve various forms and types of dataset mentions but clearly generate noise.</p>
<h3 id="document-qa-query-generation-module-entity-typing-module">Document QA + Query Generation Module + Entity Typing Module</h3>
<p>Figure 7.7 shows the results of the analyzed publications using Document QA, the Query Generation Module, and the Entity Typing Module. Although the Entity Typing Module might not remove all the wrong answers caused by the Query Generation Module such as <em>4 2.1 Micro Data</em>, most of them are successfully removed and thus, the overall precision is improved. Using this setting the model retrieves 526 answers (predictions) from 100 publications from the dev set of the first phase of the competition.</p>
<p><img src="combined_images/FullModel.png" alt="Results from Document QA + Query Generation Module + Entity Typing Module" /><em>Figure 7.7: Results from Document QA + query generation module + entity typing module. Right answers from the model in blue.</em></p>
<h2 id="research-fields-retrieval-1">Research Fields Retrieval</h2>
<p>They randomly selected 20 publications from the training set of the first phase. They can test their model using the training set because the model does not require any training phase. The model was able to predict research fields correctly for 11 of those publications. The strongest point is that the model is able to predict research fields that are significantly specific such as <em>Home health nursing management</em>. Among the weak points of the model, it has problems when two research fields are similar or share subtopics. Moreover, sometimes it fails due to the fact that it tries to retrieve excessively specific fields while more general ones would be suitable.</p>
<h2 id="research-methods-retrieval-1">Research Methods Retrieval</h2>
<p>20 random publications were selected and labeled from the training set of the second phase and the results are not satisfactory. The model is able to find proper research methods for 12 publications out of 20. For example, the model successfully detects the research method of the publication with id 15359, <em>Factor analysis</em>. However, the results contain a significant amount of noise. For example, the model retrieves for the document with id 10751 several wrong answers like <em>Reviews describe</em>, <em>Composite materials</em>, and <em>Detailed databases</em>.</p>
<h1 id="lessons-learned-and-what-would-you-do-differently">Lessons learned and what would you do differently</h1>
<p>After the completion of this project, the KAIST team realized that some steps could have been in a different way and would have led to better results. For example, they focused a lot on the model creation, but they could have spent more time on the analysis of the dataset to extract all its potential and search for additional datasets to alleviate the noise of the provided dataset.</p>
<p>In addition, since Document QA is good for prototyping, it was a good idea to use it at the beginning to check that their hypothesis of modeling dataset retrieval as a Question Answering task was right. However, at some point during the project, they should have changed it to another model with state-of-the-art performance. Also, they use symbolic queries for the MRQA model. But since they are generating specific queries for each publication, it should be possible to define and generate queries in the form of embeddings. This could help to improve even more the recall boost provided by the Query Generation Module and at the same time avoid the generation of noise. Furthermore, for research fields, other ranking methods should have been tried like BM25, a ranking function used by search engines whose performance is better than TF-IDF. Finally, for research methods, they should have analyzed more the dataset to use more suitable techniques such as unsupervised NER instead of supervised NER.</p>
<h1 id="conclusion-3">Conclusion</h1>
<p>The KAIST team proposed to model the dataset retrieval task as a Question Answering task. This is a unique approach in this competition and led to successful results as shown in the analysis. This approach is flexible because it allows the retrieval of new datasets that are not in the training set. In addition, the model does not require to be trained on a dataset discovery task. They achieved good results even though they used a pre-trained model on SQuAD, a dataset for Question Answering using Wikipedia pages. Their approach to retrieve research fields is simple, fast to compute, and powerful. It can retrieve specific research fields with high precision. On the other hand, the proposed approach to retrieve research methods did not achieve as good results as the other task. The main problem was that they did not tackle the noise problem in the dataset.</p>
<h1 id="what-comes-next">What comes next</h1>
<p>This work is the very first step of the Coleridge Initiative to build an “Amazon.com” for data users and data producers. The next step is to construct a system that recommends datasets to researchers. The KAIST team has the hypothesis that datasets depend on research fields and vice versa. For example, in the research field <em>Question Answering</em>, a subfield of <em>Natural Language Processing</em> and <em>Computer Science</em>, the most commonly used dataset is SQuAD <span class="citation" data-cites="rajpurkar2016squad">[@rajpurkar2016squad]</span>. Therefore, according to their hypothesis, two publications using SQuAD are presumably to be in the same field, <em>Question Answering</em>. Based on this hypothesis, it would be possible to build hierarchical clusters of publications with the same research field. In this way, a cluster will have publications with the same research field and similar datasets. As an example, the QA cluster will have papers about QA and those papers will use similar datasets like SQuAD and TriviaQA <span class="citation" data-cites="joshi2017triviaqa">[@joshi2017triviaqa]</span>. With these clusters, the system will be able to recommend datasets to data users. For example, if a publication is in the <em>Question Answering</em> field, the proposed system would be able to recommend the authors SQuAD and TriviaQA. Moreover, it would be able to recommend to data producers fields with a lack of datasets.</p>
<p>In addition, there is room for improvement in the models they proposed. For example, since they used a pre-trained model in Document QA, they think they did not exploit the whole potential of this system, so they would like to train the model using a big enough training set of publications.</p>
<h1 id="acknowledgments-2">Acknowledgments</h1>
<p>This work was supported by the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korean Government (MSIT) (No. 2013-0-00179, Development of Core Technology for Context-aware Deep-Symbolic Hybrid Learning and Construction of Language Resources) and Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT (2017M3C4A7065962).</p>
<h1 id="appendix-description-of-the-code-and-documentation">Appendix: Description of the code and documentation</h1>
<p>The technical documentation of the code is provided in the GitHub repository of the project https://github.com/HaritzPuerto/RCC/tree/master/project</p>
<hr />
<h1 id="chapter-8---knowledge-extraction-from-scholarly-publications-the-gesis-contribution-to-the-rich-context-competition">Chapter 8 - Knowledge Extraction from scholarly publications: The GESIS contribution to the Rich Context Competition</h1>
<h1 id="knowledge-extraction-from-scholarly-publications---the-gesis-contribution-to-the-rich-context-competition">Knowledge Extraction from scholarly publications - The GESIS contribution to the Rich Context Competition</h1>
<p><strong>Authors:</strong> <em>Wolfgang Otto, Andrea Zielinski, Behnam Ghavimi, Dimitar Dimitrov, Narges Tavakolpoursaleh, Karam Abdulahhad, Katarina Boland, Stefan Dietze</em></p>
<p><strong>Affiliation:</strong> <em>GESIS – Leibniz Institute for the Social Sciences, Cologne, Germany</em></p>
<p><strong>Corresponding author:</strong> <em>wolfgang.otto@gesis.org</em></p>
<h2 id="introduction-6">1. Introduction</h2>
<p>GESIS - the Leibniz Institute for the Social Sciences (GESIS)[1] is the largest European research and infrastructure provider for the social sciences and offers research data, services and infrastructures supporting all stages of the scientific process. The GESIS department <em>Knowledge Technologies for the Social Sciences (WTS)</em>[2] is responsible for developing all digital services and research data infrastructures at GESIS and aims at providing integrated access to social sciences data and services. Next to traditional social sciences research data, such as surveys and census data, an emerging focus is to build data infrastructures able to exploit novel forms of social sciences research data, such as large Web crawls and archives.</p>
<p>Research at WTS[3] addresses areas such as Information Retrieval (IR), Information Extraction (IE) &amp; Natural Language Processing (NLP), semantic technologies and human computer interaction and aims at ensuring access and use of social sciences research data along the FAIR principles, for instance, through interlinking of research data, established vocabularies and knowledge graphs and by facilitating semantic search across distinct platforms and datasets. Due to the increasing importance of Web- and W3C standards as well as Web-based research data platforms, in addition to traditional research data portals, findability and interoperability of research data across the Web constitutes one current challenge. In the context of Web-scale reuse of social sciences resources, the extraction of structured data about scholarly entities such as datasets and methods from unstructured and semi-structured text, as found in scientific publications or resource metadata, is crucial in order to be able to uniquely identify social sciences resources and to understand their inherent relations.</p>
<p>Prior works at WTS/GESIS addressing such challenges apply NLP and machine learning techniques to, for instance, extract and disambiguate mentions of datasets[4] (Boland et al., 2012; Ghavimi et al., 2016)), authors (Backes, 2018a, 2018b) or software tools (Boland and Krüger, 2019) from scientific publications or to extract and fuse scholarly data from large-scale Web crawls (Sahoo et al., 2017; Yu et al., 2019). Resulting pipelines and data are used to empower scholarly search engines such as the <em>GESIS-wide search</em>[5] (Hienert et al., 2019) which provides federated search for scholarly resources (datasets, publications etc.) across a range of GESIS information systems or the <em>GESIS DataSearch</em> platform[6] (Krämer et al., 2018), which enables search across a vast number of social sciences research datasets mined from the Web.</p>
<p>Given the strong overlap of our research and development profile with the recent initiatives of the Coleridge Initiative to evolve this research field through the Rich Context Competition (RCC)[7], we are enthusiastic about having participated in the RCC2018 and are looking forward to continue this collaboration towards providing sound frameworks and tools which automate the process of interlinking and retrieving scientific resources.</p>
<p>The central tasks in the RCC are the extraction and disambiguation of mentions of datasets and research methods as well as the classification of scholarly articles into a discrete set of research fields. After the first phase, each team received feedback from the organizers of the RCC consisting of a quantitative and qualitative evaluation. Whereas quantitative results of our inital contribution throughout phase one have shown significant room for improvement, the qualitative assessement, conducted by four judges on a sample of ten documents, underlined the potential of our approach.</p>
<p>Since we have been shortlisted for the second phase of the RCC, this chapter describes our approaches, techniques, and additional data used to address all three tasks. As described in the following subsections, we decided to follow a module-based approach where each module or the entire pipeline can be reused. The remaining chapter is organised as follows. The following Section 2 provides an overview of our approach, used background data and preprocessing steps, whereas Sections 3, 4, and 5 describe our approaches in more detail, including results towards each of the tasks. Finally, we discuss our results in Section 6 and provide an overview of future work in Section 7.</p>
<h2 id="approach-data-and-pre-processing">2. Approach, data and pre-processing</h2>
<p>This section describes the external data sources we used as well as our pre-processing steps.</p>
<h3 id="approach-overview-and-initial-evaluation-feedback">2.1 Approach overview and initial evaluation feedback</h3>
<p>The central tasks in the RCC are the extraction of dataset mentions from text. Even so, we considered the discovery of research methods and research fields important. To this end, we decided to follow a module-based approach. Users could choose to use each specific module solely or as parts of a data processing pipeline. Figure 8.2 shows an overview of modules developed and their dependencies. Here, the upper three modules (which are in gray) describe the pre-processing steps (cf. Section 2.3). The lower four modules (blue) are used to generate the output in a predefined format as specified by the competition.</p>
<p><img src="combined_images/information-flow.png" alt="image" /> Figure 8.1: An overview of the individual software modules described in this document and their dependencies. Gray: Our pre-processing pipeline. Blue: three main tasks of the RCC.</p>
<p>The pre-processing step consists of extracting metadata and raw text from PDF documents. The output of this step is then used by the software modules responsible for tackling the individual sub-tasks. These sub-tasks are to discover research datasets (cf. Section 3), methods (cf. Section 4) and fields (cf. Section 5). First, a Named Entity Recognition module is used to find dataset mentions. This module used a supervised approach trained on a weakly labled corpus. In the next step, we combine all recognized mentions for each publication and compare these mentions to the metadata from the list of datasets given by the competition. For this linking step the mentions and year information located in the same sentence are used. The corresponding sentence and extracted information are saved for debugging and potential usage in future pipeline components. The task of identifying research methods is solved unsing a Named Entity Recognition and Linking module with incorporated word embeddings and lexical resources. For identifying research fields, we trained a classifier on openly available abstracts and metadata from the domain of social sciences crawled from the Social Science Open Access Repository[8] (SSOAR). We tried different classifiers and selected the best performing one, a classifier based on fasttext[9], i.e. a neural net based approach with a high performance(Joulin et al., 2017).</p>
<p>After the first phase, each team received feedback from the organizers of the RCC. The feedback is two folds, a quantitative and qualitative evaluation. Unfortunately, the quantitative assessment showed our algorithm for dataset mention retrieval did not perform well regarding precision and recall metrics. In contrast to this, our approach has been found convincing regarding the quality of results. The qualitative feedback is based on a random sample of ten documents given to four judges. The judges were asked to manually extract dataset mentions. After this the overlap between their dataset extractions and the output of our algorithm was calculated. Other factors that judges took into consideration are specificity, uniqueness, and multiple occurrences of dataset mentions. As for the extraction of research methods and fields, no ground truth has been provided; these tasks were evaluated against the judges’ expert knowledge. Similarly to the extraction of dataset mentions, specificity and uniqueness have been considered for these two tasks. The feedback our team received was overall positive.</p>
<h3 id="external-data-sources">2.2 External data sources</h3>
<p>For developing our algorithms, we utilized two external data sources. For the discovery of research methods and fields, we resort to data from the Social Science Open Access Repository[10] (SSOAR). GESIS – Leibniz Institute for the Social Sciences maintains SSOAR by collecting and archiving literature of relevance to the social sciences.</p>
<p>In SSOAR, full texts are indexed using controlled social science vocabulary (Thesaurus[11], Classification[12]) and are assigned rich metadata. SSOAR offers documents in various languages. The corpus of English language publications that can be used for purposes of the competition consists of a total of 13,175 documents. All SSOAR documents can be accessed through the OAI-PMH[13] interface.</p>
<p>Another external source we have used to discover research methods is the ACL Anthology Reference Corpus (Bird et al., 2008). ACL ARC is a corpus of scholarly publications about computational linguistics. The corpus consists of a total of 22,878 articles.</p>
<h3 id="pre-processing">2.3 Pre-processing</h3>
<p>Although the organizers of the RCC offered plain texts for the publication, we decided to build our own pre-processing pipeline. The extraction of text from PDF files is still an error prone process. To handle de-hyphenation and paragraph segmentation during extraction time and benefit from automatic metadata extraction (i.e. title, author, abstracts and references) we decided to use a third party extraction tool. The Cermine Extraction Tool[14](Tkaczyk et al., 2015) transforms the files into XML documents using the Journal Article Tag Suite[15](Jats). For the competition we identified two interesting elements of the Jats XML format, i.e., (&lt;)front(&gt;) and (&lt;)body(&gt;). The (&lt;)front(&gt;) element contains the metadata of the publication, whereas the (&lt;)body(&gt;) contains the main textual and graphic content of the publication. As a last step of the pre-processing, we removed all linebreaks from the publication. The output of this step is a list of metadata fields and values, as shown in Table 8.1 for each publication paragraph.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Example Text Field Data</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">publication_id</td>
<td style="text-align: left;">12744</td>
</tr>
<tr class="even">
<td style="text-align: left;">label</td>
<td style="text-align: left;">paragraph_text</td>
</tr>
<tr class="odd">
<td style="text-align: left;">text</td>
<td style="text-align: left;">A careful reading of text, word</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">for word, was …</td>
</tr>
<tr class="odd">
<td style="text-align: left;">section_title</td>
<td style="text-align: left;">Data Analysis</td>
</tr>
<tr class="even">
<td style="text-align: left;">annotations</td>
<td style="text-align: left;">[{’start’: 270, ’end’: 295,</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">’type’: ’bibref’, …</td>
</tr>
<tr class="even">
<td style="text-align: left;">section_nr</td>
<td style="text-align: left;">[3, 2]</td>
</tr>
<tr class="odd">
<td style="text-align: left;">text_field_nr</td>
<td style="text-align: left;">31</td>
</tr>
<tr class="even">
<td style="text-align: left;">para_in_section</td>
<td style="text-align: left;">1</td>
</tr>
</tbody>
</table>
<p>Table 8.1: Example preprocessing output for a paragraph in a given publication.</p>
<h2 id="dataset-extraction">3. Dataset extraction</h2>
<h3 id="task-description">3.1 Task description</h3>
<p>In the scientific literature, datasets are cited to reference, for example, the data on which an analysis is performed or on which a particular result or claim is based. In this competition, we focus on (i) extracting and (ii) disambiguating dataset mentions from social science publications to a list of given dataset references. Identifying dataset mentions in literature is a challenging problem due to the huge number of styles of citing datasets. Although there are proposed standards for dataset citation in full-texts, researchers still ignore or neglect such standards (see, e.g., (Altman and King, 2007)). Furthermore, in many research publications, a correct citation of datasets is often missing (Boland et al., 2012). The following two sentences exemplify the problem of the usage of an abbreviation to make a reference to an existing dataset. The first example illustrates the use of abbreviations that are known mainly in the author’s research domain. The latter illustrates the ambiguity of abbreviations. In this case, <em>WHO</em> identifies a dataset published by the World Health Organization and does not refer to the institution itself. <strong>Example 1</strong>: <em>P-values are reported for the one-tail paired t-test on </em>Allbus* (dataset mention) and <em>ISSP</em> (dataset mention).<em> <strong>Example 2</strong>: </em>We used <em>WHO data from 2001</em> (dataset mention) to estimate the spreading degree of AIDS in Uganda.* We treat the problem of detecting dataset mentions in full-text as a Named Entity Recognition (NER) task.</p>
<h4 id="formal-problem-definition">Formal problem definition</h4>
<p>Let (D) denote a set of existing datasets (d) and the knowledge base (K) as a set of known dataset references (k). Furthermore, each element of (K) is referencing an existing dataset (d). The Named Entity Recognition and Linking task is defined as (i) the identification of dataset mentions (m) in a sentence, where (m) references a dataset (d) and (ii) linking them, when possible, to one element in (K) (i.e., the reference dataset list given by the RCC).</p>
<h3 id="challenges">3.2 Challenges</h3>
<p>We focus on the extraction of dataset mentions in the body of the full-text of scientific publications. There are three types of dataset mentions: (i) The full name of a dataset (”National Health and Nutrition Examination Survey“), (ii) an abbreviation (”NHaNES“) or (iii) a vague reference, e.g.,”the monthly statistic“. With all these these types, the NER task faces special challenges. In the first case, the used dataset name can vary in different publications. For instance one publication cites the dataset with”National Health and Nutrition Examination Survey“ the other could use the words ”Health and Nutrition Survey“. In the case where abbreviations are used, a disambiguation problem occurs, e.g., in”WHO data“. WHO may describe the World Health Organization or the White House Office. In the case, that an abbreviation is used after the dataset name has been written in full, the mapping between these different spellings in one text is referred to as Coreference Resolution. The biggest challenge is again the lack of annotated training data. In the following we describe how we have dealt with this lack of ground truth data.</p>
<h3 id="phase-one-approach">3.3 Phase one approach</h3>
<p>Missing ground truth data is the main problem to handle during this competition. To this end, supervised learning methods for dataset mentions extraction from texts are not applicable without the identification of external training data or the creation of useful labeled training data from information given by the competition. Because of the lack of existing training data for the task of dataset mention extraction we resort to the provided list of dataset mentions and publication pairs and re-annotate the particular sentences in the publication text. A list of dataset identifying words is provided for some of the known links between publications and datasets by the competition. These words represent the evidence of the linkage between publication and datasets and are extracted from the publication text. In the course of re-annotation, we search for each of the identifying words in the corresponding publication texts. For each match, we annotate the occurence in our raw text and use these annotations as ground truth. As described in the pre-processing section, our units for processing the publication text are paragraphs. The re-annotated corpus consists of a list of paragraphs for each publication with stand-off annotations identifying the mentions of datasets (i.e. position of the start and end characters and the entity type for each mention: <em>dataset</em>). This re-annotation is then used to train Spacy’s neural network-based NER model[16]. We created a holdout set of 1,000 publications and a training set of size 4,000. Afterwards, we train our model with the paragraphs as a sampling unit. In the training set, 0.45 percent of the paragraphs contained mentions. For each positive training example, we have added one negative sample that contains no known dataset mentions and is randomly selected. We used a batch size of 25 and a dropout rate of 0.4. The model was trained for 300 iterations.</p>
<h4 id="evaluation-1">Evaluation</h4>
<p>We evaluated our model with respect to four metrics: precision and recall, each for strict and for partial match. While the strict match metrics are standard evaluation metrics, the partial match metrics are their relaxed variants in which the degree to which dataset mentions have to match can vary. Consider the following partial match example: “National Health and Nutrition Examination Survey” is the extracted dataset mention, while “National Health and Nutrition Examination Survey (NHANES)” is the true dataset mention. In contrast to the strict version of the metrics, this overlapping match is considered a match for the partial version. The scores describe whether a model is able to find the correct positions of dataset mentions in the texts, even if the start and end positions of the characters are not the same, but the ranges overlap.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Metric</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Precision (partial match)</td>
<td style="text-align: center;">0.93</td>
</tr>
<tr class="even">
<td style="text-align: left;">Recall (strict match)</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Precision (strict match)</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr class="even">
<td style="text-align: left;">Recall (strict match)</td>
<td style="text-align: center;">0.81</td>
</tr>
</tbody>
</table>
<p>Table 8.2: Performance of phase one approach of dataset extraction.</p>
<p>Table 8.2 shows the results of the dataset mention extraction on the holdout set. The model can achieve high strict precision and recall values. As expected, the results are even better for the partial version of the metrics. It means that even if we couldn’t match the dataset mention in a text exactly, we can find the right context with very high precision.</p>
<h3 id="phase-two-approach">3.4 Phase two approach</h3>
<p>In the second phase of the competition, additional 5,000 publications were provided by RCC. We extended our approach to consider the list with dataset names supplied by the organizers and re-annotated the complete corpus of 15,000 publications in the same manner as in phase one to obtain training data. This time we split the data in 80% for training and 20% for test.</p>
<h4 id="evaluation-2">Evaluation</h4>
<p>We resort to the same evaluation metrics as in phase one. However, we calculate precision and recall on the full-text of the publication and not on the paragraphs as in the first phase. Table 8.3 shows the results achieved by our model. We observe lower precision and recall values. Compared to phase one, there is also a smaller difference between the precision and recall values for the strict and partial version of the metrics.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Metric</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Precision (partial match)</td>
<td style="text-align: center;">0.51</td>
</tr>
<tr class="even">
<td style="text-align: left;">Recall (partial match)</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Precision (strict match)</td>
<td style="text-align: center;">0.49</td>
</tr>
<tr class="even">
<td style="text-align: left;">Recall (strict match)</td>
<td style="text-align: center;">0.87</td>
</tr>
</tbody>
</table>
<p>Table 8.3: Performance of phase two approach for dataset extraction.</p>
<h2 id="research-method-extraction-2">4. Research method extraction</h2>
<h3 id="task-description-1">4.1 Task description</h3>
<p>Inspired by a recent work of Nasar et al. (Nasar et al., 2018), we define a list of basic entity types that give key-insights into scholarly publications. We adapted the list of semantic entity types to the domain of the social sciences with a focus on <em>research methods</em>, but also including related entity types such as <em>theory, todel, measurement, tool, performance</em>. We suspect that the division into semantic types might be helpful to find <em>research methods</em>. The reason is that the related semantic entities types might provide clues or might be directly related to the research method itself.</p>
<p>For example, in order to achieve a certain research goal, an experiment is used in which a certain combination of <em>methods</em> is applied to a <em>dataset</em>. The methods can be specified as concepts or indirectly through the use of certain <em>software</em>. The result is then quantified with a <em>performance</em> using a specific measure.</p>
<p><strong>Example</strong>: <em>P-values</em> (measurement) are reported for the <em>one-tail paired t-test</em> (method) on <em>Allbus</em> (dataset) and <em>ISSP</em> (dataset). We selected the entity types <em>research method</em>, <em>research theory</em>, <em>research tool</em> and <em>research measurement</em> as the target research method related entity types (see Table 8.4). This decision is based on an ecxamination of the SAGE ontology given by the RCC as a sample of how research method terms might look like.</p>
<table>
<colgroup>
<col style="width: 28%" />
<col style="width: 18%" />
<col style="width: 52%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Entity type</strong></td>
<td style="text-align: left;"><strong>Corresponding</strong></td>
<td style="text-align: left;"><strong>Examples</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong>SAGE type</strong></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Research Method</td>
<td style="text-align: left;">SAGE-METHOD</td>
<td style="text-align: left;">Bootstrapping, Active Interviews</td>
</tr>
<tr class="even">
<td style="text-align: left;">Research Measurement</td>
<td style="text-align: left;">SAGE-MEASURE</td>
<td style="text-align: left;">Latent Variables, Phi coefficient, Z-score</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Research Theory</td>
<td style="text-align: left;">SAGE-THEORY</td>
<td style="text-align: left;">Frankfurt school, Feminism, Actor network theory</td>
</tr>
<tr class="even">
<td style="text-align: left;">Research Tool</td>
<td style="text-align: left;">SAGE-TOOL</td>
<td style="text-align: left;">SPSS, R statistical package</td>
</tr>
</tbody>
</table>
<p>Table 8.4: Entity types of relevance for the research method extraction task.</p>
<h4 id="formal-problem-definition-1">Formal problem definition</h4>
<p>The task of Named Entity Recognition and Linking is to (i) identify the mentions (m) of research-related entities in a sentence and (ii) link them, if possible, to a reference knowledge base (K) (i.e. the SAGE Thesaurus[17]) or (iii) assign a type to each entity, e.g. a <em>research method</em>, selected from a set of predefined types.</p>
<h3 id="challenges-1">4.2 Challenges</h3>
<p>There are some major challenges that any named entity recognition, classification and linking system needs to handle. First, regarding NER, identifying the entities boundary is important, thus detecting the exact sequence span. Second, ambiguity errors might arise in classification. For instance,‘range’ might be a domain-specific term from the knowledge base or belong to the general domain vocabulary. This is a challenging task for which context information is required. In the literature, this relates to the problem of <strong>domain adaptation</strong> which includes fine-tuning to specific named entity classes[18]. With respect to entity linking, another challenge is detecting name variations, since entities can be referred to in many different ways. Semantically similar words, synonyms or related words, which might be lexically or syntactically different, are often not listed in the knowledge base (e.g., the lack of certain terms like ‘questioning’ but not ‘questionnaire’). This problem of automatically detecting these relationships is generally known as <strong>linking problem</strong>. Note that part of this problem also results from PDF-to-text conversion which is error-prone. Dealing with incomplete knowledge bases, i.e. <strong>handling of out of vocabulary (OOV) items</strong>, is also a major issue, since knowledge bases are often not exhaustive enough and do not cover specific terms or novel concepts from recent research. Last but not least, the combination of different semantic types gives a more coherent picture of a research article. We hypothesize that such information would be helpful and results in an insightful co-occurrence statistics, and provides additional detail directly related to entity resolution, and finally helps to assess the <strong>relevance of terms</strong> by means of a score.</p>
<h3 id="our-approach">4.3 Our approach</h3>
<p>Our research method extraction tool builds on Stanford’s CoreNLP and Named Entity Recognition System[19]. The information extraction process follows the workflow depicted in figure 8.2, using separate modules for pre-processing, classification, linking and term filtering.</p>
<p><img src="combined_images/research-methods-pipeline.png" alt="image" /> Figure 8.2: Overview of the entity extraction pipeline.</p>
<p>We envision the task of finding entities in scientific publications as a sequence labeling problem, where each input word is classified as being of a dedicated semantic type or not. In order to handle entities related to our domain, we train a CRF based machine learning classifier with major semantic classes, (see Table 8.4, using training material from the ACL RD-TEC 2.0 dataset (QasemiZadeh and Schumann, 2016). Apart from this, we follow a domain adaptation approach inspired by (Agerri and Rigau, 2016) and ingest semantic background knowledge extracted from external scientific corpora, in particular the ACL Anthology (Bird et al., 2008; Gildea et al., 2018). We perform entity linking by means of a new gazetteer based on th SAGE dictionary of Social Research Methods (Lewis-Beck et al., 2003), thus putting a special emphasis on the social sciences. The linking component addresses the synonymy problem and matches an entity despite name variations such as spelling variations. Finally, term filtering is carried out based on termhood and unithood, while scoring is achieved by calculating a relevance score based on TF-IDF (cf Table 8.6).</p>
<p>Our research experiments are based on publications from the Social Science Open Access Repository (SSOAR)[20] as well as the train and test data of the Rich Context Competition corpus[21]. Our work extends previous work on this topic (cf. (Eckle-Kohler et al., 2013)) in various ways: First, we do not limit our study to abstracts, but use the entire fulltext. Second, we focus on a broader range of semantic classes, i.e. <em>Research Method</em>, <em>Research Theory</em>, <em>Research Tool</em> and <em>Research Measurement</em>, tackling also the problem of identifying novel entities.</p>
<h4 id="distributed-semantic-models">Distributed semantic models</h4>
<p>For domain adaptation, we integrate further background knowledge. We use topical information from word embeddings trained on an scientific corpus as an additional feature to our NER model. For this, we use agglomerative clustering of the word embeddings to identify topical groups of words. The cluster number of each word is used as additional sequential input feature for our CRF model. Semantic representations of words are a successful extension of common features, resulting in higher NER performance (Turian et al., 2010) and can be trained offline. In this work, the word vectors were learned based on 22,878 documents of the scientific ACL Anthology Reference Corpus[22] using Gensim[23] with the skip-gram model (cf. (Mikolov et al., 2013)) and a pre-clustering algorithm[24].</p>
<h4 id="features">Features</h4>
<p>The features incorporated into the linear chain CRF are shown in the Table 8.5. The features depend mainly on the observations and on pairs of adjacent labels, using a log-linear combination. However, since simple token level training of CRFs leads to poor performance, more effective text features such as word shape, orthographic, gazetteer, Part-Of-Speech (POS) tags, along with word clustering have been used.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Type</strong></th>
<th style="text-align: center;"><strong>Features</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Token unigrams</strong></td>
<td style="text-align: center;">(w_{i-2}), (w_{i-1}), (w_{i}), (w_{i+1}), (w_{i+2}), …</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>POS unigrams</strong></td>
<td style="text-align: center;">(p_{i}), (p_{i-1}), (p_{i-2})</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Shapes</strong></td>
<td style="text-align: center;">shape and capitalization</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>NE-Tag</strong></td>
<td style="text-align: center;">(t_{i-1}), (t_{i-2})</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>WordPair</strong></td>
<td style="text-align: center;">((p_{i}), (w_{i}), (c_{i}))</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>WordTag</strong></td>
<td style="text-align: center;">((w_{i}), (c_{i}))</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Gazetteer</strong></td>
<td style="text-align: center;">SAGE Gazetteer</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Distributional Model</strong></td>
<td style="text-align: center;">ACL Anthology model</td>
</tr>
</tbody>
</table>
<p>Table 8.5: Features used for NER.</p>
<h4 id="knowledge-resources">Knowledge resources</h4>
<p>We use the SAGE thesaurus which includes well-defined concepts, an explicit taxonomic hierarchy between concepts as well as labels that specify synonyms of the same concept. A portion of terms is unique to the social science domain (e. g., ‘dependent interviewing’), while others are drawn from related disciplines such as statistics (e. g., ‘conditional likelihood ratio test’)[25]. However, since the thesaurus is not exhaustive and covers only the top-level concepts related to social science methods, our aim was to extend it by automatically extracting further terms from domain-specific texts, in particular from the Social Science Open Access Repository. More concretely, we carried out the following steps to extend SAGE as an off-line step. For step 2 and 3, candidate terms have been extracted by our pipeline for the entire SSOAR corpus.</p>
<ol type="1">
<li><p>Assignment of semantic types to concepts (manual)</p></li>
<li><p>Extracting terms variants such as abbreviations, synonyms, related terms from SSOAR (semi-automatic)</p></li>
<li><p>Computation of term and document frequency scores for SSOAR (automatic)</p></li>
</ol>
<h4 id="extracting-term-variants-such-as-abbreviations-synonyms-and-related-terms">Extracting term variants such as abbreviations, synonyms, and related terms</h4>
<p>26,082 candidate terms have been recognized and classified by our pipeline and manually inspected to a) find synonyms and related words that could be linked to SAGE, and b) build a post-filter for incorrectly classified terms. Moreover, abbreviations have been extracted using the algorithm of Schwartz and Hearst (Schwartz and Hearst, 2003). This way, a Named Entity gazetteer could be built and is used at run-time. It comprises 1,111 terms from SAGE and 447 terms from the used glossary of statistical terms[26] as well as 54 previously unseen terms detected by the model-based classifier.</p>
<h4 id="computation-of-term-and-document-frequency-scores">Computation of term and document frequency scores</h4>
<p>Term frequency statistics have been calculated off-line for the entire SSOAR corpus. The term frequency at corpus level will be used at run time to determine the term relevance at the document level by calculating the TF-IDF scores. The most relevant terms from SAGE are listed in Table 8.6.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>SAGE Term</strong></th>
<th style="text-align: left;"><strong>TF-IDF Score</strong></th>
<th style="text-align: left;"><strong>Semantic Class</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Fuzzy logic</td>
<td style="text-align: left;">591,29</td>
<td style="text-align: left;">Research Method</td>
</tr>
<tr class="even">
<td style="text-align: left;">arts-based research</td>
<td style="text-align: left;">547,21</td>
<td style="text-align: left;">Research Method</td>
</tr>
<tr class="odd">
<td style="text-align: left;">cognitive interviewing</td>
<td style="text-align: left;">521,13</td>
<td style="text-align: left;">Research Method</td>
</tr>
<tr class="even">
<td style="text-align: left;">QCA</td>
<td style="text-align: left;">463,13</td>
<td style="text-align: left;">Research Method</td>
</tr>
<tr class="odd">
<td style="text-align: left;">oral history</td>
<td style="text-align: left;">399,68</td>
<td style="text-align: left;">Research Method</td>
</tr>
<tr class="even">
<td style="text-align: left;">market research</td>
<td style="text-align: left;">345,37</td>
<td style="text-align: left;">Research Field</td>
</tr>
<tr class="odd">
<td style="text-align: left;">life events</td>
<td style="text-align: left;">186,61</td>
<td style="text-align: left;">Research Field</td>
</tr>
<tr class="even">
<td style="text-align: left;">Realism</td>
<td style="text-align: left;">314,34</td>
<td style="text-align: left;">Research Theory</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Marxism</td>
<td style="text-align: left;">206,77</td>
<td style="text-align: left;">Research Theory</td>
</tr>
<tr class="even">
<td style="text-align: left;">ATLAS.ti</td>
<td style="text-align: left;">544,51</td>
<td style="text-align: left;">Research Tool</td>
</tr>
<tr class="odd">
<td style="text-align: left;">GIS</td>
<td style="text-align: left;">486,01</td>
<td style="text-align: left;">Research Tool</td>
</tr>
<tr class="even">
<td style="text-align: left;">SPSS</td>
<td style="text-align: left;">136,52</td>
<td style="text-align: left;">Research Tool</td>
</tr>
</tbody>
</table>
<p>Table 8.6: Most relevant terms from SAGE by Semantic Type.</p>
<h4 id="definition-of-a-relevance-score">Definition of a relevance score</h4>
<p>Relevance of terminology is often assessed using the notion of <em>unithood</em>, i.e. ‘the degree of strength or stability of syntagmatic combinations of collections’, and <em>termhood</em>, i.e. ‘the degree that a linguistic unit is related to domain-specific concepts’ (Kageura and Umino, 1996). Regarding <em>unithood</em>, the NER model implicitly contains heuristics about legal POS tag sequences for candidate terms, consisting of at least one noun (NN), preceeded or followed by modifiers such as adjectives (JJ), participles (VB*) or cardinal numbers (CD), complemented by wordshape features.</p>
<p>In order to find out if the candidate term also fulfills the <em>termhood</em> requirement, domain-specific term frequency statistics have been computed on the SSOAR repository, and set in contrast to general domain vocabulary terms. It has to be noted that only a small portion of the social science terms is actually unique to the domain (e.g., ‘dependent interviewing’), while others might be drawn from related disciplines such as statistics (e.g., ‘conditional likelihood ratio test’).</p>
<h4 id="preliminary-results">Preliminary results</h4>
<p>Our method has been tested on 100 fulltext papers from SSOAR and ten documents from the Rich Context Competition (RCC), all randomly selected from a hold out corpus. In our experiments on SSOAR Social Science publications, we compared results to the given metadata information. The main finding was that while most entities from the SAGE thesaurus could be extracted and linked reliably (e.g., ’Paired t-test’), they could not be easily mapped to the SSOAR metadata terms, which consist of only a few abstract classes (e.g., ’quantitative analysis’). Furthermore, our tool was tested by the RCC organizer, were the judges reviewed ten random publications and generated qualitative scores for each document. In this evaluation, the research method extraction tool received the overall best results of all competitors for this task.[27]</p>
<h2 id="research-field-classification">5. Research field classification</h2>
<h3 id="task-description-2">5.1 Task description</h3>
<p>The goal of this task is to identify the research fields covered in the social science publications. In general, two approaches could be applied to this task. One is the extraction of relevant terms of the publications. It means that this task could be seen as a keyword extraction task and the detected terms considered as descriptive terms regarding the research field. The second approach is to learn to classify publications research fields with the use of annotated data in a superviesed manner. The benifit of the second approach is that the classification scheme to describe the research field can be defined by experts of the domain. The disadvantage of supervised trained classifiers for this task is the lack of applicable training data. Furthermore, it must be ensured that the training data is comparable to the texts the research field classifier should be applied on.</p>
<h4 id="formal-problem-definition-2">Formal problem definition</h4>
<p>Let (P) denote a set of publications of size (n), (A) a set of corresponding abstracts of the same size and (L) a set of (k) defined class labels describing research fields. The task of research field classification is to select for each publication (p_i) based on the information contained in the corresponding abstract (a_i) a set of labels (C_i = {c_1c_n|c_n L}) of (n) labels. The number of (n) denotes the number of labels from (L) describing the research field of (a_i) and can vary for each publication (p_i). If there is no label (l_k) representing the information given by the abstract (a_i) the set of class labels is the empty set ().</p>
<h3 id="our-approach-1">5.2 Our approach</h3>
<p>Since we didn’t receive any gold standard for this task during the competition we decided to make use of external resources. We decided to use an external labeled dataset to train a text classifier which is able to predict one or moreresearch label for a given abstract of a publication.</p>
<p>The publications given througout the competition belongs to the domain of social sciences we considered metadata from a open access repository for the social sciences called SSOAR. The advantages are twofold. On the one hand, we could rely on professional annotations in a given classification scheme covering the social sciences and related areas. On the other hand the source is openly available.[28]</p>
<p>The annotated data of SSOAR contains four different annotation schemes for research field related information. By reviewing these schemes, we decided to use the Classification Social Science (classoz) annotation scheme. The number of classes in each schema, coverage of each classification, and the distribution of data in each schema affected our decision. An exhausitve description of the used data can be found in Section 8.2.</p>
<h4 id="pre-processing-and-model-architecture">Pre-processing and model architecture</h4>
<p>SSOAR is a multilingual repository. Therefore, the available abstracts may vary in language and the language of the abstract may differ from the language of the article itself. We selected all English abstract with valid classification as our dataset. Mainly because of the language of the RCC corpus. However, it should be noted that the multilingual SSOAR abstract corpus has a skewed distribution of languages with English and German as the main languages. We count 22,453 English abstracts with valid classification after filtering. Due to the unequal distribution of labels in the dataset, we need to guaranty enough training data for each label. We selected only labels with frequency over 300 for training the model, which results in a total of 44 out of 154 classification labels representing research fields. For creating train and test set, 22,453 SSOAR publications with their assigned labels were split randomly. We used a train/validation/test split of 70/10/20. We decided to train a text classifier based on a fasttext (Joulin et al., 2017) model in the author’s implementation. The arguments to use this model was the speed in comparison to a more complex neural net architecture and the still comparable to state of the art performance (e.g.(Wang et al., 2018)). The model is trained with learning rate 1.0 for 150 epochs. Also, the negative sampling parameter is set to 25.</p>
<h3 id="evaluation-3">5.3 Evaluation</h3>
<p>Figure 8.3 shows the performance of the model regarding various evaluation metrics for different thresholds. A label is assigned to a publication if the model outputs a probability for the label above the defined threshold. In multi-label classification, this allows us to evaluate our model from different perspectives. As illustrated in figure 8.3, the intersection of the micro precision and the micro recall curves is at the threshold of 0.1, where the highest micro f1 score is achieved. By increasing the threshold from this point, the micro-precision score is increasing, but the micro recall is falling. By decreasing threshold, these trends are inverted. Also, the default threshold of 0.5 doesn’t look promising. In spite of micro-precision about 0.75, we have a problem with the very high number of items without any prediction. In respect to this observation it is advantageous to select a lower threshold in a productive environment. The curve named <em>without prediction</em> shows for a given threshold the share of publications in the test set without any prediction. If the selected threshold value is high, the number of publications for which the model cannot predict a research field increases. For example, a selected threshold value of 0.55 leads to 40% unclassified publications in the test set. The <em>one correct</em> named curve indicates the quality of the publication wise prediction. It shows the share of all publications in the test set where at least one of the predicted research field labels can be found in the ground truth data. For instance, if a threshold of 0.1 is selected for 75% of the publications in the test set, at least one of the model predictions are correct. This value decreases with increasing threshold simmilar to the recall metric. The final micro f1 value on the test set for our model and a selected threshold of 0.1 is 0.56 (precison 0.55, recall 0.56).</p>
<p><img src="combined_images/fields-precision-recall.png" alt="image" /> Figure 8.3: Performance for different selected probability thresholds (validation set).</p>
<h2 id="discussions-and-limitations">6. Discussions and Limitations</h2>
<h3 id="dataset-extraction.">6.1 Dataset Extraction.</h3>
<p>For the dataset extraction task, the proposed methods are only tested on social science related data. The performance measures we have introduced are based on a hold out data set of our automatically created dataset. Especially the recall may be biased given that our training as well as testing data is biased towards known datasets, where datasets not yet part of our reference set are not considered.</p>
<p>The results of the second phase presented during the RCC workshop[29] are showing good performance of our approach in comparison to the other finalist teams with the highest precision 52.2% (second: 47.0%) and second in recall (ours: 20.5, best: 34.8%). With respect to F1, our approach provides the second best performing system for this task (29.5%, 40.0% for first place). The results on the manually created hold out set underline, that our system performs better in respect to precision in comparison to the other finalist teams. Given that our models are supervised through a corpus of social sciences publications, we anticipate limited generalisability across other disciplines and plan to investigate this aspect as part of future work. In this context, the focus of our training data towards survey data, also reflected in dataset titles such as <em>Current Population Survey</em>, could have biased the model to detect the survey as a specific type of research datasets better than other subtypes like e.g. text corpora in the NLP community. In general, however, our approach to using a weakly labeled corpus created from a list of dataset names could be applied in other research domains.</p>
<h3 id="research-method-extraction.">6.2 Research method extraction.</h3>
<p>We consider the extraction of research methods from full text as a particularly challenging task because the sample vocabulary given by the RCC organizers covers a large thematic variety of areas. The task itself was defined as the identification of research methods associated with a specific publication, which in turn are drawn from a specific research field. Since no training data has been provided, we created and annotated a new corpus for the task and trained a CRF model, adding lexical resources. The qualitative reviews during the two phases of the competition attested that this approach works fine.</p>
<h3 id="research-field-classification.">6.3 Research field classification.</h3>
<p>Our supervised machine learning approach to handle the research field classification task performs well on the dataset created from social science publication metadata. A micro F1 measure of above 55% seems to indicate reasonable performance considering the small dataset with 44 labels and a mean number of keywords of three terms per publication. As one example of multilabel classification with a comparable size of labels we would like to mention the classification of texts in the domain of medicine presented in (Wang et al., 2018). The models tested by the authors on the task of multilabel prediction from 50 different labels leads to micro F1 values between 53% and 62%. Considering the evaulation approach, focused on publications from the social sciences, the generalisability across other disciplines remains unclear and requires further research. Even though the used classification scheme may cover neighbouring disciplines, for instance, medicine, the numbers of samples of the training data covering other research fields than the social science is limited. Our pragmatic approach of basing our classifications on the abstract of the publications makes it applicable even in scenarios where the full-text of publications is not accessible.</p>
<h2 id="conclusion-4">7. Conclusion</h2>
<p>This chapter has provided an overview on our solutions submitted to the Rich Context Competition 2018. Aimed at improving search, discovery and interpretability of scholarly resources, we are addressing three distinct tasks all aimed at extracting structured information about research resources from scientitifc publications, namely the extraction of dataset mentions, the extraction of mentions of research methods and the classification of research fields.</p>
<p>In order to address all aforementioned challenges, our pipelines make use of a range of preprocessing techniques together with state-of-the-art NLP methods as well as supervised machine learning approaches tailored towards the specific nature of scholarly publications as well as the dedicated tasks. In addition, background datasets have been used to facilitate supervision of methods at larger scale.</p>
<p>Our results indicate both significant opportunities for automating the aforementioned three tasks but also their challenging nature, in particular given the lack of publicly available gold standards for training and testing. Aggregating and publishing such data has been identified as important activity for future work, and is a prerequisite for significantly advancing state-of-the-art methods.</p>
<h2 id="acknowledgments-3">Acknowledgments</h2>
<p>This work has been partially funded by Deutsche Forschungsgemeinschaft (DFG) under grant number MA 3964/7-1. Wolfgang Otto acknowledges the enabling support provided by the Indo-German Joint Research Project titled ‘Design of a Sciento-text Computational Framework for Retrieval and Contextual Recommendations of High-Quality Scholarly Articles’ (Grant No. DST/INT/FRG/DAAD/P-28/2017) for this work.</p>
<h2 id="references-5">References</h2>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-agerri2016robust">
<p>Agerri R and Rigau G (2016) Robust multilingual named entity recognition with shallow semi-supervised features. <em>Artificial Intelligence</em> 238. Elsevier: 63–82.</p>
</div>
<div id="ref-altman2007proposed">
<p>Altman M and King G (2007) A proposed standard for the scholarly citation of quantitative data. <em>D-lib Magazine</em> 13(3/4).</p>
</div>
<div id="ref-conf/jcdl/Backes18">
<p>Backes T (2018a) Effective unsupervised author disambiguation with relative frequencies. In: <em>JCDL</em> (eds J Chen, MA Gonçalves, JM Allen, et al.), 2018, pp. 203–212. ACM. Available at: <a href="http://dblp.uni-trier.de/db/conf/jcdl/jcdl2018.html#Backes18" class="uri">http://dblp.uni-trier.de/db/conf/jcdl/jcdl2018.html#Backes18</a>.</p>
</div>
<div id="ref-conf/cikm/Backes18">
<p>Backes T (2018b) The impact of name-matching and blocking on author disambiguation. In: <em>CIKM</em> (eds A Cuzzocrea, J Allan, NW Paton, et al.), 2018, pp. 803–812. ACM. Available at: <a href="http://dblp.uni-trier.de/db/conf/cikm/cikm2018.html#Backes18" class="uri">http://dblp.uni-trier.de/db/conf/cikm/cikm2018.html#Backes18</a>.</p>
</div>
<div id="ref-bird2008acl">
<p>Bird S, Dale R, Dorr BJ, et al. (2008) The acl anthology reference corpus: A reference dataset for bibliographic research in computational linguistics. In: <em>Proceedings of the sixth international conference on language resources and evaluation (lrec 2008)</em>, 2008. European Language Resources Association (ELRA).</p>
</div>
<div id="ref-boland2019distant">
<p>Boland K and Krüger F (2019) Distant supervision for silver label generation of software mentions in social scientific publications. In: <em>Proceedings of the 4th joint workshop on bibliometric-enhanced information retrieval and natural language processing for digital libraries</em>, 2019, pp. 15–27.</p>
</div>
<div id="ref-boland2012identifying">
<p>Boland K, Ritze D, Eckert K, et al. (2012) Identifying references to datasets in publications. In: <em>International conference on theory and practice of digital libraries</em>, 2012, pp. 150–161. Springer.</p>
</div>
<div id="ref-eckle2013automatically">
<p>Eckle-Kohler J, Nghiem T-D and Gurevych I (2013) Automatically assigning research methods to journal articles in the domain of social sciences. In: <em>Proceedings of the 76th asis&amp;T annual meeting: Beyond the cloud: Rethinking information boundaries</em>, 2013, p. 44. American Society for Information Science.</p>
</div>
<div id="ref-finkel2005incorporating">
<p>Finkel JR, Grenager T and Manning C (2005) Incorporating non-local information into information extraction systems by gibbs sampling. In: <em>Proceedings of the 43rd annual meeting on association for computational linguistics</em>, 2005, pp. 363–370. Association for Computational Linguistics.</p>
</div>
<div id="ref-ghavimi2016semi">
<p>Ghavimi B, Mayr P, Lange C, et al. (2016) A semi-automatic approach for detecting dataset references in social science texts. <em>Information Services &amp; Use</em> 36(3-4). IOS Press: 171–187.</p>
</div>
<div id="ref-gildea2018acl">
<p>Gildea D, Kan M-Y, Madnani N, et al. (2018) The acl anthology: Current state and future directions. In: <em>Proceedings of workshop for nlp open source software (nlp-oss)</em>, 2018, pp. 23–28.</p>
</div>
<div id="ref-conf/jcdl/HienertKBZM19">
<p>Hienert D, Kern D, Boland K, et al. (2019) A digital library for research data and related information in the social sciences. In: <em>JCDL</em> (eds M Bonn, D Wu, JS Downie, et al.), 2019, pp. 148–157. IEEE. Available at: <a href="http://dblp.uni-trier.de/db/conf/jcdl/jcdl2019.html#HienertKBZM19" class="uri">http://dblp.uni-trier.de/db/conf/jcdl/jcdl2019.html#HienertKBZM19</a>.</p>
</div>
<div id="ref-joulin2017bag">
<p>Joulin A, Grave E, Bojanowski P, et al. (2017) Bag of tricks for efficient text classification. In: <em>Proceedings of the 15th conference of the european chapter of the association for computational linguistics: Volume 2, short papers</em>, 2017, pp. 427–431. Association for Computational Linguistics.</p>
</div>
<div id="ref-kageura1996methods">
<p>Kageura K and Umino B (1996) Methods of automatic term recognition: A review. <em>Terminology. International Journal of Theoretical and Applied Issues in Specialized Communication</em> 3(2). John Benjamins Publishing Company: 259–289.</p>
</div>
<div id="ref-Krmer2018ADD">
<p>Krämer T, Klas C-P and Hausstein B (2018) A data discovery index for the social sciences. In: <em>Scientific data</em>, 2018.</p>
</div>
<div id="ref-lewis2003sage">
<p>Lewis-Beck M, Bryman AE and Liao TF (2003) <em>The Sage Encyclopedia of Social Science Research Methods</em>. Sage Publications.</p>
</div>
<div id="ref-mikolov2013distributed">
<p>Mikolov T, Sutskever I, Chen K, et al. (2013) Distributed representations of words and phrases and their compositionality. In: <em>Advances in neural information processing systems</em>, 2013, pp. 3111–3119.</p>
</div>
<div id="ref-nasar2018information">
<p>Nasar Z, Jaffry SW and Malik MK (2018) Information extraction from scientific articles: A survey. <em>Scientometrics</em> 117(3). Springer: 1931–1990.</p>
</div>
<div id="ref-qasemizadeh2016acl">
<p>QasemiZadeh B and Schumann A-K (2016) The acl rd-tec 2.0: A language resource for evaluating term extraction and entity recognition methods. In: <em>LREC</em>, 2016.</p>
</div>
<div id="ref-sahoo2015analysing">
<p>Sahoo P, Gadiraju U, Yu R, et al. (2017) Analysing structured scholarly data embedded in web pages. <em>Lecture Notes in Computer Science</em> 9792. Springer.</p>
</div>
<div id="ref-SchwartzH03">
<p>Schwartz AS and Hearst MA (2003) A simple algorithm for identifying abbreviation definitions in biomedical text. In: <em>Pacific symposium on biocomputing</em>, 2003, pp. 451–462.</p>
</div>
<div id="ref-tkaczyk2015cermine">
<p>Tkaczyk D, Szostek P, Fedoryszak M, et al. (2015) CERMINE: Automatic extraction of structured metadata from scientific literature. <em>International Journal on Document Analysis and Recognition (IJDAR)</em> 18(4). Springer: 317–335.</p>
</div>
<div id="ref-Turian">
<p>Turian J, Ratinov L and Bengio Y (2010) Word representations: A simple and general method for semi-supervised learning. In: <em>Proceedings of the 48th annual meeting of the association for computational linguistics</em>, Stroudsburg, PA, USA, 2010, pp. 384–394. ACL ’10. Association for Computational Linguistics. Available at: <a href="http://dl.acm.org/citation.cfm?id=1858681.1858721" class="uri">http://dl.acm.org/citation.cfm?id=1858681.1858721</a>.</p>
</div>
<div id="ref-wang2018joint">
<p>Wang G, Li C, Wang W, et al. (2018) Joint embedding of words and labels for text classification. <em>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. Association for Computational Linguistics. DOI:10.18653/v1/p18-1216.</p>
</div>
<div id="ref-journals/semweb/YuGFLRD19">
<p>Yu R, Gadiraju U, Fetahu B, et al. (2019) KnowMore - knowledge base augmentation with structured web markup. <em>Semantic Web</em> 10(1): 159–180. Available at: <a href="http://dblp.uni-trier.de/db/journals/semweb/semweb10.html#YuGFLRD19" class="uri">http://dblp.uni-trier.de/db/journals/semweb/semweb10.html#YuGFLRD19</a>.</p>
</div>
</div>
<ol type="1">
<li><p><a href="https://www.gesis.org/en/institute" class="uri">https://www.gesis.org/en/institute</a></p></li>
<li><p><a href="https://www.gesis.org/en/institute/departments/knowledge-technologies-for-the-social-sciences/" class="uri">https://www.gesis.org/en/institute/departments/knowledge-technologies-for-the-social-sciences/</a></p></li>
<li><p><a href="https://www.gesis.org/en/research/applied-computer-science/labs/wts-research-labs" class="uri">https://www.gesis.org/en/research/applied-computer-science/labs/wts-research-labs</a></p></li>
<li><p><a href="https://www.gesis.org/en/research/external-funding-projects/archive/infolis-i-and-ii" class="uri">https://www.gesis.org/en/research/external-funding-projects/archive/infolis-i-and-ii</a></p></li>
<li><p><a href="https://search.gesis.org" class="uri">https://search.gesis.org</a></p></li>
<li><p><a href="https://datasearch.gesis.org/" class="uri">https://datasearch.gesis.org/</a></p></li>
<li><p><a href="https://coleridgeinitiative.org/richcontextcompetition" class="uri">https://coleridgeinitiative.org/richcontextcompetition</a></p></li>
<li><p><a href="https://www.ssoar.info" class="uri">https://www.ssoar.info</a></p></li>
<li><p><a href="https://fasttext.cc/" class="uri">https://fasttext.cc/</a></p></li>
<li><p><a href="https://www.gesis.org/ssoar/home" class="uri">https://www.gesis.org/ssoar/home</a></p></li>
<li><p><a href="https://www.gesis.org/en/services/research/tools/thesaurus-for-the-social-sciences" class="uri">https://www.gesis.org/en/services/research/tools/thesaurus-for-the-social-sciences</a></p></li>
<li><p><a href="https://www.gesis.org/angebot/recherchieren/tools-zur-recherche/klassifikation-sozialwissenschaften" class="uri">https://www.gesis.org/angebot/recherchieren/tools-zur-recherche/klassifikation-sozialwissenschaften</a> (in German)</p></li>
<li><p><a href="http://www.openarchives.org" class="uri">http://www.openarchives.org</a></p></li>
<li><p><a href="https://github.com/CeON/CERMINE" class="uri">https://github.com/CeON/CERMINE</a></p></li>
<li><p><a href="https://jats.nlm.nih.gov" class="uri">https://jats.nlm.nih.gov</a></p></li>
<li><p><a href="https://spacy.io" class="uri">https://spacy.io</a></p></li>
<li><p><a href="http://methods.sagepub.com" class="uri">http://methods.sagepub.com</a></p></li>
<li><p>apart from those used in traditional NER systems like <em>Person</em>, <em>Location</em>, or <em>Organization</em> with abundant training data, as covered in the Stanford NER system(Finkel et al., 2005)</p></li>
<li><p><a href="https://nlp.stanford.edu/projects/project-ner.shtml" class="uri">https://nlp.stanford.edu/projects/project-ner.shtml</a></p></li>
<li><p><a href="https://www.ssoar.info" class="uri">https://www.ssoar.info</a></p></li>
<li><p><a href="https://coleridgeinitiative.org/richcontextcompetition" class="uri">https://coleridgeinitiative.org/richcontextcompetition</a> with a total of 5,000 English documents</p></li>
<li><p><a href="https://acl-arc.comp.nus.edu.sg/" class="uri">https://acl-arc.comp.nus.edu.sg/</a></p></li>
<li><p><a href="https://radimrehurek.com/gensim/" class="uri">https://radimrehurek.com/gensim/</a></p></li>
<li><p>Word embeddings are trained with a skip gram model using embedding size equal to 100, word window equal to 5, minimal occurrences of a word to be considered 10. Word embeddings are clustered using agglomerative clustering with a number of clusters set to <span>500, 600, 700</span>. Ward linkage with Euclidean distance is used to minimize the variance within the clusters.</p></li>
<li><p>A glossary of statistical terms as provided in <a href="https://www.statistics.com/resources/glossary/" class="uri">https://www.statistics.com/resources/glossary/</a> has been added as well.</p></li>
<li><p>Based on <a href="https://www.statistics.com/resources/glossary" class="uri">https://www.statistics.com/resources/glossary</a></p></li>
<li><p>Rank: 1,2,2,1,1 for judges 1-5.</p></li>
<li><p>A script to download the metadata of SSOAR can be found in github/research-field-classifier</p></li>
<li><p>Agenda of the Workshop: <a href="https://coleridgeinitiative.org/richcontextcompetition/workshopagenda" class="uri">https://coleridgeinitiative.org/richcontextcompetition/workshopagenda</a>. The results of the finalists are presented here: <a href="https://youtu.be/PE3nFrEkwoU?t=9865" class="uri">https://youtu.be/PE3nFrEkwoU?t=9865</a>.</p></li>
</ol>
<hr />
<h1 id="chapter-9---finding-datasets-in-publications-the-university-of-paderborn-approach">Chapter 9 - Finding datasets in publications: The University of Paderborn approach</h1>
<h1 id="abstract">Abstract</h1>
<p>The steadily increasing number of publications available to researchers makes it difficult to keep track of the state of the art. In particular, tracking the datasets used, topics addressed, experiments performed and results achieved by peers becomes increasingly tedious. Current academic search engines render a limited number of entries pertaining to this information. However, having this knowledge would be beneficial for researchers to become acquainted with all results and baselines relevant to the problems they aim to address. With our participation in the NYU Coleridge Initiative’s Rich Context Competition, we aimed to provide approaches to automate the discovery of datasets, research fields and methods used in publications in the domain of Social Sciences. We trained an Entity Extraction model based on Conditional Random Fields and combined it with the results from a Simple Dataset Mention Search to detect datasets in an article. For the identification of Fields and Methods, we used word embeddings. In this chapter, we describe how our approaches performed, their limitations, some of the encountered challenges and our future agenda.</p>
<h1 id="literature-review-1">Literature Review</h1>
<p>Previous works on information retrieval from scientific articles are mainly seen in the field of Bio-medical Sciences and Computer Science, with systems <span class="citation" data-cites="DBLP:journals/ploscb/WestergaardSTJB18">[@DBLP:journals/ploscb/WestergaardSTJB18]</span> built using the MEDLINE<a href="#fn74" class="footnote-ref" id="fnref74" role="doc-noteref"><sup>74</sup></a> abstracts, full-text articles from PubMed Central<a href="#fn75" class="footnote-ref" id="fnref75" role="doc-noteref"><sup>75</sup></a> or ACL Anthology dataset<a href="#fn76" class="footnote-ref" id="fnref76" role="doc-noteref"><sup>76</sup></a>. The documents belonging to the above-mentioned datasets follow a similar format, and thus, several metadata and bibliographical extraction frameworks like CERMINE <span class="citation" data-cites="tkaczyk2014cermine">[@tkaczyk2014cermine]</span> have been built on them. However, since articles belonging to the domain of Social Sciences do not follow a standard format, extracting key sections and metadata using already existing frameworks like GROBID <span class="citation" data-cites="lopez2009grobid">[@lopez2009grobid]</span>, ScienceParse<a href="#fn77" class="footnote-ref" id="fnref77" role="doc-noteref"><sup>77</sup></a> or ParsCit <span class="citation" data-cites="councill2008parscit">[@councill2008parscit]</span> did not seem as viable options, majorly because these systems were still under development and lacked certain desired features. Hence, building upon the approach of Westergaard et. al <span class="citation" data-cites="DBLP:journals/ploscb/WestergaardSTJB18">[@DBLP:journals/ploscb/WestergaardSTJB18]</span>, we built our own sections-extraction framework for dataset detection and research fields and methods identification.</p>
<p>Apart from content and metadata extraction, key-phrase or topic extraction from scientific articles has been another emerging research problem in the domain of information retrieval from scientific articles. Jansen et al. <span class="citation" data-cites="jansen2016extracting">[@jansen2016extracting]</span> extracted core claims from scientific articles by first detecting keywords and key-phrases using rule-based, statistical, machine learning and domain-specific approaches and then applying document summarization techniques. For characterizing a research work in terms of its focus, application domain and techniques used, Gupta et al. <span class="citation" data-cites="gupta2011analyzing">[@gupta2011analyzing]</span> proposed applying semantic extraction patterns to the dependency trees of sentences in an article’s abstract. On the other hand, to thematically represent scientific articles and for ranking the extracted key-phrases, Mahata et al. <span class="citation" data-cites="mahata2018key2vec">[@mahata2018key2vec]</span> devised an approach for processing text documents to train phrase embeddings.</p>
<p>The problem of dataset detection and methods and fields identification is not only different from the ones mentioned above, but also our approach for tackling it is radically disparate. The following sections describe our approach in detail.</p>
<h1 id="project-architecture">Project Architecture</h1>
<figure>
<embed src="combined_images/flowchart_paper.pdf" /><figcaption>Data Flow Pipeline: Red lines depict the flow of given and generated files between components whereas black lines represent the generation of final output files<span data-label="fig:flowchart"></span></figcaption>
</figure>
<p>Our pipeline (shown in Figure [fig:flowchart]) consisted of three main components: 1) Preprocessing, 2) Fields and Methods Identification and 3) Dataset Extraction. The Preprocessing module read the text from publications and generated some additional files (see Section [preprocess] for details). These files along with the given Fields and Methods vocabularies were used to infer Research Fields and Methods from the publications. Then, the information regarding Research Fields was passed onto the Dataset Detection module and using the Dataset Vocabulary, Dataset Citations and Mentions were identified. The following sections provide a detailed overview of each of these components.</p>
<h1 id="preprocessing">Preprocessing</h1>
<p>As discussed in Chapter 5, the publications were provided in two formats: PDF and text. For Phase-1, we used the given text files, however during Phase-2, we came across many articles in the training files that had not been properly converted to text and contained mostly non-ASCII characters. To work with such articles, we relied on the open source tool <code>pdf2text</code> from <code>poppler suite</code><a href="#fn78" class="footnote-ref" id="fnref78" role="doc-noteref"><sup>78</sup></a> to extract text from PDFs. The <code>pdf2text</code> command served as the first preprocessing step and was called as a subprocess from within a python script. It was used with <code>-nopgbrk</code> argument to generate the text files.</p>
<p>Once we had the text files, we followed the rule-based approach as proposed by Westergaard et al. <span class="citation" data-cites="DBLP:journals/ploscb/WestergaardSTJB18">[@DBLP:journals/ploscb/WestergaardSTJB18]</span> for pre-processing. The following series of operations based mostly on regular expressions were performed:</p>
<ul>
<li><p>Words split by hyphens were de-hyphenated</p></li>
<li><p>Irrelevant data was removed (i.e., equations, tables, acknowledgment, references);</p></li>
<li><p>Main sections (i.e., abstract, keywords, JEL-Classification, methodology/data, summary, conclusion) were identified and extracted;</p></li>
<li><p>Noun phrases from these sections were extracted (using the python library, spaCy<a href="#fn79" class="footnote-ref" id="fnref79" role="doc-noteref"><sup>79</sup></a>).</p></li>
</ul>
<p>We came up with the heuristics for identifying the main sections after going through the articles from different domains in the training data. We collected the surface forms for the headings of all major sections (abstract, keywords, introduction, data, approach, summary, discussion) and applied regular expressions to search for them and separate them from one another. The headings and their corresponding content were stored as key-value pairs in a file. For generating noun-phrases, this file was parsed and for all the values (content) in key-value (heading-content) pairs, a spaCy object, <code>doc</code>, was created sentence-wise. Using the built-in function for extracting noun chunks (<span><code>doc.noun_chunks</code></span>), we generated key-value pairs of heading and noun-phrases found in the content and stored them in another file. This file was later used for fields and methods identification.</p>
<p>To determine how well our approach performed in distinguishing sections, we evaluated it on the articles in the validation dataset. During evaluation, we figured out the limiting cases of our approach. A section could not be differentiated either when there was no explicit mention of any of its surface forms or if there were multiple mentions of the surface forms in the articles. For instance, in the validation dataset (see Table [tab:sections]), keywords were not extracted from 13 articles because of no explicit mention of the term ’keywords’ or its variants. On manual inspection, we found keywords were actually not mentioned in these 13 articles. In the remaining articles where the keywords were present, our algorithm could not detect them from 1 article. For brevity, we have reported only four main sections in Table [tab:sections]: title, abstract, keywords and methodology/data, since these are the ones getting preferential treatment in methods and fields identification. If a section was not found in the article (because of no explicit mention of any of the surface forms), then only the sections that could be detected were extracted. The remaining content was saved as <code>reduced_content</code> after cleaning and noun-phrases were extracted from it to prevent loss of any meaningful data.</p>
<p><span>C<span>4cm</span> C<span>3.5cm</span> C<span>4cm</span></span> <strong>Sections</strong> &amp; <strong>No explicit mention</strong> &amp; <strong>Mentioned but not distinguished</strong><br />
Title &amp; 0 &amp; 0<br />
Keywords &amp; 13 &amp; 1<br />
Abstract &amp; 0 &amp; 1<br />
Methodology/Data &amp; 18 &amp; 4</p>
<p>In addition to the main sections, we also extracted PDF metadata using <code>pdfinfo</code> service from the <code>poppler suite</code> library. The metadata very often contained the keywords and subject of an article, which was helpful in those cases where the keywords were not found by the regular expression.<br />
In the end, the preprocessing module generated four text files for a publication: PDF-converted text, PDF-metadata, processed articles containing relevant data, and noun phrases from the relevant sections, respectively. These files were then passed on to the other two components of the pipeline, which have been discussed below.</p>
<h1 id="approach">Approach</h1>
<h2 id="research-fields-and-methods-identification">Research Fields and Methods Identification</h2>
<h3 id="vocabulary-generation-and-model-preperation">Vocabulary Generation and Model Preperation</h3>
<ol type="1">
<li><p><strong>Research Methods Vocabulary</strong>: In Phase-1 of the challenge, we used the given methods vocabulary. However, the feedback that we received from Phase-1 evaluation gave more emphasis to statistical methods used by the authors, references to the time scope, unit of observation, and regression equations rather than the means used to compile the data, i.e., surveys. Since the given methods vocabulary was not a complete representation of statistical methods and also consisted terms depicting surveys, in Phase-2, we decided to create our own Research Methods Vocabulary using Wikipedia and DBpedia.<a href="#fn80" class="footnote-ref" id="fnref80" role="doc-noteref"><sup>80</sup></a> We manually curated a list of all the relevant statistical methods from Wikipedia<a href="#fn81" class="footnote-ref" id="fnref81" role="doc-noteref"><sup>81</sup></a> and fetched their descriptions from the corresponding DBpedia resources. For each label in the vocabulary, we extracted noun phrases from its description and added them to the vocabulary. Please refer Table [tab:vocab] for examples.</p>
<p><span>C<span>1.5cm</span> C<span>5cm</span> C<span>5cm</span></span> <strong>Label</strong> &amp; <strong>Description</strong> &amp; <strong>Noun Phrases from Description</strong><br />
Political forecasting &amp; Political forecasting aims at predicting the outcome of elections. &amp; Political forecasting, the outcome, elections<br />
Nested sampling algorithm &amp; The nested sampling algorithm is a computational approach to the problem of comparing models in Bayesian statistics, developed in 2004 by physicist John Skilling. &amp; algorithm, a computational approach, the problem, comparing models, Bayesian statistics, physicist John Skilling</p></li>
<li><p><strong>Research Fields Vocabulary</strong>: For both the phases, we used the given research fields vocabulary and, just like the methods vocabulary, supplemented it with the noun phrases from the description of the research field labels. However, since our phase-1 model seemed to confuse fields with methods, for Phase-2, we additionally created a stopword-list of terms that didn’t contain any domain-specific information, such as; Mixed Methods, Meta Analysis, Narrative Analysis and the like.</p></li>
<li><p><strong>Word2Vec Model generation</strong>: In this pre-processing step, we used the above-mentioned vocabulary files containing noun phrases to generate a vector model for both research fields and methods. The vector model was generated by using the labels and noun phrases from the description of the available research fields and methods to form a sum vector. The sum vector was basically the sum of all the vectors of the words present in a particular noun phrase. 3em <span>The pre-trained Word2Vec model <code>GoogleNews-vectors-negative300.bin</code> <span class="citation" data-cites="DBLP:journals/corr/abs-1301-3781">[@DBLP:journals/corr/abs-1301-3781]</span> was used to extract the vectors of the individual words.</span></p></li>
<li><p><strong>Research Method training results creation</strong>: For research methods, we generated an intermediate result file with the publications present in the training data. It was generated using a <code>naïve finder algorithm</code> which, for each publication, selected the research method with the highest cosine similarity to any of its noun phrase’s vectors. This file was later used to assign weights to research methods using Inverse Document Frequency.</p></li>
</ol>
<h3 id="processing-with-trained-models">Processing with Trained Models</h3>
<ul>
<li><p><strong>Finding Research Fields and Methods:</strong> To find the research fields and methods for a given list of publications, we performed the following steps: (At first, Step 1 was executed for all the publications, thereafter Step 2 and 3 were executed iteratively for each publication).</p>
<ol type="1">
<li><p><strong>Naïve Research Method Finder run</strong> - In this step, we executed the <code>naïve research method finding algorithm</code> (i.e. selected a research method based on the highest cosine similarity between vectors) against all the current publications and then merged the results with the existing result from the <code>research methods’ preprocessing step</code>. The combined result was then used to generate IDF weight values for each <code>research method</code>, to compute the significance of recurring terms.</p></li>
<li><p><strong>IDF-based Research Method Selection</strong> - We re-ran the algorithm to find the closest research method to each noun phrase and then sorted the pairs based on their weighted cosine similarity. The weights were taken from the IDF values generated in the first step and the manual weights assigned (section-wise weighting). Here, the noun phrases that came from the methodology section and from the methods listed in JEL-classification (if present) were given a higher preference. The pair with the highest weighted cosine similarity was then chosen as the Research Method of the article.</p></li>
<li><p><strong>Research Field Finder run</strong> - In this step, we first found the closest research field from each noun phrase in the publication. Then we selected the Top N (= 10) pairs that had the highest weighted cosine similarity. Afterwards, the noun phrases that had a similarity score less than a given threshold (= 0.9) were filtered out. The end-result was then passed on to a post-processing algorithm.<br />
For weighted cosine similarity, the weights were assigned manually based on the section of publication from which the noun phrases came. In general, noun phrases from title and keywords (if present) were given a higher preference than other sections, since usually these two sections hold the crux of an article. Note, if sections could not be discerned from an article, then noun phrases from the section, reduced_content (see section [preprocess]), were used to find both fields and methods.</p></li>
<li><p><strong>Research Field Selection</strong> - The top-ranked term from the result of step 3, which was not present in the stopword-list of irrelevant terms, was marked as the research field of the article.</p></li>
</ol></li>
</ul>
<p>The experimental set-up and average training times (ATT) have been reported in Table [tab:setup]:</p>
<p><span>|C<span>5cm</span> | C<span>7cm</span> |</span> Computing Infrastructure &amp; macOS, 2 GHz Intel Core i7 processor, 4 cores RAM 16 GB 1600 MHz<br />
ATT - RF model &amp; 3m 21s<br />
ATT - RM model &amp; 3m 19s<br />
Link to Implemented Code &amp; <a href="https://github.com/nikit91/Jword2vec/tree/rich-context" class="uri">https://github.com/nikit91/Jword2vec/tree/rich-context</a></p>
<h2 id="dataset-extraction-1">Dataset Extraction</h2>
<p>For identifying the datasets in a publication, we followed two approaches and later combined results from both. Both the approaches have been described below.</p>
<ol type="1">
<li><p><strong>Simple Dataset Mention Search:</strong> We chose the dataset citations from the given Dataset Vocabulary that occurred for one dataset only and used these unique mentions to search for the corresponding datasets using regular expressions in the text documents. Then, we computed a frequency distribution of the datasets. As can be seen from Figure [fig:graph], certain dataset citations occurred more often than others. This is because while searching for dataset citations, apart from the dataset title, the corresponding mention_list from Dataset Vocabulary was also considered, which contained many commonly occurring terms like ‘time’, ‘series’, ‘time series’, ‘population’ etc. Therefore, we filtered out those dataset citations that occurred more than a certain threshold value (=1.20) multiplied by the median of the frequency distribution and that had less than 3 distinct mentions in a publication. The remaining citations were written to an interim result file. Table [tab:simple] depicts the improvement in performance of Simple Dataset Mention Search with the inclusion of filtering. The filtering process improved the F1-measure by 42.86%. Note, as the validation data consisted of only 100 articles, changing the threshold value to 1.10 or 1.30 didn’t result in any significant change, hence we have maintained a constant threshold value of 1.20 in our comparison table.</p>
<p><span>C<span>1.5cm</span> C<span>3.5cm</span> C<span>3.5cm</span> C<span>3.5cm</span></span> <strong>Metrics</strong> &amp; <strong>without filtering</strong> &amp; <strong>Threshold=1.20, mentions <span class="math inline">&lt;</span> 3</strong> &amp; <strong>Threshold=1.20, mentions <span class="math inline">&lt;</span> 4</strong><br />
Precision &amp; 0.09 &amp; 0.71 &amp; 0.09<br />
Recall &amp; 0.28 &amp; 0.12 &amp; 0.28<br />
F1-score &amp; 0.14 &amp; <strong>0.20</strong> &amp; 0.14</p>
<figure>
<embed src="combined_images/freq.pdf" /><figcaption>Frequency Distribution of Dataset Citations<span data-label="fig:graph"></span></figcaption>
</figure></li>
<li><p><strong>Rasa-based Dataset Detection:</strong> In our second approach, we trained an entity extraction model based on conditional random fields (CRF) using Rasa NLU <span class="citation" data-cites="DBLP:journals/corr/abs-1712-05181">[@DBLP:journals/corr/abs-1712-05181]</span>. For training the model we used the Spacy Tokenizer<a href="#fn82" class="footnote-ref" id="fnref82" role="doc-noteref"><sup>82</sup></a> for the preprocessing step. For Entity Recognition we used BILOU tagging and used 50 iterations to train the CRF. We used the Part of Speech tags, the case of the input tokens and the suffixes of the tokens as input features for the CRF model. We particularly tested two configurations for training the CRF-based Named Entity Recognition (NER) model. In Phase-1, the 2500 labeled publications from the training dataset were used for training the Rasa NLU<a href="#fn83" class="footnote-ref" id="fnref83" role="doc-noteref"><sup>83</sup></a> model. Later in Phase-2, when the Phase-1 holdout corpus was released, we combined its 5000 labeled publications with the previously given 2500 labeled publications and then retrained the model again with these 7500 labeled publications.<br />
<strong>Running the CRF-Model:</strong> The trained model was run against the preprocessed data to detect dataset citations and mentions. Only the entities that had a confidence score greater than a certain threshold value (= 0.72) were considered as dataset mentions. A dataset mention was considered as a citation only if it was found in the given Dataset Vocabulary (via string matching either with a dataset title or any of the terms in a dataset ‘mention_list’) and if it belonged to the research field of the article. To check if a dataset belonged to the field of research, we found the cosine similarity of the terms in the ‘subjects’ field of the Dataset Vocabulary with the keywords and the identified Research Field of the article.</p></li>
<li><p><strong>Combining the two approaches:</strong> The output generated by the Rasa-based approach was first checked for irrelevant citations before a union was performed to combine the results. This was done by checking if a given dataset_id occured more than a threshold value (= 1.20) multiplied by the median of the frequency distribution (same as the filtering process of the Simple Dataset Mention Search).</p></li>
</ol>
<p>Note that, the threshold values mentioned above were set after some experiments of trial and testing. For dataset extraction, the goal was to keep the number of false positives low while not compromising the true positives. For research methods and fields, a manual evaluation (see the next section for details) was done to test if the results made sense with the articles.</p>
<h1 id="evaluation-4">Evaluation</h1>
<p>We performed a quantitative evaluation for Dataset Extraction using the evaluation script provided by the competition organizers (refer Chapter 5 for more details). This evaluation (see Table [tab:dataset]) was carried out against the validation data, wherein we compared four different configurations. As can be inferred from the table, there was only a slight increase in performance for the Rasa-based model, when the training samples were increased. However, combining it with the Simple Dataset Mention Search, increased the performance by <em>19.42%</em>. Interestingly, there was no improvement in performance in the combined approach even when the training samples for the Rasa-based model were increased. This might be because of the removal of frequently-occuring terms from the Rasa-generated output, based on the frequency distribution of dataset mentions as computed in the Simple Dataset Mention Search.</p>
<p><span>M<span>2.2cm</span> | M<span>2.3cm</span> | M<span>2.2cm</span> M<span>2.2cm</span> M<span>2.2cm</span></span> &amp; &amp;<br />
<strong>Metrics</strong>&amp; <strong>Rasa-based Approach</strong> (2500) &amp; <strong>Rasa-based Approach</strong> (7500) &amp; <strong>Combined Approach</strong> (2500) &amp; <strong>Combined Approach</strong> (7500)<br />
<strong>Precision</strong> &amp; 0.382 &amp; 0.388 &amp; <strong>0.456</strong> &amp; <strong>0.456</strong><br />
<strong>Recall</strong> &amp; 0.26 &amp; 0.26 &amp; <strong>0.31</strong> &amp; <strong>0.31</strong><br />
<strong>F1</strong> &amp; 0.309 &amp; 0.311 &amp; <strong>0.369</strong> &amp; <strong>0.369</strong></p>
<p>For Research Fields and Methods, we carried out a qualitative evaluation against 10 randomly selected articles from Phase-1 holdout corpus. Tables [tab:field] and [tab:method] depict a comparison between the predicted fields and methods in Phase-1 and Phase-2. In general, our models returned a more granular output in the second phase, solely because of the modifications we made in the vocabularies.</p>
<p><span>C<span>1cm</span> C<span>4.5cm</span> C<span>3cm</span> C<span>3.5cm</span></span> <strong>pubid</strong> &amp; <strong>Keywords</strong> &amp; <strong>Phase-1</strong> &amp; <strong>Phase-2</strong><br />
10328 &amp; Cycling for transport, leisure and sport cyclists &amp; Health evaluation &amp; <strong>Public health and health promotion</strong><br />
7270 &amp; Older adult drug users, harm reduction &amp; Health Education &amp; <strong>Correctional health care</strong><br />
6053 &amp; Economic conditions - crime relationship, homicide &amp; Homicide &amp; <strong>Gangs and crime</strong></p>
<p><span>C<span>1cm</span> C<span>4.5cm</span> C<span>3cm</span> C<span>3.5cm</span></span> <strong>pubid</strong> &amp; <strong>Keywords</strong> &amp; <strong>Phase-1</strong> &amp; <strong>Phase-2</strong><br />
10328 &amp; Thematic content analysis &amp; Thematic analysis &amp; <strong>Sidak correction</strong><br />
7270 &amp; Interviews conducted face to face, finding systematic patterns or relationships among categories identified by reading the interview transcript &amp; Qualitative interviewing &amp; <strong>Sampling design</strong><br />
6053 &amp; Autoregressive integrated moving average (ARIMA) time-series model &amp; Methodological pluralism &amp; <strong>Multivariate statistics</strong></p>
<h1 id="discussion-2">Discussion</h1>
<p>Throughout the course of this competition, we encountered several challenges and limitations in all the three stages of the pipeline. In the preprocessing step, the appropriate extraction of text from PDFs turned out to be rather challenging. This was especially due to the varied formats of the publications, which made the extraction of specific sections—that contained all data relevant to our work—demanding. As mentioned before, if there was no explicit mention of the key-terms like <code>Abstract, Keywords, Introduction, Methodology/Data, Summary, Conclusion</code> in the text, then the content was saved as ‘reduced_content’ after applying all other preprocessing steps and filtering out any irrelevant data.<br />
Our experiments suggest that the labeled publications we received for dataset detection were not uniform in the dataset mentions provided, which made it difficult to train an entity extraction model even with an increased number of training samples. Hence, there was only a slight improvement in performance when the Rasa-model was trained with 7500 publications instead of 2500. This was also why we combined the Rasa-based approach with the Simple Dataset Mention Search, so that at least the datasets that were present in the vocabulary do not get missed.</p>
<p>Regarding the fields and methods, vocabularies played an immense role in their identification. The vocabularies that were provided by the SAGE publications contained some terms that were either polysemous or very high-level and therefore, were picked up by our model very often. Hence, for research methods, we created our own vocabulary containing all the relevant statistical methods, and for fields, we introduced a stopword-list of irrelevant terms and looked it up each time, before writing the result to the output file. The goal of stopword-list generation was to filter the terms that did not carry domain-specific information and sounded more like research methods than fields. Since the focus was on more granulated results, we tried to look for open ontologies for Social Science Fields and Methods and unfortunately, could not find any. It is worth mentioning that since our approach for Fields and Methods identification relied heavily upon vocabularies, it could not find any new methods or fields from the publications.</p>
<p>Based on the final evaluation feedback, since our Phase-2 models did not perform as good as we expected, following are a few things that we could have done differently.</p>
<ol type="1">
<li><p>For research methods, merging the given SAGE methods vocabulary with our manually curated vocabulary, could have resulted in methods that would have been both granular and statistical while still being relevant to the publications. Introducing a stopword-list just as we did for research field identification, could also have been another workaround.</p></li>
<li><p>For both fields and methods identification, we could have also tried pre-trained embeddings from glove<a href="#fn84" class="footnote-ref" id="fnref84" role="doc-noteref"><sup>84</sup></a> and fastText<a href="#fn85" class="footnote-ref" id="fnref85" role="doc-noteref"><sup>85</sup></a>.</p></li>
<li><p>As our entity-extraction approach for Dataset Detection suffered from a limitation of inconsistent labels (i.e. datasets mentioned in the form of abbreviation, full-name, collection procedure, and keywords) in training data, we could have extended the Simple Dataset Mention Search to a pattern-oriented search based on handcrafted rules derived from sentence structure and other heuristics.</p></li>
</ol>
<h1 id="future-agenda">Future Agenda</h1>
<p>The data provided to us in the competition displayed a cornucopia of inconsistencies even after human processing. We hence propose that machine-aided methods for computing correct and complete structured representation of publications are of central importance for scientific research such as an Open Research Knowledge Graph <span class="citation" data-cites="DBLP:journals/corr/abs-1901-10816">[@DBLP:journals/corr/abs-1901-10816]</span><span class="citation" data-cites="DBLP:conf/esws/BuscaldiDMOR19">[@DBLP:conf/esws/BuscaldiDMOR19]</span>. Previous works on never-ending learning have shown how humans and extraction algorithms can work together to achieve high-precision and high-recall knowledge extraction from unstructured sources. In our future work, we hence aim to populate a <strong>scientific knowledge graph</strong> based on never-ending learning. The methodology we plan to develop will be domain-independent and rely on active learning to classify, extract, link and publish scientific research artifacts extracted from open-access papers. Inconsistency will be remedied by ontology-based checks learned from other publications such as SHACL constraints which can be manually or automatically added.<a href="#fn86" class="footnote-ref" id="fnref86" role="doc-noteref"><sup>86</sup></a> The resulting graphs will</p>
<ul>
<li><p>rely on advanced distributed storage for RDF to scale to the large number of publications available;</p></li>
<li><p>be self-feeding, i.e., crawl the web for potentially relevant content and make this content available for processing;</p></li>
<li><p>be self-repairing, i.e., be able to update previous extraction results based on insights gathered from new content;</p></li>
<li><p>be weakly supervised by humans (e.g. authors of publications), who would assist in correcting wrong hypotheses, thereby leveraging semi-supervised learning;</p></li>
<li><p>provide standardized access via W3C Standards such as SPARQL.</p></li>
</ul>
<p>Having such knowledge graphs would make it easier for the researchers (both young and veteran) to easily follow along with their domain of fast-paced research and eliminate the need to manually update the domain-specific ontologies for fields, methods and other metadata as new research innovations come up.</p>
<h1 id="appendix-1">Appendix</h1>
<p>The code and documentation for all our submissions can be found here: <a href="https://github.com/dice-group/rich-context-competition" class="uri">https://github.com/dice-group/rich-context-competition</a>.</p>
<hr />
<h1 id="chapter-10---finding-datasets-in-publications-the-singapore-management-university-approach">Chapter 10 - Finding datasets in publications: The Singapore Management University approach</h1>
<!--
---
author:
- |
    Philips Kokoh Prasetyo, Amila Silva, Ee-Peng Lim, Palakorn Achananuparp  
    Living Analytics Research Centre  
    Singapore Management University  
    [{pprasetyo,amilasilva,eplim,palakorna}@smu.edu.sg]{}
bibliography:
- 'rcc-02.bib'
title: Simple Extraction for Social Science Publications
---
-->
<h1 id="finding-datasets-in-publications-the-singapore-management-university-approach">Finding datasets in publications: The Singapore Management University approach</h1>
<h3 id="simple-extraction-for-social-science-publications">Simple Extraction for Social Science Publications</h3>
<p>Philips Kokoh Prasetyo<sup>1</sup>, Amila Silva<sup>2</sup><sup>^</sup>, Ee-Peng Lim<sup>1</sup>, Palakorn Achananuparp<sup>1</sup><br />
<sup>1</sup>Living Analytics Research Centre, Singapore Management University<br />
<sup>2</sup>University of Melbourne <sup>^</sup>work done while working at Living Analytics Research Centre <sup>1</sup>{pprasetyo,eplim,palakorna}<span class="citation" data-cites="smu.edu.sg">@smu.edu.sg</span>, <sup>2</sup>amila.silva@student.unimelb.edu.au</p>
<p>First draft: 11 February 2019; Second draft: 12 June 2019, Final draft: 29 November 2019</p>
<h2 id="abstract-1">Abstract</h2>
<p>With the vast number of datasets and literature collections available for research today, it is very difficult to keep track on the use of datasets and literature articles for scientific research and discovery. Many datasets and research work using them are left undiscovered and under-utilized due to the lack of available search tools to automatically find out who worked with the data, on what research topics, using what research methods and generating what results. The Coleridge Rich Context Competition (RCC) therefore aims to build automated dataset discovery tools for analyzing and searching social science research publications. In this chapter, we describe our approach to solving the first phase of Coleridge Rich Context Competition.</p>
<h2 id="introduction-7">Introduction</h2>
<p>Automated discovery from scientific research publications is an important task for analysts, researchers, and learners as they develop the scientific knowledge and use them to gain new insights. More specifically, on the tasks of discovering datasets and methods mentioned in a research publication, we have seen a lack of available tools to easily find who else worked on a particular dataset, what research methods people apply on the dataset, and what results they have found using the dataset. Furthermore, new datasets are not easy to discover, and as a result, good datasets and methods are often neglected.</p>
<p>The Coleridge Rich Context Competition (RCC) aims to build automated datasets discovery from social science research publications, filling the gap of this problem. In this competition, given a corpus of social science research publications, we have to automatically identify datasets used, and then infer the research methods and research fields in the publications. Note that no labeled data are given for research methods and fields identification.</p>
<p>We describes our submission to the first phase of RCC. We perform dataset detection followed by implicit entity linking approach to tackle dataset extraction task. We adopt weakly supervised classification for research methods and fields identification tasks utilizing external resource SAGE Knowledge as proxy for weak labels.</p>
<!-- This manuscript describes summary of our submission for the first phase
of RCC. We begin with related work in section \[sec:relatedwork\]. We
present our analysis on RCC dataset in section \[sec:data\], describe
our approach in section \[sec:methods\], and discuss our experiment
results in section \[sec:experiments\]. Finally, we wrap up with
conclusion and future work in section \[sec:conclusion\].
 -->
<h2 id="related-work">Related Work</h2>
<p>Extracting information from scientific text has been explored in the past [<a href="#PM04">PM04</a>; <a href="#NCKL15">NCKL15</a>; <a href="#SBP+16">SBP<sup>+</sup>16</a>]. One type of information extraction from scientific articles is extracting keyphrases and relation between them [<a href="#ADR+17">ADR<sup>+</sup>17</a>]. Luan et al. (2017) propose semi-supervised sequence tagging approach to extract keyphrases [<a href="#LOH17">LOH17</a>]. Augenstein and Søgaard (2017) explore multi-task deep recurrent neural network approach with several auxiliary tasks to extract keyphrases [<a href="#AS17">AS17</a>].</p>
<p>Another type of extraction is citation extraction. Two citation extraction settings have been explored before: reference mining inside the full text [<a href="#ACK18">ACK18</a>], and citation metadata extraction [<a href="#Het08">Het08</a>; <a href="#APBM14">APBM14</a>; <a href="#AGJ+17">AGJ<sup>+</sup>17</a>]. Nasar er al. (2018) write a survey on information extraction from scientific articles [<a href="#NJM18">NJM18</a>].</p>
<p>Recently, there are some work to explore dataset extraction from scientific text [<a href="#BREM12">BREM12</a>; <a href="#GML+16">GML<sup>+</sup>16</a>; <a href="#GMVL16">GMVL16</a>]. Boland et al. (2012) propose weakly supervised pattern induction to identify references in social science publications [<a href="#BREM12">BREM12</a>]. Ghavimi et al. (2016) propose a semi automatic approach for detecting dataset references for social science texts [<a href="#GML+16">GML<sup>+</sup>16</a>; <a href="#GMVL16">GMVL16</a>]. Dataset extraction is a challenging task because of the inconsistency and wide range of dataset mention styles in research publications [<a href="#GMVL16">GMVL16</a>].</p>
<h2 id="data-analysis">Data Analysis</h2>
<p>The first phase of RCC dataset consists of a labeled corpus of 5,000 publications for training set, and additional 100 publications for development set. The RCC organizer keeps a separate corpus of 5,000 publications for evaluation. Each article in the dataset contains full text article and dataset citation labels. The metadata of cited datasets in the corpus are also provided. For research methods and fields, no label information is provided, only SAGE social science research method graph and research fields vocabulary are provided. More details on RCC dataset and competition design can be read on chapter 5.</p>
<h4 id="preprocessing-1">Preprocessing</h4>
<p>In order to reliably access important structures of paper publications, we parse all papers using AllenAI Science Parse (<a href="https://github.com/allenai/science-parse">https://github.com/allenai/science-parse</a>) [<a href="#AGB+18">AGB<sup>+</sup>18</a>]. AllenAI Science Parse reads PDF file, and returns title, authors, abstract, sections, and bibliography (references). Since this parser utilizes machine learning models to parse PDF file, the parsing results may not be 100% accurate. Furthermore, this parser is unable to parse scan copy of old publication. In the situation where we are unable to access parsed fields, we fall back to the given text files.</p>
<h4 id="mention-analysis">Mention Analysis</h4>
<p>There are 5,499 and 123 dataset citations in training and development set respectively. Among these citations, 320 citations in training set and 6 citations in development set do not have mentions information. We analyze the paper sections where the dataset mentions commonly occur. Table <a href="#user-content-tab_train_top_sections">10.1</a> and <a href="#user-content-tab_dev_top_sections">10.2</a> show top 12 most common sections mentioning dataset in training and development set. The tables suggest that abstract, reference titles, discussion, results, and methods are the most common sections where the dataset mentions occur. We exploit reference titles for dataset extraction.</p>
<p><a name="tab_train_top_sections">Table 10.1</a>: Top 12 Sections Mentioning Datasets in Training Set</p>
<table>
<thead>
<tr class="header">
<th>Section Header</th>
<th style="text-align: right;">Mention Frequency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Abstract</td>
<td style="text-align: right;">2,548</td>
</tr>
<tr class="even">
<td>Reference Titles</td>
<td style="text-align: right;">1,997</td>
</tr>
<tr class="odd">
<td>Discussion</td>
<td style="text-align: right;">1,390</td>
</tr>
<tr class="even">
<td>Results</td>
<td style="text-align: right;">836</td>
</tr>
<tr class="odd">
<td>Methods</td>
<td style="text-align: right;">804</td>
</tr>
<tr class="even">
<td>Introduction</td>
<td style="text-align: right;">530</td>
</tr>
<tr class="odd">
<td>Statistical Analysis</td>
<td style="text-align: right;">285</td>
</tr>
<tr class="even">
<td>Comment</td>
<td style="text-align: right;">279</td>
</tr>
<tr class="odd">
<td>Acknowledgements</td>
<td style="text-align: right;">261</td>
</tr>
<tr class="even">
<td>Materials and Methods</td>
<td style="text-align: right;">254</td>
</tr>
<tr class="odd">
<td>Study Population</td>
<td style="text-align: right;">227</td>
</tr>
<tr class="even">
<td>Data</td>
<td style="text-align: right;">214</td>
</tr>
</tbody>
</table>
<p><a name="tab_dev_top_sections">Table 10.2</a>: Top 12 Sections Mentioning Datasets in Development Set</p>
<table>
<thead>
<tr class="header">
<th>Section Header</th>
<th style="text-align: right;">Mention Frequency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Abstract</td>
<td style="text-align: right;">78</td>
</tr>
<tr class="even">
<td>Reference Titles</td>
<td style="text-align: right;">37</td>
</tr>
<tr class="odd">
<td>Discussion</td>
<td style="text-align: right;">19</td>
</tr>
<tr class="even">
<td>Introduction</td>
<td style="text-align: right;">14</td>
</tr>
<tr class="odd">
<td>Results</td>
<td style="text-align: right;">12</td>
</tr>
<tr class="even">
<td>Statistical Analyses</td>
<td style="text-align: right;">9</td>
</tr>
<tr class="odd">
<td>Methods</td>
<td style="text-align: right;">8</td>
</tr>
<tr class="even">
<td>Ethics</td>
<td style="text-align: right;">7</td>
</tr>
<tr class="odd">
<td>Population</td>
<td style="text-align: right;">7</td>
</tr>
<tr class="even">
<td>Population Impact</td>
<td style="text-align: right;">7</td>
</tr>
<tr class="odd">
<td>Price</td>
<td style="text-align: right;">7</td>
</tr>
<tr class="even">
<td>2.1 Data</td>
<td style="text-align: right;">5</td>
</tr>
</tbody>
</table>
<h4 id="citation-analysis">Citation Analysis</h4>
<p>We build citation network from training set. Each node in the network is a paper publication, and an edge between two node <img src="https://latex.codecogs.com/svg.latex?A" title="A" /> and <img src="https://latex.codecogs.com/svg.latex?B" title="B" /> is generated if a paper <img src="https://latex.codecogs.com/svg.latex?A" title="A" /> cites paper <img src="https://latex.codecogs.com/svg.latex?B" title="B" />. Table <a href="#user-content-tab_network_stats">10.3</a> shows the statistics of the citation network.</p>
<p><a name="tab_network_stats">Table 10.3</a>: Statistics of Citation Network</p>
<table>
<tbody>
<tr class="odd">
<td>Number of nodes</td>
<td style="text-align: right;">5,000</td>
</tr>
<tr class="even">
<td>Number of edges</td>
<td style="text-align: right;">1,222</td>
</tr>
<tr class="odd">
<td>Network density</td>
<td style="text-align: right;">0.0098%</td>
</tr>
</tbody>
</table>
<p>Initially, we propose an approach utilizing citation network based on an intuition that datasets, research methods, and research fields are shared by: 1) same or similar issues, 2) same or similar context, 3) same or similar authors and communities, 4) same or similar metrics used in the publication. However, based on table <a href="#user-content-tab_network_stats">10.3</a>, we learn that exploring rich context using paper-paper citation network is not viable at this stage because most papers listed in publications’ bibliography are not available in the training set, and therefore, paper-paper citation network becomes very sparse with many unknown information. Figure <a href="#fig_citation_graph">10.1</a> shows the visualization of citation network. As we can see in the citation network visualization, most papers only have one edge, although the average number of papers in bibliography list is 34.5. Furthermore, only 1,466 out of 5,000 publications are not isolated (having at least one bibliography paper in training data). Due to this reason, we drop our idea on utilizing paper-paper citation network at this stage. Nevertheless, we believe that bibliography contains important signals and information about datasets, and research fields.</p>
<p><img src="citation_graph.png" alt="image" /> <a name="fig_citation_graph">Figure 10.1</a>: Citation Network from Training Data. Green nodes are publications with dataset citations, and red nodes are publication without dataset citations. Isolated nodes are not visualized.</p>
<h2 id="methods-3">Methods</h2>
<p>In this section, we describe our approach for RCC tasks: dataset extraction, research methods identification, and research fields identification.</p>
<h4 id="dataset-extraction-2">Dataset Extraction</h4>
<p>We employ a pipeline of two subtasks for dataset extraction: dataset detection, followed by dataset recognition. The goal of dataset detection is to detect whether a publication cites a dataset or not. This first subtask helps us to quickly filter out non-dataset publications. After the first subtask, we mine dataset mentions for the remaining publications in dataset recognition subtask.</p>
<p>For dataset detection, we utilize paper title in bibliography (reference list) combined with explicit research methods mentions to detect whether a publication citing a dataset or not. Explicit research methods mentions are determined based on exact match between paper title and SAGE research methods vocabulary. We train an SVM classifier using explicit research method mentions and n-gram features from paper titles in bibliography. We use the SVM classifier to classify each publication, if the classifier gives positive label, then we proceed to dataset recognition subtask, otherwise we ignore the publication.</p>
<p>For dataset recognition, we use an implicit entity linking approach. We start with the Naive Bayes model, which can be regarded as a standard information retrieval baseline, and entity indicative weighting strategy is used to improve the model. In order to calculate the word distribution of each dataset, we represent each dataset using its title, dataset mentions (provided in the training set), and dataset relevant sentences, filtered from the relevant publications using the rule based approach proposed in [<a href="#GMVL16">GMVL16</a>]. All these texts related to a particular dataset are considered as a single string, and we calculate the word distribution as follows. Let <img src="https://latex.codecogs.com/svg.latex?$\textbf{w}$" title="$\textbf{w}$" /> be the set of words in a dataset. In our problem setting, we assume the dataset prior probability <img src="https://latex.codecogs.com/svg.latex?p(d)" title="p(d)" /> to be uniform. The probability of dataset <img src="https://latex.codecogs.com/svg.latex?d" title="d" /> given <img src="https://latex.codecogs.com/svg.latex?$$w$&space;\in&space;\textbf{w}$" title="$$w$ \in \textbf{w}$" /> is:</p>
<!-- $$\begin{split}
    p(d|\textbf{w}) & \propto \prod _{w \in \textbf{w}} p(w|d) \\
    & = \prod _{w \in \textbf{w}} \frac{f(d,w) + \gamma }{ \sum_{w'} f(d,w') + |W| \gamma}
\end{split}$$ -->
<p><a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;p(d|\textbf{w})&space;&&space;\propto&space;\prod&space;_{w&space;\in&space;\textbf{w}}&space;p(w|d)&space;\\&space;&&space;=&space;\prod&space;_{w&space;\in&space;\textbf{w}}&space;\frac{f(d,w)&space;&plus;&space;\gamma&space;}{&space;\sum_{w'}&space;f(d,w')&space;&plus;&space;|W|&space;\gamma}&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\begin{align*}&space;p(d|\textbf{w})&space;&&space;\propto&space;\prod&space;_{w&space;\in&space;\textbf{w}}&space;p(w|d)&space;\\&space;&&space;=&space;\prod&space;_{w&space;\in&space;\textbf{w}}&space;\frac{f(d,w)&space;&plus;&space;\gamma&space;}{&space;\sum_{w'}&space;f(d,w')&space;&plus;&space;|W|&space;\gamma}&space;\end{align*}" title="\begin{align*} p(d|\textbf{w}) & \propto \prod _{w \in \textbf{w}} p(w|d) \\ & = \prod _{w \in \textbf{w}} \frac{f(d,w) + \gamma }{ \sum_{w'} f(d,w') + |W| \gamma} \end{align*}" /></a></p>
<p>where <img src="https://latex.codecogs.com/svg.latex?f(d,&space;w)" title="f(d, w)" /> is the number of co-occurrences of word <img src="https://latex.codecogs.com/svg.latex?w" title="w" /> with entity <img src="https://latex.codecogs.com/svg.latex?d" title="d" />, <img src="https://latex.codecogs.com/svg.latex?\gamma" title="\gamma" /> is the smoothing parameter, and <img src="https://latex.codecogs.com/svg.latex?|W|" title="|W|" /> is the vocabulary size. For each dataset <img src="https://latex.codecogs.com/svg.latex?d" title="d" />, we derive <img src="https://latex.codecogs.com/svg.latex?f(d,&space;w)" title="f(d, w)" /> by the count of <img src="https://latex.codecogs.com/svg.latex?w" title="w" /> occurrences in the text extracted for each dataset. In order to stress more priority for dataset indicative words, we improved the final objective function of our model as follows:</p>
<!-- $$ln(p(d|\textbf{w})) \propto \sum _{w \in \textbf{w}} \beta(w) * ln(p(w|d))$$ -->
<p><a href="https://www.codecogs.com/eqnedit.php?latex=$$ln(p(d|\textbf{w}))&space;\propto&space;\sum&space;_{w&space;\in&space;\textbf{w}}&space;\beta(w)&space;*&space;ln(p(w|d))$$" target="_blank"><img src="https://latex.codecogs.com/svg.latex?$$ln(p(d|\textbf{w}))&space;\propto&space;\sum&space;_{w&space;\in&space;\textbf{w}}&space;\beta(w)&space;*&space;ln(p(w|d))$$" title="$$ln(p(d|\textbf{w})) \propto \sum _{w \in \textbf{w}} \beta(w) * ln(p(w|d))$$" /></a></p>
<p>where <img src="https://latex.codecogs.com/svg.latex?\beta(w)" title="\beta(w)" /> is the entity-indicative weight for word <img src="https://latex.codecogs.com/svg.latex?w" title="w" />. This weight <img src="https://latex.codecogs.com/svg.latex?\beta(w)" title="\beta(w)" /> is added as an exponent to the term <img src="https://latex.codecogs.com/svg.latex?p(w|d)" title="p(w|d)" />. <img src="https://latex.codecogs.com/svg.latex?\beta(w)" title="\beta(w)" /> is calculated as:</p>
<!-- $$\beta(w) = log(1 + E / df(w))$$ -->
<p><a href="https://www.codecogs.com/eqnedit.php?latex=$$\beta(w)&space;=&space;log(1&space;&plus;&space;E&space;/&space;df(w))$$" target="_blank"><img src="https://latex.codecogs.com/svg.latex?$$\beta(w)&space;=&space;log(1&space;&plus;&space;E&space;/&space;df(w))$$" title="$$\beta(w) = log(1 + E / df(w))$$" /></a></p>
<p>where <img src="https://latex.codecogs.com/svg.latex?E" title="E" /> is the number of distinct datasets considered and <img src="https://latex.codecogs.com/svg.latex?df(w)" title="df(w)" /> counts the number of datasets with at least one occurrence of <img src="https://latex.codecogs.com/svg.latex?w" title="w" />. This model can be trained efficiently, and training the model on RCC dataset needs approximately 5 minutes.</p>
<p>Then for a given unseen publication, we use same rule based approach [<a href="#GMVL16">GMVL16</a>] to filter a few relevant sentences, and datasets are ranked by <img src="https://latex.codecogs.com/svg.latex?ln(p(d|w))" title="ln(p(d|w))" /> to select the most suitable datasets. In order to select exact datasets related to particular publication, we select top 10 datasets ranked using above approach. And then the confidence probability related to the top 10 datasets are normalized and select the datasets with the normalized probability higher than a predefined threshold value. We return the entity indicative words as relevant dataset mentions.</p>
<h4 id="research-methods-identification">Research Methods Identification</h4>
<p>Since we do not have labeled training data for this task, we use explicit research method mentions (based on exact match with SAGE research methods vocabulary) in a publication as weak signals on research methods used in the publication. When these mentions frequently appear in a publication, there is a high chance that this publication is using these particular research methods.</p>
<p>Based on this intuition, we generate training set for research method classification utilizing sentences that explicitly mention research method in a publication. Publication title and the sentences mentioning research method serve as context information of a specific research method. In order to reduce noisy weak signals, we apply minimum support of three sentences in a publication. We exclude research methods which only being mentioned one or two times in a publication. We also exclude research methods that only being mentioned in less than 10 different publications from the training set. Finally, we have 133 research methods having sufficient context information for training data. This number is 20.18% of 659 research methods in SAGE research method graph.</p>
<p>We use the training data to train logistic regression classifier to classify research methods from publication title and sentences. We utilize n-gram features from publication title and sentences for the classifier. We apply the logistic regression classifier to recommend top 3 research methods based on logistic regression probability score.</p>
<p>This approach can be extended by utilizing research method graph to expand the context. Context information does not only comes from sentences in publication, but also comes from related research methods as well as broader concept information. By using this information, we can potentially expand to more than 133 research methods and perform more accurate prediction.</p>
<h4 id="research-fields-identification">Research Fields Identification</h4>
<p>Similar to research methods identification, this task does not have labeled training data. We only have access to list of SAGE research fields. SAGE research fields are organized hierarchically into three levels, namely L1, L2, and L3, for example: Soc-2-4 (<em>kinship</em>) is under Soc (<em>sociology</em>) in L1, and under Soc-2 (<em>anthropology</em>) in L2.</p>
<p>To gain more understanding about the characteristic of each field, we crawl top search results from SAGE Knowledge<a href="http://sk.sagepub.com/browse/"><sup>2</sup></a>. From the search result snippets, we collect information such as title and abstract on various publications including case, major work, books, handbooks, and dictionary. We exclude video and encyclopedia. Due to sparseness of the SAGE Knowledge, we exclude all research fields with less than 10 search results. In the end, we have samples of 414 L3 research fields under 101 L2 research fields and 10 L1 research fields. This numbers cover 20.87% of 1,984 L3 research fields, and 67.79% of 149 L2 research fields in the list of SAGE research fields. We use this data to train research fields classifiers.</p>
<p>We build three SVM classifiers for L1, L2, and L3 to classify a publication using paper title and abstract. Instead of taking the highest score, we take top-k research fields and perform re-ranking considering agreement among L1, L2, L3. We return a research field if its upper level are also in top ranks. Since level L1 is too general, we only output research fields from L2, and L3. We outline our heuristic to reorder the ranking below:</p>
<ol type="1">
<li><p>Get top-5 L3 research fields, top-4 L2 research fields, and top-3 L1 research fields.</p></li>
<li><p>Assign initial score <img src="https://latex.codecogs.com/svg.latex?v" title="v" /> for each research field based on its ranking.</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=v(f_i)&space;=&space;(K&space;-&space;i)&space;/&space;K" target="_blank"><img src="https://latex.codecogs.com/svg.latex?v(f_i)&space;=&space;(K&space;-&space;i)&space;/&space;K" title="v(f_i) = (K - i) / K" /></a></p>
<p>where <img src="https://latex.codecogs.com/svg.latex?K" title="K" /> is the length of top-k, and <img src="https://latex.codecogs.com/svg.latex?i" title="i" /> is the ranking of a research field <img src="https://latex.codecogs.com/svg.latex?f" title="f" />. For example, research fields in top-5 L3 have initial score of <code>[1, 0.8, 0.6, 0.4, 0.2]</code>, top-4 L2 have initial score of <code>[1, 0.75, 0.5, 0.25]</code>, and top-3 L1 have <code>[1, 0.666, 0.333]</code></p></li>
<li><p>Update the score by multiplying each score with the score of matching research fields at upper level, and <img src="https://latex.codecogs.com/svg.latex?0" title="0" /> otherwise.</p>
<!-- $$score(f_i^l) =
        \begin{cases}
        \prod _{l \in L} v(f^l) & \text{if field matched} \\
        0 & \text{otherwise}
        \end{cases}$$  -->
<p><a href="https://www.codecogs.com/eqnedit.php?latex=score(f_i^l)&space;=&space;\begin{cases}&space;\prod&space;_{l&space;\in&space;L}&space;v(f^l)&space;&&space;\text{if&space;field&space;matched}&space;\\&space;0&space;&&space;\text{otherwise}&space;\end{cases}" target="_blank"><img src="https://latex.codecogs.com/svg.latex?score(f_i^l)&space;=&space;\begin{cases}&space;\prod&space;_{l&space;\in&space;L}&space;v(f^l)&space;&&space;\text{if&space;field&space;matched}&space;\\&space;0&space;&&space;\text{otherwise}&space;\end{cases}" title="score(f_i^l) = \begin{cases} \prod _{l \in L} v(f^l) & \text{if field matched} \\ 0 & \text{otherwise} \end{cases}" /></a></p>
<p>where <img src="https://latex.codecogs.com/svg.latex?L" title="L" /> is the level of research field <img src="https://latex.codecogs.com/svg.latex?f" title="f" /> and its upper levels. Here are examples of score update:</p>
<ul>
<li>Soc-2-4 at rank-2 in L3, Soc-2 at rank-3 in L2, and Soc at rank-1 in L1. In this case, the score of Soc-2-4 is <code>0.8 * 0.5 * 1 = 0.4</code>.</li>
<li>Soc-2-4 at rank-1 in L3, Soc-2 at rank-2 in L2, but Soc is not found in top rank in L1. In this case, the score of Soc-2-4 is <code>0</code>.</li>
</ul></li>
<li><p>Collect score from L2 and L3, and exclude L2 if we see more specific of L2 in top-5 L3.</p></li>
<li><p>Re-rank L2 and L3 research fields based on the score.</p></li>
<li><p>Return research fields having score <img src="https://latex.codecogs.com/svg.latex?>=&amp;space;0.4" title=“&gt;= 0.4” /&gt;.</p></li>
</ol>
<p>To expand to more context from paper list in bibliography section, we also build other three Naive Bayes classifiers for L1, L2, and L3 using paper title feature only. We believe that a publication from a certain field also cites other publications from same or similar fields. For each publication in the bibliography, we apply the same procedure as mentioned above, then we average the score to get top research fields from bibliography. Finally, we combine top research fields from paper titles and abstract with results from bibliography.</p>
<h2 id="experiment-results">Experiment Results</h2>
<p>We discuss our experiment results for each task in this section. We use standard precision, recall, and F1 as evaluation metrics.</p>
<h4 id="dataset-extraction-3">Dataset Extraction</h4>
<p>First, we analyze our experiment for dataset detection subtask comparing Naive Bayes and SVM classifier. Using only paper titles in bibliography and explicit research method mentions, Naive Bayes and SVM classifiers are able to reach 0.88 &amp; 0.92 F1 score respectively. Since SVM outperforms Naive Bayes, we use SVM for our dataset detection module. Table <a href="#user-content-tab_dd_dev_result">10.4</a> shows detail dataset detection results on development set.</p>
<p><a name="tab_dd_dev_result">Table 10.4</a>: Dataset Detection Results on Development Set</p>
<table>
<thead>
<tr class="header">
<th>Classifier</th>
<th style="text-align: right;">Prec.</th>
<th style="text-align: right;">Rec.</th>
<th style="text-align: right;">F1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Naive Bayes</td>
<td style="text-align: right;">0.85</td>
<td style="text-align: right;">0.92</td>
<td style="text-align: right;">0.88</td>
</tr>
<tr class="even">
<td>SVM</td>
<td style="text-align: right;">0.96</td>
<td style="text-align: right;">0.88</td>
<td style="text-align: right;">0.92</td>
</tr>
</tbody>
</table>
<p>To see the impact of performing dataset detection, we test the performance of dataset extraction with and without dataset detection on development set. Table <a href="#user-content-tab_de_dev_result">10.5</a> summarizes the results. As shown in the table, performing dataset detection before extraction significantly improves the dataset extraction on development set.</p>
<p><a name="tab_de_dev_result">Table 10.5</a>: Dataset Extraction Results on Development Set</p>
<table>
<thead>
<tr class="header">
<th>Method</th>
<th style="text-align: right;">Prec.</th>
<th style="text-align: right;">Rec.</th>
<th style="text-align: right;">F1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>No Dataset Detection</td>
<td style="text-align: right;">0.18</td>
<td style="text-align: right;">0.33</td>
<td style="text-align: right;">0.24</td>
</tr>
<tr class="even">
<td>With Dataset Detection</td>
<td style="text-align: right;">0.34</td>
<td style="text-align: right;">0.30</td>
<td style="text-align: right;">0.32</td>
</tr>
</tbody>
</table>
<p><a name="tab_de_test_result">Table 10.6</a>: Dataset Extraction Result on Test Set</p>
<table>
<thead>
<tr class="header">
<th>Dataset</th>
<th style="text-align: right;">Prec.</th>
<th style="text-align: right;">Rec.</th>
<th style="text-align: right;">F1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Test Set (phase1)</td>
<td style="text-align: right;">0.17</td>
<td style="text-align: right;">0.10</td>
<td style="text-align: right;">0.13</td>
</tr>
</tbody>
</table>
<p>Table <a href="#user-content-tab_de_test_result">10.6</a> shows dataset extraction performance on test set (phase 1). The significant drop from development set result suggests that the test set might have different distribution compare to the training and development set. It might also contain dataset citations that are never been seen in training set. As mentioned in chapter 5, the test set contains new data source, non-open access journals from Sage publications which are not available in training and development set. It would be better to evaluate open access publications and non-open access publications separately so that we can have better understanding on the characteristics on each source in test set.</p>
<h4 id="research-methods-identification-1">Research Methods Identification</h4>
<p>We only consider Naive Bayes and Logistic Regression classifiers for research method identification because they naturally outputs probability score. We perform 5-fold cross validation to evaluate classification performance, and the result can be seen in table <a href="#user-content-tab_rmethods_5cv">10.7</a>. Logistic regression classifier outperforms Naive Bayes with 0.86 F1 score in classifying 133 research methods.</p>
<p><a name="tab_rmethods_5cv">Table 10.7</a>: F1 Score for Research Method Classification</p>
<table>
<thead>
<tr class="header">
<th>Classifier</th>
<th style="text-align: right;">F1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Naive Bayes</td>
<td style="text-align: right;">0.55</td>
</tr>
<tr class="even">
<td>Logistic Regression</td>
<td style="text-align: right;">0.86</td>
</tr>
</tbody>
</table>
<h4 id="research-fields-identification-1">Research Fields Identification</h4>
<p>We perform 5-fold cross validation to evaluate our classifiers to classify L1, L2, and L3 research fields. Table <a href="#user-content-tab_rfields_pub_5cv">10.8</a> shows the results using n-gram features from paper title and abstract, whereas table <a href="#user-content-tab_rfields_rt_5cv">10.9</a> shows the results using n-gram features from title only. Naive Bayes tends to perform slightly better on L3 research fields where we have large number of research field labels. We decide to use SVM for research field identification on publication level because SVM is generally better than Naive Bayes. On the other hand, we decide to use Naive Bayes for research field identification on bibliography level because Naive Bayes prefer to have more accurate L2 and L3 research fields.</p>
<p><a name="tab_rfields_pub_5cv">Table 10.8</a>: F1 Score for Research Field Classification on Publication Level using Paper Title and Abstract</p>
<table>
<thead>
<tr class="header">
<th>Classifier</th>
<th style="text-align: right;">L1</th>
<th style="text-align: right;">L2</th>
<th style="text-align: right;">L3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Naive Bayes</td>
<td style="text-align: right;">0.78</td>
<td style="text-align: right;">0.37</td>
<td style="text-align: right;">0.13</td>
</tr>
<tr class="even">
<td>SVM</td>
<td style="text-align: right;">0.82</td>
<td style="text-align: right;">0.38</td>
<td style="text-align: right;">0.12</td>
</tr>
</tbody>
</table>
<p><a name="tab_rfields_rt_5cv">Table 10.9</a>: F1 Score for Research Field Classification on Bibliography Level using Paper Title Only</p>
<table>
<thead>
<tr class="header">
<th>Classifier</th>
<th style="text-align: right;">L1</th>
<th style="text-align: right;">L2</th>
<th style="text-align: right;">L3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Naive Bayes</td>
<td style="text-align: right;">0.80</td>
<td style="text-align: right;">0.35</td>
<td style="text-align: right;">0.12</td>
</tr>
<tr class="even">
<td>SVM</td>
<td style="text-align: right;">0.81</td>
<td style="text-align: right;">0.35</td>
<td style="text-align: right;">0.11</td>
</tr>
</tbody>
</table>
<h2 id="lesson-learned">Lesson Learned</h2>
<p>Extraction of research datasets, associated research methods and fields from social science publication is challenging, yet an important problem to organize social science publications. We have described our approach for the RCC challenge, and table <a href="#user-content-tab_summary">10.10</a> summarizes our approach. Beside publication content such as paper titles, abstract, full text, our approach also leverages on the information from bibliography. Furthermore, we also collect external information from SAGE Knowledge to get more information about research fields.</p>
<p><a name="tab_summary">Table 10.10</a>: Summary of Our Approach</p>
<table>
<colgroup>
<col style="width: 26%" />
<col style="width: 73%" />
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Features (n-gram)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Dataset extraction</strong></td>
<td></td>
</tr>
<tr class="even">
<td>SVM for dataset detection</td>
<td>paper titles in bibliography and explicit research method mentions</td>
</tr>
<tr class="odd">
<td>Implicit entity linking</td>
<td>paper title and full text</td>
</tr>
<tr class="even">
<td><strong>Research method identification</strong></td>
<td></td>
</tr>
<tr class="odd">
<td>Logistic regression</td>
<td>paper title, abstract, and full text</td>
</tr>
<tr class="even">
<td><strong>Research field identification</strong></td>
<td></td>
</tr>
<tr class="odd">
<td>SVM (on paper)</td>
<td>paper title and abstract</td>
</tr>
<tr class="even">
<td>Naive Bayes (on bibliography)</td>
<td>paper titles in bibliography</td>
</tr>
</tbody>
</table>
<p>Apart from F1 score on 5-fold cross validation, we have no good way to evaluate research method and research field identification without ground truth label. Our methods are unable to automatically extract and recognize new datasets, research methods, and fields. An extension to automatically handle such cases using advance Natural Language Processing (NLP) approach is a promising future direction. All RCC finalists have shown that NLP approaches worked well on dataset extraction. Readers are encouraged to read their solutions on chapter 6, 7, 8, and 9.</p>
<p>Our model did not perform well in test set, and unable to advance to the second phase. Nevertheles, we recommend to use our approach as a baseline method as it is simple, efficient, and fast to train. From this competition, we have learned that lacks of labelled training data is a huge challenge, and it directs us to other external resources (i.e., SAGE Knowledge) as proxy for our label. We are also interested in exploring more advanced information extraction approaches on the RCC datasets. Another challenge is data sparsity. Although we see many papers listed in bibliography, lacks of access to these publication make us difficult to exploit citation network. Expanding labeled data from the list of papers in bibliography will be very beneficial to improve rich context of paper publications and datasets.</p>
<h2 id="acknowledgments-4">Acknowledgments</h2>
<p>This research is supported by the National Research Foundation, Prime Minister’s Office, Singapore under its International Research Centres in Singapore Funding Initiative.</p>
<h2 id="references-6">References</h2>
<ul>
<li>[<a name="PM04">PM04</a>] Fuchun Peng and Andrew McCallum (2004): Accurate information extraction from research papers using conditional random fields. In HLT-NAACL.</li>
<li>[<a name="Het08">Het08</a>] Erik Hetzner (2008): A simple method for citation metadata extraction using hidden markov models. In JCDL.</li>
<li>[<a name="BREM12">BREM12</a>] Katarina Boland, Dominique Ritze, Kai Eckert, and Brigitte Mathiak (2012): Identifying references to datasets in publications. In TPDL.</li>
<li>[<a name="APBM14">APBM14</a>] Sam Anzaroot, Alexandre Passos, David Belanger, and Andrew McCallum (2014): Learning soft linear constraints with application to citation field extraction. In ACL.</li>
<li>[<a name="NCKL15">NCKL15</a>] Viet Cuong Nguyen, Muthu Kumar Chandrasekaran, Min-Yen Kan, and Wee Sun Lee (2015): Scholarly document information extraction using extensible features for efficient higher order semi-crfs. In JCDL.</li>
<li>[<a name="GML+16">GML<sup>+</sup>16</a>] Behnam Ghavimi, Philipp Mayr, Christoph Lange, Sahar Vahdati, and Sören Auer (2016a): A semi-automatic approach for detecting dataset references in social science texts. Inf. Services and Use, 36:171–187.</li>
<li>[<a name="GMVL16">GMVL16</a>] Behnam Ghavimi, Philipp Mayr, Sahar Vahdati, and Christoph Lange (2016b): Identifying and improving dataset references in social sciences full texts. In ELPUB.</li>
<li>[<a name="SBP+16">SBP<sup>+</sup>16</a>] Mayank Singh, Barnopriyo Barua, Priyank Palod, Manvi Garg, Sidhartha Satapathy, Samuel Bushi, Kumar Ayush, Krishna Sai Rohith, Tulasi Gamidi, Pawan Goyal, and Animesh Mukherjee (2016): Ocr++: A robust framework for information extraction from scholarly articles. In COLING.</li>
<li>[<a name="ADR+17">ADR<sup>+</sup>17</a>] Isabelle Augenstein, Mrinal Das, Sebastian Riedel, Lakshmi Vikraman, and Andrew McCallum (2017): Semeval 2017 task 10: Scienceie - extracting keyphrases and relations from scientific publications. In SemEval@ACL.</li>
<li>[<a name="AGJ+17">AGJ<sup>+</sup>17</a>] Dong An, Liangcai Gao, Zhuoren Jiang, Runtao Liu, and Zhi Tang (2017): Citation metadata extraction via deep neural network-based segment sequence labeling. In CIKM.</li>
<li>[<a name="AS17">AS17</a>] Isabelle Augenstein and Anders Søgaard (2017): Multi-task learning of keyphrase boundary classification. In ACL.</li>
<li>[<a name="LOH17">LOH17</a>] Yi Luan, Mari Ostendorf, and Hannaneh Hajishirzi (2017): Scientific information extraction with semi-supervised neural tagging. In EMNLP.</li>
<li>[<a name="ACK18">ACK18</a>] Danny Rodrigues Alves, Giovanni Colavizza, and Frédéric Kaplan (2018): Deep reference mining from scholarly literature in the arts and humanities. In Front. Res. Metr. Anal.</li>
<li>[<a name="AGB+18">AGB<sup>+</sup>18</a>] Waleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz Beltagy, Miles Crawford, Doug Downey, Jason Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu Ha, Rodney Kinney, Sebastian Kohlmeier, Kyle Lo, Tyler Murray, Hsu-Han Ooi, Matthew E. Peters, Joanna Power, Sam Skjonsberg, Lucy Lu Wang, Chris Wilhelm, Zheng Yuan, Madeleine van Zuylen, and Oren Etzioni (2018): Construction of the literature graph in semantic scholar. In NAACL-HTL.</li>
<li>[<a name="NJM18">NJM18</a>] Zara Nasar, S. W. Jaffry, and Muhammad Kamran Malik (2018): Information extraction from scientific articles: a survey. Scientometrics, 117:1931–1990.</li>
</ul>
<h2 id="appendix-technical-documentation">Appendix: Technical Documentation</h2>
<p>Source codes to run and replicate our experiments are available at <a href="https://github.com/LARC-CMU-SMU/coleridge-rich-context-larc">https://github.com/LARC-CMU-SMU/coleridge-rich-context-larc</a>.</p>
<hr />
<h1 id="chapter-11---finding-datasets-in-publications-the-university-of-syracuse-approach">Chapter 11 - Finding datasets in publications: The University of Syracuse approach</h1>
<h1 id="abstract-2">Abstract</h1>
<p>Datasets are critical for scientific research, playing a role in replication, reproducibility, and efficiency. Researchers have recently shown that datasets are becoming more important for science to function properly, even serving as artifacts of study themselves. However, citing datasets is not a common or standard practice in spite of recent efforts by data repositories and funding agencies. This greatly affects our ability to track their usage and importance. A potential solution to this problem is to automatically extract dataset mentions from scientific articles. In this work, we propose to achieve such extraction by using a neural network based on a BiLSTM-CRF architecture. Our method achieves <span class="math inline"><em>F</em><sub>1</sub> = 0.885</span> in social science articles released as part of the Rich Context Dataset. We discuss future improvements to the model and applications beyond social sciences.</p>
<h1 id="introduction-8">Introduction</h1>
<p>Science is fundamentally an incremental discipline that depends on previous scientist’s work. Datasets form an integral part of this process and therefore should be shared and cited as any other scientific output. This ideal is far from reality: the credit that datasets currently receive does not correspond to their actual usage<span class="citation" data-cites="datarank">[@datarank]</span>. One of the issues is that there is no standard for citing datasets, and even if they are cited, they are not properly tracked by major scientific indices. Interestingly, while datasets are still used and mentioned in articles, we lack methods to extract such mentions and properly reconstruct dataset citations. The Rich Context Competition challenge aims at closing this gap by inviting scientists to produce automated dataset mention and linkage detection algorithms. In this article, we detail our proposal to solve the dataset mention step. Our approach attempts to provide a first approximation to better give credit and keep track of datasets and their usage.</p>
<p>The problem of dataset extraction has been explored before. <span class="citation" data-cites="ghavimiIdentifyingImprovingDataset2016">@ghavimiIdentifyingImprovingDataset2016</span> and <span class="citation" data-cites="ghavimiSemiautomaticApproachDetecting2017">@ghavimiSemiautomaticApproachDetecting2017</span> use a relatively simple tf-idf representation with cosine similarity for matching dataset identification in social science articles. Their method consists of four major steps: preparing a curated dictionary of typical mention phrases, detecting dataset references, and ranking matching datasets based on cosine similarity of tf-idf representations. This approach achieved a relatively high performance, with <span class="math inline"><em>F</em><sub>1</sub> = 0.84</span> for mention detection and <span class="math inline"><em>F</em><sub>1</sub> = 0.83</span>, for matching. <span class="citation" data-cites="singhalDataExtractMining2013">@singhalDataExtractMining2013</span> proposed a method using normalized Google distance to screen whether a term is in a dataset. However, this method relies on external services and is not computational efficient. They achieve a good <span class="math inline"><em>F</em><sub>1</sub> = 0.85</span> using Google search and <span class="math inline"><em>F</em><sub>1</sub> = 0.75</span> using Bing. A somewhat similar project was proposed by <span class="citation" data-cites="luDatasetSearchEngine2012">@luDatasetSearchEngine2012</span>. They built a dataset search engine by solving the two challenges: identification of the dataset and association to a URL. They build a dataset of 1000 documents with their URLs, containing 8922 words or abbreviations representing datasets. They also build a web-based interface. This shows the importance of dataset mention extraction and how several groups have tried to tackle the problem.</p>
<p>In this article, we describe a method for extracting dataset mentions based on a deep recurrent neural network. In particular, we used a Bidirectional Long short-term Memory (BiLSTM) sequence to sequence model paired with a Conditional Random Field (CRF) inference mechanism. The architecture is similar to chapter 6, but we only focus on the detection of dataset mentions. We tested our model on a novel dataset produced for the Rich Context Competition challenge. We achieve a relatively good performance of <span class="math inline"><em>F</em><sub>1</sub> = 0.885</span>. We discuss the limitations of our model.</p>
<h1 id="the-dataset">The dataset</h1>
<p>The Rich Context Dataset challenge was proposed by the New York University’s Coleridge Initiative <span class="citation" data-cites="richtextcompetition">[@richtextcompetition]</span>. The challenge comprised several phases, and participants moved through the phases depending on their performance. We only analyze data of the first phase. This phase contained a list of datasets and a labeled corpus of around 5K publications. Each publication was labeled indicating whether a dataset was mentioned within it and which part of the text mentioned it. The challenge used the accuracy for measuring the performance of the competitors and also the quality of the code, documentation, and efficiency.</p>
<p>We adopt the CoNLL 2003 format <span class="citation" data-cites="tjong2003introduction">[@tjong2003introduction]</span> to annotate whether a token is a part of dataset mention. Concretely, we use the tag DS denotes a dataset mention; The B- prefix indicates that the token is the beginning of a dataset mention, the I- prefix indicates the token is inside of dataset mention, and O denotes a token that is not a part of dataset mention. We put each token and its tag (separated by horizontal tab control character) in one line, and use the end of line (\n) control character as separator between sentences (see Table 2.1). The dataset were randomly split by 70%, 15%, 15% for training set, validation set and testing set, respectively.</p>
<p>Table 2.1. Example of a sentence annotated by IOB tagging format.</p>
<table>
<thead>
<tr class="header">
<th>Token</th>
<th>Annotation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>This</td>
<td>O</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td>data</td>
<td>O</td>
</tr>
<tr class="even">
<td>from</td>
<td>O</td>
</tr>
<tr class="odd">
<td>the</td>
<td>O</td>
</tr>
<tr class="even">
<td>Monitoring</td>
<td>B-DS</td>
</tr>
<tr class="odd">
<td>the</td>
<td>I-DS</td>
</tr>
<tr class="even">
<td>Future</td>
<td>I-DS</td>
</tr>
<tr class="odd">
<td>(</td>
<td>O</td>
</tr>
<tr class="even">
<td>MTF</td>
<td>B-DS</td>
</tr>
<tr class="odd">
<td>)</td>
<td>O</td>
</tr>
<tr class="even">
<td>\n</td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="the-proposed-method">The Proposed Method</h1>
<h2 id="overall-view-of-the-architecture">Overall view of the architecture</h2>
<p>In this section, we propose a model for detecting mentions based on a BiLSTM-CRF architecture. At a high level, the model uses a sequence-to-sequence recurrent neural network that produces the probability of whether a token belongs to a dataset mention. The CRF layer takes those probabilities and estimates the most likely sequence based on constrains between label transitions (e.g., mention–to–no-mention–to-mention has low probability). While this is a standard architecture for modeling sequence labeling, the application to our particular dataset and problem is new.</p>
<p>We now describe in more detail the choices of word representation, hyper-parameters, and training parameters. A schematic view of the model is in Figure 3.1 and the components are as follows:</p>
<ol type="1">
<li><p>Character encoder layer: treat a token as a sequence of characters and encode the characters by using a bidirectional LSTM to get a vector representation.</p></li>
<li><p>Word embedding layer: mapping each token into fixed sized vector representation by using a pre-trained word vector.</p></li>
<li><p>BiLSTM layer: make use of Bidirectional LSTM network to capture the high level representation of the whole token sequence input.</p></li>
<li><p>Dense layer: project the output of the previous layer to a low dimensional vector representation of the the distribution of labels.</p></li>
<li><p>CRF layer: find the most likely sequence of labels.</p></li>
</ol>
<figure>
<embed src="combined_images/Figure2.1.pdf" style="width:80.0%" /><figcaption>Figure 3.1. Network Architecture of BiLSTM-CRF network</figcaption>
</figure>
<p>Figure 3.1. Network Architecture of BiLSTM-CRF network</p>
<h2 id="character-encoder">Character encoder</h2>
<p>Similar to the bag of words assumption, a word could be composed of characters sampled from a bag of characters. Previous research <span class="citation" data-cites="santos2014learning jozefowicz2016exploring">[@santos2014learning; @jozefowicz2016exploring]</span> has shown that the use of character-level embedding could benefit multiple NLP-related tasks. In order to use character-level information, we break down a word into a sequence of characters, then build a vocabulary of characters. We initialize the character embedding weights using the vocabulary size of a pre-defined embedding dimension, then update the weights during the training process to get the fixed-size character embedding. Next, we feed a sequence of the character embedding into an encoder (a bidirectional LSTM network) to produce a vector representation of a word. By using a character encoder, we can solve the out-of-vocabulary problem for pre-trained word embedding, as every word could be composed of characters.</p>
<h2 id="word-embedding">Word Embedding</h2>
<p>The word embedding layer is responsible for storing and retrieving the vector representation of words. Concretely, the word embedding layer contains a word embedding matrix <span class="math inline"><em>M</em><sup><em>t</em><em>k</em><em>n</em></sup> ∈ ℝ<sup>|<em>V</em>|<em>d</em></sup></span>, where the <span class="math inline"><em>V</em></span> is the vocabulary of the tokens and the <span class="math inline"><em>d</em></span> is the size of the embedding vector. The embedding matrix was initialized by a pre-trained GloVe vectors <span class="citation" data-cites="pennington2014glove">[@pennington2014glove]</span>, and updated by learning from the data. In order to retrieve from the embedding matrix, we first convert a given sentence into a sequence of tokens, then for each token we lookup the embedding matrix to get its vector representation. Finally, we get a sequence of vectors as input for the encoder layer.</p>
<h2 id="lstm">LSTM</h2>
<p>The Recurrent Neural Network (RNN) is a type of artificial neural network which takes the output of previous step as input of the current step recurrently. This recurrent nature allows it to learn from sequential data, for example, the text which consists of a sequence of works. RNN could capture contextual information in variable-length sequences in theory but it suffers from gradient exploding/vanishing problems <span class="citation" data-cites="pascanu2013difficulty">[@pascanu2013difficulty]</span>. The Long Short-Term Memory (LSTM) architecture was proposed by <span class="citation" data-cites="hochreiter1997long">@hochreiter1997long</span> to cope with these gradient problems. Similar to standard RNN, the LSTM network also has a repeating module called LSTM cell. The cell remembers information over arbitrary time steps because it allows information to flow along it without change. The cell state is regulated by a forget gate and an input gate which control the proportion of information to forget from a previous time step and to remember for a next time step. Also, there is a output gate controlling the information to flow out of the cell. The LSTM could be defined formally by the following equations:</p>
<p><br /><span class="math display"><em>i</em><sub><em>t</em></sub> = <em>σ</em>(<em>W</em><sub><em>i</em></sub><em>x</em><sub><em>t</em></sub> + <em>W</em><sub><em>i</em></sub><em>h</em><sub><em>t</em> − 1</sub> + <em>b</em><sub><em>i</em></sub>)</span><br /></p>
<p><br /><span class="math display"><em>f</em><sub><em>t</em></sub> = <em>σ</em>(<em>W</em><sub><em>f</em></sub><em>x</em><sub><em>t</em></sub> + <em>W</em><sub><em>f</em></sub><em>h</em><sub><em>t</em> − 1</sub> + <em>b</em><sub><em>f</em></sub>)</span><br /></p>
<p><br /><span class="math display"><em>g</em><sub><em>t</em></sub> = <em>t</em><em>a</em><em>n</em><em>h</em>(<em>W</em><sub><em>g</em></sub><em>x</em><sub><em>t</em></sub> + <em>W</em><sub><em>g</em></sub><em>h</em><sub><em>t</em> − 1</sub> + <em>b</em><sub><em>g</em></sub>)</span><br /></p>
<p><br /><span class="math display"><em>o</em><sub><em>t</em></sub> = <em>σ</em>(<em>W</em><sub><em>o</em></sub><em>x</em><sub><em>t</em></sub> + <em>W</em><sub><em>o</em></sub><em>h</em><sub><em>t</em> − 1</sub> + <em>b</em><sub><em>o</em></sub>)</span><br /></p>
<p><br /><span class="math display"><em>c</em><sub><em>t</em></sub> = <em>f</em><sub><em>t</em></sub>⨂<em>c</em><sub><em>t</em> − 1</sub> + <em>i</em><sub><em>t</em></sub>⨂<em>g</em><sub><em>t</em></sub></span><br /></p>
<p><br /><span class="math display"><em>h</em><sub><em>t</em></sub> = <em>o</em><sub><em>t</em></sub>⨂<em>t</em><em>a</em><em>n</em><em>h</em>(<em>c</em><sub><em>t</em></sub>)</span><br /></p>
<p>where <span class="math inline"><em>x</em><sub><em>t</em></sub></span> is the input at time <span class="math inline"><em>t</em></span>, <span class="math inline"><em>W</em></span> is the weights, <span class="math inline"><em>b</em></span> is the bias. The <span class="math inline"><em>σ</em></span> is the sigmoid function, <span class="math inline">⨂</span> denotes the dot product, <span class="math inline"><em>c</em><sub><em>t</em></sub></span> is the LSTM cell state at time <span class="math inline"><em>t</em></span> and <span class="math inline"><em>h</em><sub><em>t</em></sub></span> is hidden state at time <span class="math inline"><em>t</em></span>. The <span class="math inline"><em>i</em><sub><em>t</em></sub></span>, <span class="math inline"><em>f</em><sub><em>t</em></sub></span>, <span class="math inline"><em>o</em><sub><em>t</em></sub></span> and <span class="math inline"><em>g</em><sub><em>t</em></sub></span> are named as input, forget, output and cell gates respectively.</p>
<p>LSTM can learn from the previous steps, which is the left context if we feed the sequence from left to right. However, the information in the right context is also important for some tasks. The bidirectional LSTM <span class="citation" data-cites="graves2013speech">[@graves2013speech]</span> satisfies this information need by using two LSTMs. Concretely, one LSTM layer was fed by a forward sequence and the other by a backward sequence. The final hidden states of each LSTM were concatenated to model the left and right contexts</p>
<p><br /><span class="math display">$$h_{t}=[\overrightarrow{h_{t}}\varoplus\overleftarrow{h_{t}}]$$</span><br /></p>
<p>Finally, the outcomes of the states are taken by a Conditional Random Field (CRF) layer that takes into account the transition nature of the beginning, intermediate, and ends of mentions. For a reference of CRF, refer to <span class="citation" data-cites="lafferty2001conditional">[@lafferty2001conditional]</span></p>
<h1 id="results-1">Results</h1>
<p>In this work, we wanted to propose a model for the Rich Context Competition challenge. We propose a relatively standard architecture based on a BiLSTM-CRF recurrent neural network. We now describe the evaluation metrics, hyper-parameter setting, and the results of this network on the dataset provided by the competition.</p>
<p>For all of our results, we use <span class="math inline"><em>F</em><sub>1</sub></span> as the measure of performance. This measure is the harmonic average of the precision and recall and it is the standard measure used in sequence labeling tasks. This metric varies from 0 to 1, the higher the better. Our method achieved a relatively high <span class="math inline"><em>F</em><sub>1</sub></span> of 0.885 for detecting mentions.</p>
<p>Table 4.1. Model search space and best assignments</p>
<table>
<thead>
<tr class="header">
<th>Hyper-parameter</th>
<th>Search space</th>
<th>Best parameter</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>number of epochs</td>
<td>50</td>
<td>50</td>
</tr>
<tr class="even">
<td>patience</td>
<td>10</td>
<td>10</td>
</tr>
<tr class="odd">
<td>batch size</td>
<td>64</td>
<td>64</td>
</tr>
<tr class="even">
<td>pre-trained word vector size</td>
<td>choice[50, 100, 200,300]</td>
<td>100</td>
</tr>
<tr class="odd">
<td>encoder hidden size</td>
<td>300</td>
<td>300</td>
</tr>
<tr class="even">
<td>number of encoder layers</td>
<td>2</td>
<td>2</td>
</tr>
<tr class="odd">
<td>dropout rate</td>
<td>choice[0.0,0.5]</td>
<td>0.5</td>
</tr>
<tr class="even">
<td>learning rate optimizer</td>
<td>adam</td>
<td>adam</td>
</tr>
<tr class="odd">
<td>l2 regularizer</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr class="even">
<td>learning rate</td>
<td>0.001</td>
<td>0.001</td>
</tr>
</tbody>
</table>
<p>We train models using the training data and monitor the performance using the validation data (we stop training if the performance does not improve for the last 10 epochs). We are using the Adam optimizer with learning rate of 0.001 and batch size equal to 64. The hidden size of LSTM for character and word embedding is 80 and 300, respectively. For the regularization methods, and to avoid over-fitting, we use L2 regularization set to 0.01 and we also use dropout rate equal to 0.5. We trained 8 models with a combination of different GloVe vector size (50, 100, 300 and 300) and dropout rate (0.0, 0.5). The hyper-parameter settings are present in Table 4.1.</p>
<p>Table 4.2. Performance of proposed network</p>
<table>
<thead>
<tr class="header">
<th>Models</th>
<th>GloVe size</th>
<th>Dropout rate</th>
<th>Precision</th>
<th>Recall</th>
<th>F1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>m1</td>
<td>50</td>
<td>0.0</td>
<td>0.884</td>
<td>0.873</td>
<td>0.878</td>
</tr>
<tr class="even">
<td>m2</td>
<td>50</td>
<td>0.5</td>
<td>0.877</td>
<td>0.888</td>
<td>0.882</td>
</tr>
<tr class="odd">
<td>m3</td>
<td>100</td>
<td>0.0</td>
<td>0.882</td>
<td>0.871</td>
<td>0.876</td>
</tr>
<tr class="even">
<td>m4</td>
<td>100</td>
<td>0.5</td>
<td>0.885</td>
<td>0.885</td>
<td>0.885</td>
</tr>
<tr class="odd">
<td>m5</td>
<td>200</td>
<td>0.0</td>
<td>0.882</td>
<td>0.884</td>
<td>0.883</td>
</tr>
<tr class="even">
<td>m6</td>
<td>200</td>
<td>0.5</td>
<td>0.885</td>
<td>0.880</td>
<td>0.882</td>
</tr>
<tr class="odd">
<td>m7</td>
<td>300</td>
<td>0.0</td>
<td>0.868</td>
<td>0.886</td>
<td>0.877</td>
</tr>
<tr class="even">
<td>m8</td>
<td>300</td>
<td>0.5</td>
<td>0.876</td>
<td>0.878</td>
<td>0.877</td>
</tr>
</tbody>
</table>
<p>The test performances are reported in Table 4.2. The best model is trained by word vector size 100 and dropout rate 0.5 with <span class="math inline"><em>F</em><sub>1</sub></span> score 0.885 (Table 4.2), and it takes 15 hours 58 minutes for the training on an NVIDIA GTX 1080 Ti GPU in a computer with an Intel Xeon E5-1650v4 3.6 GHz CPU with 128 GB of RAM.</p>
<p>We also found some limitations to the dataset. Firstly, we found that mentions are nested (e.g. HRS, RAND HRS, RAND HRS DATA are linked to the same dataset). The second issue most of the mentions have ambiguous relationships to datasets. In particular, only 17,267 (16.99%) mentions are linked to one dataset, 15,292 (15.04%) mentions are listed to two datasets, and 12,624 (12.42%) are linked to three datasets. If these difficulties are not overcome, then the predictions from the linkage process will be noisy and therefore impossible to tell apart.</p>
<h1 id="conclusion-5">Conclusion</h1>
<p>In this work, we report a high accuracy model for the problem of detecting dataset mentions. Because our method is based on a standard BiLSTM-CRF architecture, we expect that updating our model with recent developments in neural networks would only benefit our results. We also provide some evidence of how difficult we believe the linkage step of the challenge could be if the dataset noise are not lowered.</p>
<p>One of the shortcomings of our approach is that the architecture is lacking some modern features of RNN networks. In particular, recent work has shown that attention mechanisms are important especially when the task requires spatially distant information, such as this one. These benefits could also translate to better linkage. We are exploring new architectures using self-attention and multiple-head attention. We hope to explore these approaches in the near future.</p>
<p>There are number of improvements that we can make in the future. A first improvement is to use non-recurrent neural architectures such as the Transformer which has shown to be faster and a more effective learner compared to recurrent neural networks. Another improvement would be to bootstrap information from other dataset sources such as open access full-text articles from PubMed Open Access Subset. This dataset contains dataset <em>citations</em> <span class="citation" data-cites="datarank">[@datarank]</span>—in contrast to the most common types of citations to publications. The location of this citations within the full-text could be exploited to perform entity recognition. While this would be a somewhat different problem than the one solved in this article, it would still be useful for the goal of tracking dataset usage. In sum, by improving the learning techniques and the dataset size and quality, we could significantly increase the success of finding datasets in publications.</p>
<p>Our proposal, however, is surprisingly effective. Because we have barely modified a general RNN architecture, we expect that our results will generalize relatively well either to the second phase of the challenge or even to other disciplines. We would emphasize, however, that the quality of the dataset has a great deal of room for improvement. Given how important this task is for the whole of science, we should try to strive to improve the quality of these datasets so that techniques like this one can be more broadly applied. The importance of dataset mention and linkage therefore could be fully appreciated by the community.</p>
<h1 id="acknowledgements" class="unnumbered unnumbered">Acknowledgements</h1>
<p>Tong Zeng was funded by the China Scholarship Council #201706190067. Daniel E. Acuna was funded by the National Science Foundation awards #1646763 and #1800956.</p>

<hr />
<h1 id="chapter-12---the-future-of-context">Chapter 12 - The future of context</h1>
<h1 id="the-future-of-ai-in-rich-context">The Future of AI in Rich Context</h1>
<p><strong>Paco Nathan</strong></p>
<p>The setting for Rich Context originated in needs to analyze confidential micro-data for <a href="https://en.wikipedia.org/wiki/Foundations_for_Evidence-Based_Policymaking_Act">evidence-based policymaking</a>. The nature of that work involves collaboration using <a href="http://linkeddata.org/"><em>linked data</em></a>, with strict requirements for data privacy and security, plus provisions for data stewardship and dataset curation. Due to the highly regulated environments, that data cannot be examined outside of its specific use cases. In a world where public search engines crawl and index millions of terabytes, making search results available within milliseconds to anyone with a browser and an Internet connection, the setting for Rich Context may appear utterly alien. Seemingly, a reasonable compromise would be to run queries of sensistive data within their secure environments, and otherwise leave the process unexamined.</p>
<p>However, there’s a broader scope to consider, far beyond the process of managing research projects or curating particular datasets. The great challenges of our time are human in nature – climate change, terrorism, overuse of natural resources, the nature of work, and so on – and these require robust social science to understand their causes and consequences. Effective use of data for social science research depends on understanding how datasets have been produced and how they’ve been used in previous works. That understanding of data provenance is complicated by the fact that research often must link datasets from different data producers, different agencies, different organizations.</p>
<p>Other factors confound this situation. On the one hand, the availability of inexpensive computing resources, ubiquitous connected mobile devices, social networks with global reach, etc., implies that researchers can acquire large, rich datasets. Researchers can also fit statistical models that might have seemed intractably complex merely a decade ago. On the other hand, accumulating important information about datasets, their provenance, and their usage has historically been a manual process. Sharing this kind of information across organizations is difficult in general, and when datasets include confidential data about human subjects it becomes impossible to provide open access to the original data. These issues combine to contribute to a lack of reproducibility and replicability in the study of human behavior, and threaten the legitimacy and utility of social science research.</p>
<p>The problems enumerated above make it difficult for people to understand about data usage, although at the same time they present opportunities for leveraging automation. Consider how one of the major challenges in social science research is search and discovery: the vast majority of data and research results cannot be easily discovered by other researchers. From one perspective, researchers are the users of micro data and its related <a href="https://en.wikipedia.org/wiki/Metadata"><em>metadata</em></a> – in other words, information about the structure of datasets, their provenance, etc. – and those researchers produce outcomes, often in the form of publications. Publications accumulate expertise and nuances about datasets, including the data preparation required, research topics and methodology, what kinds of analyses were attempted and ultimately used, which information within the datasets was most valuable for the results obtained, and so on. These details produced through publications represent <em>metadata about datasets</em>. While the metadata within publications may be relatively unstructured – i.e., not explicitly articulated, nor shared outside of the current project – advances in machine learning provide means to extract metadata from unstructured sources.</p>
<p>The exchange of metadata plays another important role. From the perspective of a data publisher (i.e., an agency) the many concerns about security and data privacy indicate use of <em>tiered access</em> for sensitive data. Datasets which do not contain sensitive data may be made freely available to the public as <a href="https://www.data.gov/"><em>open data</em></a>. Other datasets may require DUAs before researchers can access them. So the data sharing may need to be organized in tiers. Nonetheless, metadata for the private tiers in many cases may still be shared even though the data cannot be linked directly without explicit authorizations and stewardship. So metadata provides a role of exchanging information about sensitive data, in ways that can be accumulated across a broader scope than individual research projects.</p>
<p>The opportunity at hand is to leverage machine learning advances to create feedback loops among the entities involved: researchers, datasets, data publishers, publications, and so on. A new generation of tooling for search and discovery could leverage that to augment researchers: informing them about what datasets are being used, in which research fields, the tools involved, as well as the methods used and findings from the research.</p>
<h2 id="the-case-for-rich-context">The Case for Rich Context</h2>
<p>Consider the two most fundamental workflows within Rich Context, where analysts and other researchers interact among data providers, data stewards, training programs, security audits, etc.:</p>
<ul>
<li><strong>Collaboration and Workspace:</strong> where researchers collaborate within a secured environment, having obtained authorizations via NDAs (non-disclosure agreements), DUAs (data use authorizations), etc.</li>
<li><strong>Data Stewardship:</strong> where data stewards can review and determine whether to approve requests for using the datasets that they curate, and then monitor and report on subsequent usage.</li>
</ul>
<p>These components represent <em>explicitly</em> linked feedback loops among the researchers, projects, datasets, and data stewards. Researchers also use other <em>implicitly</em> linked feedback loops externally to draw from published social science research. Overall, the general category of linked data describes these interactions.</p>
<p>A large body of AI applications leverages linked data. Related R&amp;D efforts have focused mostly on public search engines, e-commerce platforms, and research in life sciences – while in social science research the use of this technology is relatively uncharted territory. Also, given the security and compliance requirements involved with sensitive data, the process of leveraging linked data in social science research takes on nuanced considerations and compels novel solutions.</p>
<p>This area of focus represents the core of Rich Context: the interconnection of point solutions that facilitate research, as explicit feedback loops, along with means to leverage the implicit feedback loops that draw from published research. Making use of AI applications to augment social science research is the goal of Rich Context work, and that interconnection of feedback loops, through a graph, creates a kind of <em>virtuous cycle</em> for metadata – analogous to the famous <a href="https://youtu.be/21EiKfQYZXc">virtuous cycle of data</a> required for AI applications in industry, as described Andrew Ng.</p>
<p>In general, guidance for Rich Context can be drawn from the FAIR<a href="#fn87" class="footnote-ref" id="fnref87" role="doc-noteref"><sup>87</sup></a> data principles for data management and data stewardship in science. The FAIR acronym stands for <em>Findable</em>, <em>Accessible</em>, <em>Interoperable</em>, and <em>Reusable</em> data, addressing the issue of reproducibility in scientific research. One observation from the original FAIR paper describes core tenets of Rich Context:</p>
<blockquote>
<p>Humans, however, are not the only critical stakeholders in the milieu of scientific data. Similar problems are encountered by the applications and computational agents that we task to undertake data retrieval and analysis on our behalf. These ‘computational stakeholders’ are increasingly relevant, and demand as much, or more, attention as their importance grows. One of the grand challenges of data-intensive science, therefore, is to improve knowledge discovery through assisting both humans, and their computational agents, in the discovery of, access to, and integration and analysis of, task-appropriate scientific data and other scholarly digital objects.</p>
</blockquote>
<p>In other words, throughout the use cases for scientific data there are substantial opportunities for human-in-the-loop AI approaches, where the people involved increasingly have their work augmented by automated means, while the automation involved increasingly gets improved by incorporating human expertise. One can use the metaphor of a <em>graph</em> to represent the linkages: those that span across distinct research projects, those that require cross-agency collaboration with sensitive data, and those that integrate workflows beyond the scope of specific tools. Specifically, this work entails the development of a <a href="https://en.wikipedia.org/wiki/Knowledge_base"><em>knowledge graph</em></a> to represent metadata about datasets, researchers, projects, agencies, etc., – including the computational agents involved – as distinct entities connected through relations that model their linkage.</p>
<figure>
<img src="combined_images/illo.2.png" title="example graph relations" alt="example graph relations" /><figcaption>example graph relations</figcaption>
</figure>
<p>Much of the intelligence in this kind of system is based on leveraging inference across the graph, insights which could not be inferred within the scope of a single research project or through the use of one particular tool. Over time, the process accumulates a richer context of relations into that graph while clarifying and leveraging the feedback loops among the entities within the graph. Rich Context establishes foundations for that work in social science research.</p>
<p>The <a href="https://coleridgeinitiative.org/richcontextcompetition#problemdescription">Rich Context Competition</a> held during September 2018 through February 2019 invited AI research teams to compete in one aspect of Rich Context requirements. Several teams submitted solutions to automate the discovery of research datasets along with associated research methods and fields, as identified in social science research publications. Methods for machine learning and text analytics used by the four finalist teams provided complementary approaches, all focused on the problem of <a href="https://en.wikipedia.org/wiki/Entity_linking"><em>entity linking</em></a>, with a corpus of social science research papers used as their training data.</p>
<p>The results of the competition provided metadata to describe links among datasets used in social science research. In other words, the outcome of the competition generated the basis for a moderately-sized knowledge graph. There are many publication sources to analyze, and the project will pursue that work as an ongoing process to extract the implied metadata. Meanwhile the increasing adoption and usage of the ADRF framework continues to accumulate metadata directly.</p>
<h2 id="use-cases-for-rich-context">Use Cases for Rich Context</h2>
<p>Looking at potential use cases for Rich Context more formally, we can identify needs for leveraging a knowledge graph about research datasets and related entities. For each of these needs, we can associate solutions based on open source software which have well-known use cases in industry.</p>
<p>As an example, consider a dataset <em>A001</em> published by a data provider <em>XYZ Agency</em> where <em>Jane Smith</em> works as the data steward responsible for curating that dataset. Over time, multiple research projects describe the use of <em>A001</em> in their published results. Some researchers note, on the one hand, that particular columns in data tables within <em>A001</em> have some troubling data quality issues – inconsistent names and acronyms, identifiers that require transformations before they can be used to join with other datasets, and so on. On the other hand, the body of research related to <em>A001</em> illustrates how it gets joined frequently with another dataset <em>B023</em> to support analysis using a particular research method. The two datasets provide more benefits when used together.</p>
<p>While access to the <em>A001</em> dataset gets managed through the <em>XYZ Agency</em> and its use of the ADRF framework, other datasets such as <em>B023</em> get used outside of that context. A knowledge graph is used to accumulate information about the datasets, research projects, the resulting published papers, etc., and applications for augmenting research derive quite directly from that graph. For example, feedback from researchers about how <em>A001</em> gets combined with other datasets outside of the <em>XYZ Agency</em> domain help guide <em>Jane Smith</em> to resolve some of the data quality issues. New columns get added with cleaner data for identifiers, which allows more effective linking. Other feedback based on machine learning models that have classified published papers then helps recommend research methods and candidate datasets to new analysts – and also to agencies that have adjacent needs, but did not previously have visibility into the datasets published by <em>XYZ Agency</em>.</p>
<p>These are the kinds of applications that become enabled through Rich Context. Search and discovery is clearly a need, although other use cases can help improve the discovery process and enhance social science research. The following sections discuss specific use cases and their high-level requirements for the associated technologies.</p>
<h3 id="search-and-discovery">Search and Discovery</h3>
<p>As described above, the vast majority of social science data and research results cannot be easily discovered by other researchers. While public search engines based on keyword search have been popularized by e-commerce platforms such as Google and Bing, the more general problem of search and discovery can be understood best as a graph problem, and the needs in social science research are more formally understood as recommendations across a graph.</p>
<p>For example, starting with a given dataset, who else has worked with that data? Which topics did they research? Which methods did they use? What were their results? In other words, starting from one entity in a knowledge graph, what other neighboring entities are linked?</p>
<p>These kinds of capabilities may be implemented simply by users traversing directly through the links of the graph. However, at scale, that volume of information can become tedious and overwhelming. It’s generally more effective for user experience (UX) to have machine learning models summarize, then predict a set of the most likely paths through the graph from a particular starting point.</p>
<p>One good approach for this is the general case of <em>link prediction</em><a href="#fn88" class="footnote-ref" id="fnref88" role="doc-noteref"><sup>88</sup></a>: given a researcher starting with a particular dataset and goals for topics or methods, represent that as a local, smaller graph. Then use link prediction to fill-in missing entities and relations, extending the local graph for that researcher. In other words, what other datasets should be joined, how can particular fields be used, what research topics or methods are related, which published papers might become foundations for this work? The most likely links inferred become top recommendations. Also, this kind of recommendation is not limited to the start of projects, it can be leveraged at almost any stage of research.</p>
<h3 id="entity-linking">Entity Linking</h3>
<p>The Rich Context Competition demonstrated how entities and relations used to construct a knowledge graph can be mined from a corpus of scientific papers. Machine learning methods for <em>entity linking</em><a href="#fn89" class="footnote-ref" id="fnref89" role="doc-noteref"><sup>89</sup></a> used in the competition need to be generalized and extended, then used to analyze the ongoing stream of published social science research. This work provides potential benefits for the publishers, for example helping them analyze and annotate newly published papers, developing dashboards about data impact metrics for journals or authors, and so on.</p>
<p>An additional benefit of entity linking is to help correct abbreviations, localized acronyms, or mistakes in linked data references. This is an iterative process which will need integration and feedback with data stewardship workflows.</p>
<h3 id="classifiers">Classifiers</h3>
<p>As any researcher or librarian knows well, curating a large set of research papers by hand is labor-intensive and prone to errors. Machine learning models based on <em>supervised learning</em> or <em>semi-supervised learning</em> (human-in-the-loop) can produce classifiers that annotate research papers automatically.</p>
<p>At some point, the ADRF framework may run classifiers on the workflows (e.g., Jupyter notebooks) for projects in progress. By extension, classifiers may infer across the knowledge graph to add annotations for datasets as well. This work can be considered a subset of link prediction, also related to entity linking.</p>
<h3 id="transitive-inference">Transitive Inference</h3>
<p>The metadata collected through the use of the ADRF framework or extracted from research publications includes relations that link entities in the graph. Once a graph is constructed, additional relations may be inferred. This is a case of <a href="https://en.wikipedia.org/wiki/Transitive_relation"><em>transitive inference</em></a>, which can help add useful annotations to the graph, as shown in the following diagram:</p>
<figure>
<img src="combined_images/illo.3.png" title="transitive relations" alt="transitive relations" /><figcaption>transitive relations</figcaption>
</figure>
<p>In an example from Norse mythology, Torunn is the daughter of Thor, and Thor is the son of Gaea, therefore Torun is the <em>granddaughter</em> of Gaea. The same process can apply, for example, to relations that describe links between datasets and researchers.</p>
<p>Note that embeddings have proven to be a powerful approach for inference about patterns, based on deep learning. On the current forefront of AI research, methods that leverage <em>reinforcement learning</em><a href="#fn90" class="footnote-ref" id="fnref90" role="doc-noteref"><sup>90</sup></a> are positioned to outperform embeddings soon, since they explore/exploit the graph structure instead of relying on a history of observed patterns. This is especially useful for <em>knowledge graph completion</em>, where there are cases of incomplete metadata in the knowledge graph, which is essential for Rich Context work.</p>
<h3 id="iterative-improvement-of-the-knowledge-graph">Iterative Improvement of the Knowledge Graph</h3>
<p>Most of the finalist teams in the Rich Context Competition made use of other existing graphs to bootstrap their machine learning development work, such as the <a href="https://docs.microsoft.com/en-us/academic-services/graph/reference-data-schema">Microsoft Academic Graph</a>, <a href="http://api.semanticscholar.org/corpus/">Semantic Scholar</a>, and others purpose-built for the competition. Those teams cited how some graph would need to be extended in the future, to improve recognition accuracy.</p>
<p>Rich Context now subsumes that effort, making the iterative improvement of the knowledge graph an ongoing priority. In lieu of those other graphs used for bootstrap purposes during the competition, the Rich Context knowledge graph provides the foundation for machine learning.</p>
<p>This process of accreting more entities into the graph and refining their relations leads to better training data and improved machine learning models. Over time, as our models improve, the previously analyzed research papers can be re-evaluated to extract richer results. That work in turn enhances social science research within the ADRF framework, along with data curation. That overall dynamic represents the virtuous cycle of metadata, which continually improves Rich Context.</p>
<h3 id="axioms-for-dataset-curation">Axioms for Dataset Curation</h3>
<p>Another immediate use of Rich Context is to assist the data stewards to understand the broader scope of usage for the datasets that they curate. For example, ontology <em>axioms</em> used on the metadata in the graph can help analyze:</p>
<ul>
<li>consistency checks for the incoming metadata</li>
<li>which data stewardship rules apply in a given case</li>
</ul>
<p>In a way, that helps codify what would otherwise be “institutional lore” – instead that’s now captured for others to study, use for training new staff, etc.</p>
<p>Note that the ADRF framework must provide means for customizing and configuring these kinds of axioms, so that data stewardship rules rules are not tightly coupled with the security audits and release cycle. Those rules can change rapidly, depending on new legislation or other policy updates, or even due to different agency environments.</p>
<h2 id="leveraging-open-standards-and-open-source">Leveraging Open Standards and Open Source</h2>
<p>Overall, the Rich Context portion of the ARDF framework represents a <em>data catalog</em> along with associated <em>data governance</em> practices. As a first step in knowledge graph work, we can make use of existing open standards for metadata about data catalogs and datasets. For example, the <a href="https://www.w3.org/2013/data/">W3C Data Activity</a> coordinates a wide range of metadata standards, including:</p>
<ul>
<li><a href="https://www.w3.org/TR/vocab-dcat/">DCAT</a> – metadata about data catalogs</li>
<li><a href="https://www.w3.org/TR/void/#dataset">VoID</a> – metadata about datasets</li>
<li><a href="http://dublincore.org/specifications/dublin-core/dcmi-terms/">DCMI</a> – Dublin Core metadata terms</li>
<li><a href="https://www.w3.org/TR/swbp-skos-core-guide/">SKOS</a> – “simple knowledge organization system”</li>
</ul>
<p>These represent <a href="https://en.wikipedia.org/wiki/Controlled_vocabulary">controlled vocabularies</a> described in <a href="https://www.w3.org/OWL/">OWL</a> and based atop <a href="https://www.w3.org/RDF/">RDF</a>. These standards can be combined and extended to suit the needs of specific use cases, such as within the ADRF framework. In particular, the Rich Context knowledge graph is a superset of a <a href="https://www.w3.org/TR/vocab-dcat/#conformance"><em>DCAT-compliant data catalog</em></a>. Taken together, localized extensions of these open standards represent an <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontology</a> – essentially as a specification for defining metadata that can be added into the knowledge graph and how that graph should be structured. Development of that ontology along with example metadata plus Python code to validate the graph is managed in the public repository <a href="https://github.com/Coleridge-Initiative/adrf-onto/wiki">adrf-onto</a> on GitHub.</p>
<p>The workflows within the ADRF framework represent use cases of data governance, and there is substantial overlap between Rich Context and emerging trends for data governance in industry. There are open source projects which leverage knowledge graphs to collect metadata about datasets and their usage, where machine learning helps address the complexities<a href="#fn91" class="footnote-ref" id="fnref91" role="doc-noteref"><sup>91</sup></a> of data governance in industry data science work. For instance:</p>
<ul>
<li><a href="https://eng.lyft.com/amundsen-lyfts-data-discovery-metadata-engine-62d27254fbb9">Amundsen</a> from Lyft</li>
<li><a href="https://marquezproject.github.io/marquez/">Marquez</a> from WeWork</li>
<li><a href="https://github.com/linkedin/WhereHows">WhereHows</a> from LinkedIn</li>
<li><a href="https://eng.uber.com/databook/">Databook</a> from Uber (pending release as open source)</li>
</ul>
<p>Of course the Rich Context work addresses special considerations for sensitive data and compliance requirements. Even so, much can be learned from these related open source projects in industry, which are pursuing similar kinds of use cases. <a href="https://www.topquadrant.com/">TopQuadrant</a> and <a href="https://www.astrazeneca.com/">AstraZeneca</a> are examples of commercial vendors which construct knowledge graphs about datasets, also for data governance purposes – respectively in the Finance and Pharma business verticals. These commercial solutions similarly make use of DCAT, VoID, DMCI, SKOS, and also the FAIR data principles.</p>
<p>In general, the subject of metadata exchange for data governance use cases is addressed by the <a href="https://www.odpi.org/">ODPi</a> open standard <a href="https://www.odpi.org/projects/egeria">Egeria</a> and related work by Mandy Chessell<a href="#fn92" class="footnote-ref" id="fnref92" role="doc-noteref"><sup>92</sup></a>, et al., including the <a href="https://atlas.apache.org/">Apache Atlas</a> open source project. Much of that work focuses on standards used to validate the exchange of metadata reliably across different frameworks. This implies potential opportunities for Rich Context to interoperate with other data governance solutions or related metadata services.</p>
<p>To help establish open standards and open source implementations related to Rich Context, the ADRF team has collaborated with <a href="https://jupyter.org/">Project Jupyter</a>. A new Rich Context feature set is being added to <a href="https://jupyterlab.readthedocs.io/en/stable/">JupyterLab</a>, which is one of the key open source projects used in the architecture of the ADRF framework, and these new features will be integrated into its future releases. The new Rich Context features support projects as top-level entities, real-time collaboration and commenting, data registry, metadata handling, annotations, and usage tracking – as described in the Project Jupyter “press release” requests for comments: <a href="https://github.com/jupyterlab/jupyterlab-data-explorer/blob/master/press_release.md">data explorer</a>, <a href="https://github.com/jupyterlab/jupyterlab-metadata-service/blob/master/press_release.md">metadata explorer</a>, and <a href="https://github.com/jupyterlab/jupyterlab-commenting/blob/master/press_release.md">commenting</a>. For example, a team of social science researchers working on a project could use the commenting feature in Jupyter to make an annotation about data quality issues encountered in a particular dataset. That comment, as metadata about the dataset, would get imported into the knowledge graph, and could later be used for recommendations to a data steward or other researchers.</p>
<p>Note that most of the machine learning approaches referenced above are specific cases of <a href="https://en.wikipedia.org/wiki/Deep_learning"><em>deep learning</em></a>, based on layered structures of artificial neural networks. In particular, <em>graph embedding</em><a href="#fn93" class="footnote-ref" id="fnref93" role="doc-noteref"><sup>93</sup></a> is an approach that vectorizes portions of graphs to use as training data for deep learning models. Graph embedding can be used to perform entity linking, link prediction, etc. In many of these cases, the resulting machine learning models become proxies for the graph data, such that the entire knowledge graph data is not required in production use cases. That practice contrasts earlier and generally less effective approaches which relied on graph queries applied to the full data. Note that the <a href="https://ocean.sagepub.com/blog/an-interview-with-the-allen-institute-for-artificial-intelligence">winning team</a> in the Rich Context Competition was from <a href="https://allenai.org/">Allen AI</a> which is a leader in the field of using embedded models for natural language. Typical open source frameworks which are popular for deep learning research include <a href="https://pytorch.org/">PyTorch</a> (from Facebook) and the more recent <a href="https://ray.readthedocs.io/en/latest/distributed_training.html">Ray</a> (from UC Berkeley RISElab).</p>
<h2 id="system-architecture-overview">System Architecture Overview</h2>
<p>The following diagram illustrates a proposed system architecture for Rich Context as an additional module in the ADRF framework:</p>
<figure>
<img src="combined_images/illo.4.png" title="Rich Context module" alt="Rich Context module" /><figcaption>Rich Context module</figcaption>
</figure>
<p>Building on the DFCore features plus the Data Stewardship module, Rich Context provides both a destination for metadata (logging events from components, or extracted metadata from analysis of publications) and a source for metadata ontology used in the ADRF framework. Machine learning models get trained and updated based on the knowledge graph, then used for services (recommender system, classifiers, etc.) provided back into the ADRF framework, and additionally to support training initiatives – or for general purpose search and discovery by researchers.</p>
<p>The additional system components for implementing Rich Context are based primarily on open source software (e.g., PyTorch) and extensions of open standards (e.g., W3C), all within the security context of AWS GovCloud implementation of the ADRF framework.</p>
<h2 id="trends-from-origins-to-near-term-future-projections">Trends, from origins to near-term future projections</h2>
<p>Meanwhile, the development of Rich Context has followed a familiar progression, echoing how the history of IT and data analytics practices matured over decades – albeit at a much faster pace. That progression indicates likely directions for how AI applications will come into use for Rich Context. Initial steps for Rich Context allowed researchers to analyze and report about sensistive data, while maintaining security and privacy compliance. That’s roughly analogous to data analytics during the heyday of <em>enterprise data warehouses</em> and <em>business intelligence</em> during the 1990s. Subsequent work improved online workflows for data stewards, adding reports about usage along with some metadata derived as “exhaust” from logs. That’s roughly analogous to “data driven” organizations that emerged during the late 2000s after initial adoption of <em>data science</em> practices. Next steps, such as the Rich Context Competition, began to use <em>machine learning models</em> to extract metadata that was embedded in unstructured data (i.e., research publication) to augment research efforts. That’s roughly analogous to the trends of machine learning adoption in industry during the mid 2010s. In the immediate future, Rich Context applications begin to leverage inference based on <em>knowledge graph</em> representations about researchers, datasets, publications, data stewards, and so on. Contemporary work in <em>deep learning</em> promises AI-based applications that can leverage embeddings in the graph, to give social science researchers better recommendations for their work. While that depends on historical patterns, current research on <em>reinforcement learning</em> to explore/exploit the structure of graphs can move beyond history and patterns, effectively considering “what if” scenarios that suggest unexplored research opportunities. That echoes the contemporary AI landscape, leading into the 2020s.</p>
<h2 id="summary">Summary</h2>
<p>Rich Context recognizes that social science research depends on <em>linked data</em> usage of micro data and its metadata. Effective management of that metadata is based on a graph that exists outside the context of component point solutions and specific workflows. While there is substantial use of linked data for ecommerce platforms and research in life sciences, social science research presents nuances and new challenges that haven’t been addressed previously.</p>
<p>The Rich Context portions of the ADRF framework interconnect workflows that facilitate research – as explicit feedback loops in the graph – along with means to extract metadata from published research – as implicit feedback loops in the graph. That process creates a kind of virtuous cycle for metadata, making use of AI applications to augment social science research, with continual improvement of the entities and relations represented within the graph.</p>
<p>A prerequisite was to create a corpus of research publications, used for training data during the Rich Context Competition, which demonstrated how to extract metadata from research publications.</p>
<p>The next step will be a formal implementation of the knowledge graph, based primarily on extensions of open standards and use of open source software. That graph is represented as an extension of a DCAT-compliant data catalog. It will eventually incorporate the new Rich Context features going into Project Jupyter. Immediate goals are to augment search and discovery in social science research, plus additional use cases that help improve the knowledge graph and augment research through the ADRF framework.</p>
<p>In the longer term, the process introduces human-in-the-loop AI into data curation, ultimately to reward researchers and data stewards whose work contributes additional information into the system. With this latter step, in the broader sense Rich Context helps establish a community focused on contributing code plus knowledge into the research process.</p>
<h2 id="notes">Notes</h2>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Wilkinson, M. D. et al. <a href="https://www.nature.com/articles/sdata201618">The FAIR Guiding Principles for scientific data management and stewardship</a>. Sci. Data 3:160018 doi: 10.1038/sdata.2016.18 (2016)<a href="#fnref1" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn2" role="doc-endnote"><p>Wilkinson, M. D. et al. <a href="https://www.nature.com/articles/sdata201618">The FAIR Guiding Principles for scientific data management and stewardship</a>. Sci. Data 3:160018 doi: 10.1038/sdata.2016.18 (2016)<a href="#fnref2" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn3" role="doc-endnote"><p>Wilkinson, M. D. et al. <a href="https://www.nature.com/articles/sdata201618">The FAIR Guiding Principles for scientific data management and stewardship</a>. Sci. Data 3:160018 doi: 10.1038/sdata.2016.18 (2016)<a href="#fnref3" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn4" role="doc-endnote"><p>The authors would like to thank Jannick Blaschke, Rafael Beier, and the editors for helpful comments. We would like to thank Jannick Blaschke for providing graphics. The views expressed here represent the authors‘ personal opinions and do not necessarily reflect the views of the Deutsche Bundesbank or the Eurosystem. ## Abstract<a href="#fnref4" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn5" role="doc-endnote"><p>See Desai et al. (2016) for a detailed description of the Five Safes framework.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn6" role="doc-endnote"><p>Data producers in different departments across Bundesbank compile data, e.g. microdata, indicators, or time series.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn7" role="doc-endnote"><p>In our model, we have called this knowledge user specific knowledge. Here the knowledge is in that sense specific, that it can be used to fulfil the task of Bundesbank in a better way<a href="#fnref7" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn8" role="doc-endnote"><p>Chapter 4 discusses in more detail how metadata may support the discovery of microdata.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn9" role="doc-endnote"><p>See Chapter 5 for a more detailed description of the evaluation process used in the competition.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn10" role="doc-endnote"><p>For the future, ongoing effort is needed to support all four “corners of the circle” / “pillars of the level model”. The current competition strengthens the arrow from publication to knowledge and structures gained knowledge to improve data services. To support the data services pillar – for example -, digital RDC environments with facilitated access processes like a “data stewardship module” will in our view improve data access.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn11" role="doc-endnote"><p>Wilkinson, M. D. et al. <a href="https://www.nature.com/articles/sdata201618">The FAIR Guiding Principles for scientific data management and stewardship</a>. Sci. Data 3:160018 doi: 10.1038/sdata.2016.18 (2016)<a href="#fnref11" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn12" role="doc-endnote"><p>For a sample of recent research papers regarding link prediction through graph embedding, see <a href="https://arxiv.org/search/?query=%22link+prediction%22+%22graph+embedding%22&amp;searchtype=all&amp;abstracts=show&amp;order=&amp;size=50">these Arxiv results</a>.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn13" role="doc-endnote"><p>One of the better resources online for entity linking is <a href="http://nlpprogress.com/english/entity_linking.html">NLP-progress</a> which specifically tracks the state-of-the-art (SOTA) papers, along with their scores on recognized benchmarks.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn14" role="doc-endnote"><p>between approaches based on RL vs. embedding, see <a href="https://arxiv.org/abs/1808.10568">“Multi-Hop Knowledge Graph Reasoning with Reward Shaping”</a>; Xi Victoria Lin, Richard Socher, Caiming Xiong; <em>EMNLP 2018 </em><a href="https://arxiv.org/abs/1808.10568">arXiv:1808.10568 [cs.AI]</a><a href="#fnref14" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn15" role="doc-endnote"><p>A good survey paper about these issues is given <a href="http://cidrdb.org/cidr2017/papers/p111-hellerstein-cidr17.pdf">Ground: A Data Context Service</a>, Hellerstein et al., <em>CIDR 2017</em>, based on research by <a href="https://rise.cs.berkeley.edu/blog/publication/ground-data-context-service-2/">UC Berkeley RISElab</a>.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn16" role="doc-endnote"><p>See <a href="https://zenodo.org/record/556504#.XSAUAJNKgWo">“The Case for Open Metadata”</a>, Mandy Chessell, <em>Frontiers in Data Science</em>, 2016-09-15.<a href="#fnref16" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn17" role="doc-endnote"><p>For an overview of graph embedding, see <a href="https://towardsdatascience.com/overview-of-deep-learning-on-graph-embeddings-4305c10ad4a4">“Graph Embedding for Deep Learning”</a>, Flawson Tong (2019-05-06).<a href="#fnref17" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn18" role="doc-endnote"><p><a href="https://en.wikipedia.org/wiki/Category:Statistical_methods" class="uri">https://en.wikipedia.org/wiki/Category:Statistical_methods</a><a href="#fnref18" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn19" role="doc-endnote"><p>https://spacy.io/api/tokenizer<a href="#fnref19" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn20" role="doc-endnote"><p><a href="https://rasa.com/docs/nlu" class="uri">https://rasa.com/docs/nlu</a><a href="#fnref20" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn21" role="doc-endnote"><p><a href="https://nlp.stanford.edu/projects/glove" class="uri">https://nlp.stanford.edu/projects/glove</a><a href="#fnref21" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn22" role="doc-endnote"><p><a href="https://fasttext.cc/docs/en/crawl-vectors.html" class="uri">https://fasttext.cc/docs/en/crawl-vectors.html</a><a href="#fnref22" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn23" role="doc-endnote"><p><a href="https://www.w3.org/TR/shacl/" class="uri">https://www.w3.org/TR/shacl/</a><a href="#fnref23" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn24" role="doc-endnote"><p>https://joinup.ec.europa.eu/release/dcat-ap/11<a href="#fnref24" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn25" role="doc-endnote"><p>This differs from library or archival collections, which are usually thematically related, and for which the selection of items for inclusion is defined by an express collection policy.<a href="#fnref25" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn26" role="doc-endnote"><p>The challenges of metadata for data streams are related to the cataloguing of different editions of a work and of serials in a text-based library.<a href="#fnref26" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn27" role="doc-endnote"><p>https://wikidata.org/<a href="#fnref27" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn28" role="doc-endnote"><p>https://duraspace.org/vivo/about/<a href="#fnref28" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn29" role="doc-endnote"><p>https://ddialliance.org/Specification/RDF/XKOS<a href="#fnref29" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn30" role="doc-endnote"><p>http://www.obofoundry.org/<a href="#fnref30" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn31" role="doc-endnote"><p>https://www.icpsr.umich.edu/<a href="#fnref31" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn32" role="doc-endnote"><p>http://ddialliance.org<a href="#fnref32" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn33" role="doc-endnote"><p>DDI is also used for datasets from other organizations such as the National Opinion Research Center (NORC).<a href="#fnref33" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn34" role="doc-endnote"><p>https://www.cessda.eu/<a href="#fnref34" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn35" role="doc-endnote"><p>https://www.europeansocialsurvey.org/data/<a href="#fnref35" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn36" role="doc-endnote"><p>Australia National Data Service: https://www.ands.org.au/<a href="#fnref36" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn37" role="doc-endnote"><p>There are additional collections at http://data.census.gov, http://gss.norc.org. <span class="underline"></span> http://electionstudies.org, http://psidonline.isr.umich.edu, and http://www.nlsinfo.org.<a href="#fnref37" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn38" role="doc-endnote"><p>https://www.earthcube.org/<a href="#fnref38" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn39" role="doc-endnote"><p>https://pds.nasa.gov/<a href="#fnref39" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn40" role="doc-endnote"><p>https://www.uniprot.org/<a href="#fnref40" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn41" role="doc-endnote"><p>http://www.rcsb.org/<a href="#fnref41" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn42" role="doc-endnote"><p>re3data.org<a href="#fnref42" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn43" role="doc-endnote"><p>https://datadryad.org/<a href="#fnref43" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn44" role="doc-endnote"><p>https://datacite.org/<a href="#fnref44" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn45" role="doc-endnote"><p>https://www.crossref.org/<a href="#fnref45" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn46" role="doc-endnote"><p>Health Level Seven International, https://www.hl7.org/<a href="#fnref46" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn47" role="doc-endnote"><p>Kyoto Encyclopedia of Genes and Genomes, https://www.genome.jp/kegg/<a href="#fnref47" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn48" role="doc-endnote"><p>See also, e.g., http://www.dcc.ac.uk/digital-curation/planning-preservation<a href="#fnref48" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn49" role="doc-endnote"><p>Controlled Lots of Copies Keep Stuff Safe, https://clockss.org/<a href="#fnref49" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn50" role="doc-endnote"><p>http://irods.org<a href="#fnref50" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn51" role="doc-endnote"><p>The policies are based on the International Research on Permanent Authentic Records in Electronic Systems (InterPARES) standard. https://interparestrust.org/<a href="#fnref51" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn52" role="doc-endnote"><p>http://www.dcc.ac.uk/sites/default/files/DRAMBORA_Interactive_Manual%5B1%5D.pdf; see also, http://www.dcc.ac.uk/resources/repository-audit-and-assessment/drambora.<a href="#fnref52" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn53" role="doc-endnote"><p>http://www.loc.gov/standards/premis/ontology/<a href="#fnref53" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn54" role="doc-endnote"><p>https://www.w3.org/TR/prov-o/<a href="#fnref54" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn55" role="doc-endnote"><p>The term micro-data is used in two distinct ways. In the context of HTML, it is associated with embedding Schema.org codes into web pages similar to micro-formats. In the context of survey data, it refers to individual-level data.<a href="#fnref55" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn56" role="doc-endnote"><p>https://jupyter.org/<a href="#fnref56" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn57" role="doc-endnote"><p>http://wiss-ki.eu<a href="#fnref57" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn58" role="doc-endnote"><p>https://www.colectica.com/<a href="#fnref58" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn59" role="doc-endnote"><p>https://gssdataexplorer.norc.org/<a href="#fnref59" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn60" role="doc-endnote"><p>https://eng.lyft.com/amundsen-lyfts-data-discovery-metadata-engine-62d27254fbb9<a href="#fnref60" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn61" role="doc-endnote"><p>https://mmisw.org/<a href="#fnref61" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn62" role="doc-endnote"><p>https://www.w3.org/TR/vocab-data-cube/<a href="#fnref62" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn63" role="doc-endnote"><p>https://www.earthcube.org/info/about<a href="#fnref63" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn64" role="doc-endnote"><p>http://sdmx.org/<a href="#fnref64" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn65" role="doc-endnote"><p>https://taverna.incubator.apache.org/<a href="#fnref65" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn66" role="doc-endnote"><p>https://www.myexperiment.org/about<a href="#fnref66" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn67" role="doc-endnote"><p>https://statswiki.unece.org/display/gsim/Generic+Statistical+Information+Model. GSIM is coordinated with the Common Statistical Production Architecture (CSPA). https://unstats.un.org/unsd/nationalaccount/workshops/2015/gabon/BD/CSPA-ENG.pdf<a href="#fnref67" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn68" role="doc-endnote"><p>https://www.niem.gov/about-niem<a href="#fnref68" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn69" role="doc-endnote"><p>https://www.openarchives.org/pmh/<a href="#fnref69" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn70" role="doc-endnote"><p>Wilkinson, M. D. et al. <a href="https://www.nature.com/articles/sdata201618">The FAIR Guiding Principles for scientific data management and stewardship</a>. Sci. Data 3:160018 doi: 10.1038/sdata.2016.18 (2016)<a href="#fnref70" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn71" role="doc-endnote"><p>For a sample of recent research papers regarding link prediction through graph embedding, see <a href="https://arxiv.org/search/?query=%22link+prediction%22+%22graph+embedding%22&amp;searchtype=all&amp;abstracts=show&amp;order=&amp;size=50">these Arxiv results</a>.<a href="#fnref71" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn72" role="doc-endnote"><p>One of the better resources online for entity linking is <a href="http://nlpprogress.com/english/entity_linking.html">NLP-progress</a> which specifically tracks the state-of-the-art (SOTA) papers, along with their scores on recognized benchmarks.<a href="#fnref72" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn73" role="doc-endnote"><p>between approaches based on RL vs. embedding, see <a href="https://arxiv.org/abs/1808.10568">“Multi-Hop Knowledge Graph Reasoning with Reward Shaping”</a>; Xi Victoria Lin, Richard Socher, Caiming Xiong; <em>EMNLP 2018 </em><a href="https://arxiv.org/abs/1808.10568">arXiv:1808.10568 [cs.AI]</a><a href="#fnref73" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn74" role="doc-endnote"><p>Wilkinson, M. D. et al. <a href="https://www.nature.com/articles/sdata201618">The FAIR Guiding Principles for scientific data management and stewardship</a>. Sci. Data 3:160018 doi: 10.1038/sdata.2016.18 (2016)<a href="#fnref74" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn75" role="doc-endnote"><p>For a sample of recent research papers regarding link prediction through graph embedding, see <a href="https://arxiv.org/search/?query=%22link+prediction%22+%22graph+embedding%22&amp;searchtype=all&amp;abstracts=show&amp;order=&amp;size=50">these Arxiv results</a>.<a href="#fnref75" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn76" role="doc-endnote"><p>One of the better resources online for entity linking is <a href="http://nlpprogress.com/english/entity_linking.html">NLP-progress</a> which specifically tracks the state-of-the-art (SOTA) papers, along with their scores on recognized benchmarks.<a href="#fnref76" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn77" role="doc-endnote"><p>between approaches based on RL vs. embedding, see <a href="https://arxiv.org/abs/1808.10568">“Multi-Hop Knowledge Graph Reasoning with Reward Shaping”</a>; Xi Victoria Lin, Richard Socher, Caiming Xiong; <em>EMNLP 2018 </em><a href="https://arxiv.org/abs/1808.10568">arXiv:1808.10568 [cs.AI]</a><a href="#fnref77" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn78" role="doc-endnote"><p>A good survey paper about these issues is given <a href="http://cidrdb.org/cidr2017/papers/p111-hellerstein-cidr17.pdf">Ground: A Data Context Service</a>, Hellerstein et al., <em>CIDR 2017</em>, based on research by <a href="https://rise.cs.berkeley.edu/blog/publication/ground-data-context-service-2/">UC Berkeley RISElab</a>.<a href="#fnref78" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn79" role="doc-endnote"><p>See <a href="https://zenodo.org/record/556504#.XSAUAJNKgWo">“The Case for Open Metadata”</a>, Mandy Chessell, <em>Frontiers in Data Science</em>, 2016-09-15.<a href="#fnref79" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn80" role="doc-endnote"><p>For an overview of graph embedding, see <a href="https://towardsdatascience.com/overview-of-deep-learning-on-graph-embeddings-4305c10ad4a4">“Graph Embedding for Deep Learning”</a>, Flawson Tong (2019-05-06).<a href="#fnref80" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn81" role="doc-endnote"><p><a href="https://en.wikipedia.org/wiki/Category:Statistical_methods" class="uri">https://en.wikipedia.org/wiki/Category:Statistical_methods</a><a href="#fnref81" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn82" role="doc-endnote"><p>https://spacy.io/api/tokenizer<a href="#fnref82" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn83" role="doc-endnote"><p><a href="https://rasa.com/docs/nlu" class="uri">https://rasa.com/docs/nlu</a><a href="#fnref83" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn84" role="doc-endnote"><p><a href="https://nlp.stanford.edu/projects/glove" class="uri">https://nlp.stanford.edu/projects/glove</a><a href="#fnref84" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn85" role="doc-endnote"><p><a href="https://fasttext.cc/docs/en/crawl-vectors.html" class="uri">https://fasttext.cc/docs/en/crawl-vectors.html</a><a href="#fnref85" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn86" role="doc-endnote"><p><a href="https://www.w3.org/TR/shacl/" class="uri">https://www.w3.org/TR/shacl/</a><a href="#fnref86" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn87" role="doc-endnote"><p>Wilkinson, M. D. et al. <a href="https://www.nature.com/articles/sdata201618">The FAIR Guiding Principles for scientific data management and stewardship</a>. Sci. Data 3:160018 doi: 10.1038/sdata.2016.18 (2016)<a href="#fnref87" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn88" role="doc-endnote"><p>For a sample of recent research papers regarding link prediction through graph embedding, see <a href="https://arxiv.org/search/?query=%22link+prediction%22+%22graph+embedding%22&amp;searchtype=all&amp;abstracts=show&amp;order=&amp;size=50">these Arxiv results</a>.<a href="#fnref88" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn89" role="doc-endnote"><p>One of the better resources online for entity linking is <a href="http://nlpprogress.com/english/entity_linking.html">NLP-progress</a> which specifically tracks the state-of-the-art (SOTA) papers, along with their scores on recognized benchmarks.<a href="#fnref89" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn90" role="doc-endnote"><p>between approaches based on RL vs. embedding, see <a href="https://arxiv.org/abs/1808.10568">“Multi-Hop Knowledge Graph Reasoning with Reward Shaping”</a>; Xi Victoria Lin, Richard Socher, Caiming Xiong; <em>EMNLP 2018 </em><a href="https://arxiv.org/abs/1808.10568">arXiv:1808.10568 [cs.AI]</a><a href="#fnref90" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn91" role="doc-endnote"><p>A good survey paper about these issues is given <a href="http://cidrdb.org/cidr2017/papers/p111-hellerstein-cidr17.pdf">Ground: A Data Context Service</a>, Hellerstein et al., <em>CIDR 2017</em>, based on research by <a href="https://rise.cs.berkeley.edu/blog/publication/ground-data-context-service-2/">UC Berkeley RISElab</a>.<a href="#fnref91" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn92" role="doc-endnote"><p>See <a href="https://zenodo.org/record/556504#.XSAUAJNKgWo">“The Case for Open Metadata”</a>, Mandy Chessell, <em>Frontiers in Data Science</em>, 2016-09-15.<a href="#fnref92" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn93" role="doc-endnote"><p>For an overview of graph embedding, see <a href="https://towardsdatascience.com/overview-of-deep-learning-on-graph-embeddings-4305c10ad4a4">“Graph Embedding for Deep Learning”</a>, Flawson Tong (2019-05-06).<a href="#fnref93" class="footnote-back" role="doc-backlink">↩</a></p></li>
</ol>
</section>
