\subsection{Discussions and Limitations}
\label{sec:discussion}
% Structure suggestions from Julia Lane

%\subsubsection{what-worked-what-not}
%\subsubsection{summary-of-results-and-caveats}
%\subsubsection{lessons-learned-and-what-would-you-do-differently}
%\subsubsection{what-comes-next}

%In this section we discuss our approaches for each of the three tasks.

%The paper presents/has presented several solutions to
\paragraph{Dataset Extraction.} 
%TODO ich w√ºrde hier noch einmal kurz die wichtigsten Ergebnisse zusammenfassen (P/R) und diese interpretieren (taugt/taugt nicht). Dann kannst Du in die Limitations einsteigen.
For the dataset extraction task, the proposed methods are only tested on social science related data.
The performance measures we have introduced are based on a hold out data set of our automatically created dataset.
Especially the recall may be biased given that our training as well as testing data is biased towards known datasets, where datasets not yet part of our reference set are not considered.

The results of the second phase presented during the RCC workshop\footnote{
    Agenda of the Workshop: 
    \url{https://coleridgeinitiative.org/richcontextcompetition/workshopagenda}.
    The results of the finalists are presented here: 
    \url{https://youtu.be/PE3nFrEkwoU?t=9865}.
}
are showing good performance of our approach in comparison to the other finalist teams with the highest precision 52.2\% (second: 47.0\%) and second in recall (ours: 20.5, best: 34.8\%).
%, 39.6\% and 33.3\%)
With respect to F1, our approach provides the second best performing system for this task (29.5\%, 40.0\% for first place).
The results on the manually created hold out set underline, that our system performs better in respect to precision in comparison to the other finalist teams.
%This could be affected by the decision to annotate only full dataset titles during the automatic annotation process.
%In comparison to the usage of the ground truth term set there are nearly no abbreviation. 
Given that our models are supervised through a corpus of social sciences publications, we anticipate limited generalisability across other disciplines and plan to investigate this aspect as part of future work. In this context, the focus of our training data towards survey data, also reflected in dataset titles such as \textit{Current Population Survey}, could have biased the model to detect the survey as a specific type of research datasets better than other subtypes like e.g. text corpora in the NLP community.
In general, however, our approach to using a weakly labeled corpus created from a list of dataset names could be applied in other research domains.
%The needed roussources are a set of full text publications and a sufficiently large list of dataset names for this domain.
\begin{comment}
* limitations\
**    Performance is difficult to measure\\
**    Ground truth needed to have a good measure\\
**    During development a ground truth holdout set would have prevented the feeling to fish in troubled waters.\\
%One of the biggest problems of the dataset extraction task was to handle abbreaviations.
\end{comment}

\paragraph{Research method extraction.} %TODO as above, best to start with a brief summary of the results (evaluation results) and their interpretation. 
We consider the extraction of research methods from full text as a particularly challenging task because the sample vocabulary given by the RCC organizers covers a large thematic variety of areas.
% We consider the extraction of research methods from full text as the most challenging task.
% This is because the sample vocabulary given by the RCC covers a large thematic area.
The task itself was defined as the identification of research methods associated with a specific publication, which in turn are drawn from a specific research field.
Since no training data has been provided, we created and annotated a new corpus for the task and trained a CRF model, adding lexical resources.
The qualitative reviews during the two phases of the competition attested that this approach works fine.



% old

\paragraph{Research field classification.}
Our supervised machine learning approach to handle the research field classification task performs well on the dataset created from social science publication metadata. A micro F1 measure of above 55\% seems to indicate reasonable performance considering the small dataset with 44~labels and a mean number of keywords of three terms per publication.
As one example of multilabel classification with a comparable size of labels we would like to mention the classification of texts in the domain of medicine presented in \cite{wang2018joint}.
The models tested by the authors on the task of multilabel prediction from 50 different labels leads to micro F1 values between 53\% and 62\%.\\
Considering the evaulation approach, focused on publications from the social sciences, the generalisability across other disciplines remains unclear and requires further research.
Even though the used classification scheme may cover neighbouring disciplines, for instance, medicine, the numbers of samples of the training data covering other research fields than the social science is limited.
%Taking this into account, it can be assumed that the performance for corpora of other disciplines is lower.
Our pragmatic approach of basing our classifications on the abstract of the publications makes it applicable even in scenarios where the full-text of publications is not accessible. %On the other hand, the use of Cermine to extract information from publications available as PDF files enables us to automatically extract abstracts. With the help of this we are able to classify publications, even if abstracts not present in the publications metadata.

%For each method, brief summary of:
%\begin{itemize}
%\item results / performance
%\item applicability / generalisability
%\item limitations and challenges (eg lack of large-scale GT, corpus-specific training etc etc)
%\end{itemize}

