\subsection{Data and Pre-processing}
This section describes the external data sources we used as well as our pre-processing steps.

%\subsubsection{The RCC Corpus}
%\label{subsec:rcc-corpus}
%For the first phase, the data provided by the organizers consisted of 5,000 publications. Additionally, a development fold of 100 plain text publications, their metadata, a list of datasets of interest (including all datasets that were explicitly referenced in the curated corpus) were given. The list of datasets should not be considered complete as  there could be additional datasets mentioned in these publications. 
%The organizers also provided examples of social science research methods and fields vocabularies in term of SAGE Publications research field and method vocabularies. 
%In the second phase of the competition, an additional set of 5,000 publications from the social sciences has been provided.

\subsubsection{External Data Sources}
For developing our algorithms, we utilized two external data sources. For the discovery of research methods and fields, we resort to data from Social Science Open Access Repository\footnote{\url{https://www.gesis.org/ssoar/home}} (SSOAR). 
GESIS â€“ Leibniz Institute for the Social Sciences maintains  SSOAR by collecting and archiving literature of relevance to the social sciences. 

In SSOAR, full texts are indexed using controlled social science vocabulary (Thesaurus\footnote{\url{https://www.gesis.org/en/services/research/tools/thesaurus-for-the-social-sciences}}, Classification\footnote{\url{https://www.gesis.org/angebot/recherchieren/tools-zur-recherche/klassifikation-sozialwissenschaften} (in German)}) and are assigned rich metadata. SSOAR offers documents in various languages. The corpus of English language publications that can be used for purposes of the competition consists of a total of 13,175 documents. All SSOAR documents can be accessed through the OAI-PMH\footnote{{\url{http://www.openarchives.org}}} interface. 

Another external source that we used for the discovery of research methods is the ACL Anthology Reference Corpus~\cite{bird2008acl}. ACL ARC is a corpus of scholarly publications about computational linguistics.  
The corpus consists of a total of 22,878 articles.

%SSOAR contains 
%\subsubsection{SSOAR}
%\label{subsubsec:ssoar-dataset}
%For example, 22,453 records with English abstracts in SSOAR also contain information about the classification of social sciences documents - 156 different labels of the classification.
%For items with English title and classoz classification, the numbers are different. SSOAR includes 15,522 items with English titles and cover 154 classoz classification labels.
%Classification Schemes used in the anntotations of SSOAR publications.
%\begin{enumerate}
%    \item Thesaurus Social Science (thesoz)
%    \item classification of the social sciences %(classoz)\footnote{\url{https://www.gesis.org/angebot/recherchieren/tools-zur-recherche/klassifikation-sozialwissenschaften} (in German)}
%    \item ddc classification (ddc)
%\end{enumerate}

%\subsubsection{ACL}
%ACL 

%\dd{table of external data/sources?SSOAR metedata including abstracts and classifications(thesoz/ classsoz)}

\subsubsection{Pre-processing}
\label{sec:prepro}
Although the organizers of the RCC, offered plain texts for the publication, we decided to build our own pre-process pipeline. The pipeline uses the Cermine Tool to extract information from PDF documents. The main benefit of using this tool is the structured metadata output, including better disambiguation of sections and paragraphs in the publications. The output XML file uses the Journal Article Tag Suite\footnote{\url{https://jats.nlm.nih.gov}}. For the competition, there are only two interesting elements of the Jats XML format, i.e., $<$front$>$ and $<$body$>$. The $<$front$>$ element contains the metadata of the publication, whereas the $<$body$>$ contains the main textual and graphic content of the publication.
Another advantage of Cermine is that the hyphenation and segmentation of paragraphs are carried out automatically. 
As a last step of the pre-processing, we remove all linebreaks from the publication. The output of this step is a list of metadata fields and values, as shown in Table~\ref{tab:example-paragraph} for each publication paragraph.

\begin{table}[h]
    \centering
     \caption{An Example output of our pre-processing for a paragraph in a given publication.}
    \begin{tabular}{ll}
        \toprule
        {}                  &                            Example Text Field Data \\
        \midrule
        publication\_id     &                                              12744 \\
        label               &                                    paragraph\_text \\
        text                &                     A careful reading of text, word\\
                            &                                  for word, was ... \\
        section\_title      &                                      Data Analysis \\
        annotations         &                        [\{'start': 270, 'end': 295,\\
                            &                              'type': 'bibref', ... \\
        section\_nr         &                                             [3, 2] \\
        text\_field\_nr     &                                                 31 \\
        para\_in\_section   &                                                  1 \\
        \bottomrule
    \end{tabular}
    \label{tab:example-paragraph}
\end{table}

