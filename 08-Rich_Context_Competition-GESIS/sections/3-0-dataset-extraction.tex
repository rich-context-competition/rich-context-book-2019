\subsection{Dataset extraction}
\label{sec:dataset-extraction}
\subsubsection{Task description}
In the scientific literature, datasets are cited to reference, for example, the data on which an analysis is performed or on which a particular result or claim is based.
In this competition, we focus on (i) extracting and (ii) disambiguating dataset mentions from social science publications to a list of given dataset references.
Identifying dataset mentions in literature is a challenging problem due to the huge number of styles of citing datasets. Although there are proposed standards for dataset citation in full-texts, researchers still ignore or neglect such standards (see, e.g., \cite{altman2007proposed}).
Furthermore, in many research publications, a correct citation of datasets is often missing~\cite{boland2012identifying}. 
The following two sentences exemplify the problem of the usage of an abbreviation to make a reference to an existing dataset.
The first example illustrates the use of abbreviations that are known mainly in the author's research domain.
The latter illustrates the ambiguity of abbreviations.
In this case, \emph{WHO} identifies a dataset published by the World Health Organization and does not refer to the institution itself.\\
\textbf{Example 1}: \emph{P-values are reported for the one-tail paired t-test on \emph{Allbus} (dataset mention) and \emph{ISSP} (dataset mention).}\\
\textbf{Example 2}: \emph{We used \emph{WHO data from 2001} (dataset mention) to estimate the spreading degree of AIDS in Uganda.}\\
We treat the problem of detecting dataset mentions in full-text as a Named Entity Recognition (NER) task. 
%BG: pranthesis in the text are confusing since reader may think that (dataset mention) is part of the the real text 
    %COMMENT (AZ): The following paragraph might be skipped
\paragraph{Formal problem definition}%\ \\[1pt]
%Let $D$ denote a set of existing datasets $d$. The Named Entity Recognition task is defined as the identification of dataset mentions $m$ in a sentence where $m$ references a dataset $d$. 
Let $D$ denote a set of existing datasets $d$ and the knowledge base $K$ as a set of known dataset references $k$. Furthermore, each element of $K$ is referencing an existing dataset $d$. The Named Entity Recognition and Linking task is defined as (i) the identification of dataset mentions $m$ in a sentence, where $m$ references a dataset $d$ and (ii) linking them, when possible, to one element in $K$ (i.e., the reference dataset list given by the RCC). 
%BG: this part could be improved. Could be confusing for user.


\subsubsection{Challenges}
We focus on the extraction of dataset mentions in the body of the full-text of scientific publications.
There are three types of dataset mentions:
(i) The full name of a dataset (''National Health and Nutrition Examination Survey``),
(ii) an abbreviation (''NHaNES``) or (iii) a vague reference, e.g., ''the monthly statistic``.
With all these these types, the NER task faces special challenges.
In the first case, the used dataset name can vary in different publications.
For instance one publication cites the dataset with ''National Health and Nutrition Examination Survey`` the other could use the words  ''Health and Nutrition Survey``.
In the case where abbreviations are used, a disambiguation problem occurs, e.g., in ''WHO data``. WHO may describe the World Health Organization or the White House Office.
In the case, that an abbreviation is used after the dataset name has been written in full, the mapping between these different spellings in one text is referred to as Coreference Resolution.
The biggest challenge is again the lack of annotated training data.
%The major challenge of using a NER approach to detect dataset mentions in text is the necessity of an annotated training corpus which is not given by the RCC.
In the following we describe how we have dealt with this lack of ground truth data.

\subsubsection{Phase one approach}
Missing ground truth data is the main problem to handle during this competition.
To this end, supervised learning methods for dataset mentions extraction from texts are not applicable without the identification of external training data or the creation of useful labeled training data from information given by the competition.
Because of the lack of existing training data for the task of dataset mention extraction we resort to the provided list of dataset mentions and publication pairs and re-annotate the particular sentences in the publication text.
A list of dataset identifying words is provided for some of the known links between publications and datasets by the competition.
These words represent the evidence of the linkage between publication and datasets and are extracted from the publication text.
In the course of re-annotation, we search for each of the identifying words in the corresponding publication texts.
For each match, we annotate the occurence in our raw text and use these annotations as ground truth.
As described in the pre-processing section, our units for processing the publication text are paragraphs.
The re-annotated corpus consists of a list of paragraphs for each publication with stand-off annotations identifying the mentions of datasets (i.e. position of the start and end characters and the entity type for each mention: \emph{dataset}).
This re-annotation is then used to train Spacy's neural network-based NER model\footnote{\url{spacy.io}}.
We created a holdout set of 1,000 publications and a training set of size 4,000.
Afterwards, we train our model with the paragraphs as a sampling unit.
In the training set, 0.45 percent of the paragraphs contained mentions.
For each positive training example, we have added one negative sample that contains no known dataset mentions and is randomly selected.   
%(paragraphs without mentions: 265029; paragraphs with mentions: 12566)
%We extracted all paragraphs with mentions and merged them with a randomly chosen subset of paragraphs without mentions with the same the size resulting in 25132 paragraphs.
We used a batch size of 25 and a dropout rate of 0.4.
The model was trained for 300 iterations.
\paragraph{Evaluation}
We evaluated our model with respect to four metrics: precision and recall, each for strict and for partial match.
While the strict match metrics are standard evaluation metrics, the partial match metrics are their relaxed variants in which the degree to which dataset mentions have to match can vary. 
%This evaluation scheme is pased Semeval 2013 description can be found in this blog
% \url(http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/)
%In the evaluation method, we have a parameter to control the weight of only partly matched mentions. 
Consider the following partial match example: "National Health and Nutrition Examination Survey" is the extracted dataset mention, while "National Health and Nutrition Examination Survey (NHANES)" is the true dataset mention. 
In contrast to the strict version of the metrics, this overlapping match is considered a match for the partial version.
The scores describe whether a model is able to find the correct positions of dataset mentions in the texts, even if the start and end positions of the characters are not the same, but the ranges overlap.
%We set $alpha$ to $1.0$, meaning that the true and extracted dataset mention have to overlap in at least one token to count as a match. Setting $\alpha$
\begin{table}[b]
    \center 
    \caption{Performance of phase one approach of dataset extraction. } 
    \begin{tabular}{lc} 
        \toprule
        Metric  & Value \\
        \midrule
        Precision (partial match)   & 0.93 \\
        Recall (strict match)      & 0.95 \\
        \midrule
        Precision (strict match)    & 0.80 \\
        Recall (strict match)       & 0.81 \\ 
        \bottomrule \\ 
    \end{tabular} 
    \label{table:dataset-mention-eval} 
\end{table}
%Evaluation results of dataset mention extraction on holdout set

Table~\ref{table:dataset-mention-eval} show the results of the dataset mention extraction on the holdout set. The model can achieve high strict precision and recall values. As expected, the results are even better for the partial version of the metrics. It means that even if we couldn't match the dataset mention in a text exactly, we can find the right context with very high precision.
%Notably  The results show, that in 10\% of the found matches we have not found the strict correct dataset mention, but the correct position and there is a partial match.

\subsubsection{Phase two approach}
In the second phase of the competition, additional 5,000 publications were provided by RCC. We extended our approach to consider the list with dataset names supplied by the organizers and re-annotated the complete corpus of 15,000 publications in the same manner as in phase one to obtain training data. This time we split the data in 80\% for training and 20\% for test. 

\paragraph{Evaluation}
We resort to the same evaluation metrics as in phase one. However, we calculate precision and recall on the full-text of the publication and not on the paragraphs as in the first phase. Table~\ref{table:dataset-mention-eval-phase-two} shows the results achieved by our model. We observe lower precision and recall values. Compared to phase one, there is also a smaller difference between the precision and recall values for the strict and partial version of the metrics. 
\begin{table}[hb]
    \center 
    \caption{Performance of phase two approach for dataset extraction.} 
    \begin{tabular}{lc} 
        \toprule
        Metric  & Value \\
        \midrule
        Precision (partial match)   & 0.51 \\
        Recall (partial match)      & 0.90 \\
        \midrule
        Precision (strict match)    & 0.49 \\
        Recall (strict match)       & 0.87 \\ 
        \bottomrule \\ 
    \end{tabular} 
    \label{table:dataset-mention-eval-phase-two} 
\end{table}

